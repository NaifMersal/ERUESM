{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n",
      "\n",
      "Processing 22135 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 22135/22135 [00:00<00:00, 268194.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing EYASE...\n",
      "\n",
      "Processing ShEMO...\n",
      "\n",
      "Processing CREMA-D...\n",
      "\n",
      "Processing RAVDESS...\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-13.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-02-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-02-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-01-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-02-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-02-02-01-21.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-01-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-02-01-01-06.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-02-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-02-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-09.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-04-01-02-02-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-01-20.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-02-01-10.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-02.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-19.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-02-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-02-02-01-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-03.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-07.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-02-02-04.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-05-01-01-01-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-01-02-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-01-02-01-12.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-08-01-02-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-17.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-01-02-15.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-01-02-24.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-01-05.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-01-01-02-01-18.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-02-02-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-22.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-02-02-02-16.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-02-01-02-01-11.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-03-02-01-02-08.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-02-01-01-14.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-06-01-01-01-23.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-01-02-01-01.wav\n",
      "/home/naif/projects/videoEmotionRecognition/data/preprocessed_faces/audio/02-01-07-02-01-02-12.wav\n",
      "\n",
      "Processing URDU...\n",
      "\n",
      "Processing UJ_ARABIC...\n",
      "\n",
      "Processing SaudiEMO...\n",
      "\n",
      "Processing SUBESCO...\n",
      "\n",
      "Files per dataset:\n",
      "Dataset\n",
      "CREMA-D      7442\n",
      "SUBESCO      7000\n",
      "ShEMO        3000\n",
      "RAVDESS       864\n",
      "SaudiEMO      659\n",
      "EYASE         579\n",
      "URDU          400\n",
      "UJ_ARABIC     175\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Files per emotion:\n",
      "Emotion\n",
      "neutral     4099\n",
      "angry       3978\n",
      "sad         3253\n",
      "happy       2984\n",
      "fear        2309\n",
      "disgust     2271\n",
      "surprise    1225\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Metadata saved to: ./data/metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.helpers import setup_env\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.simplefilter('ignore')\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Dataset', 'Path', 'Emotion'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "neutral     4099\n",
       "angry       3978\n",
       "sad         3253\n",
       "happy       2984\n",
       "fear        2309\n",
       "disgust     2271\n",
       "surprise    1225\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.helpers import train_test_split, seed\n",
    "df =pd.read_csv('data/metadata.csv')\n",
    "print(df.columns)\n",
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happy', 'angry', 'neutral', 'sad', 'surprise', 'fear', 'disgust'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "angry      3499\n",
       "neutral    3252\n",
       "sad        2858\n",
       "happy      2603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =df[~((df['Emotion']=='surprise') | (df['Emotion']=='disgust')| (df['Emotion']=='fear') |  (df['Dataset']=='RAVDESS')|(df['Dataset']=='EYASE')| (df['Dataset']=='SaudiEMO') )].reset_index(drop=True) #|(df['Dataset']=='SaudiEMO')| (df['Dataset']=='RAVDESS')| (df['Dataset']=='EYASE') | (df['Dataset']=='CREMA-D') | (df['Dataset']=='ShEMO')| (df['Dataset']=='CREMA-D')\n",
    "# df =df[df['Dataset']=='UJ_ARABIC' ] #| ((df['Dataset']=='SaudiEMO') & (df['Emotion']=='sad')) |(df['Emotion']=='neutral') (df['Dataset']=='EYASE') | ((df['Dataset']=='SaudiEMO')&((df['Emotion']!='happy')))|\n",
    "counts=df['Emotion'].value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stratify_on\n",
       "CREMA-D_angry        1271\n",
       "CREMA-D_sad          1271\n",
       "CREMA-D_happy        1271\n",
       "CREMA-D_neutral      1087\n",
       "ShEMO_angry          1059\n",
       "ShEMO_neutral        1028\n",
       "SUBESCO_neutral      1000\n",
       "SUBESCO_sad          1000\n",
       "SUBESCO_happy        1000\n",
       "SUBESCO_angry        1000\n",
       "ShEMO_sad             449\n",
       "ShEMO_happy           201\n",
       "URDU_happy            100\n",
       "URDU_angry            100\n",
       "URDU_neutral          100\n",
       "URDU_sad              100\n",
       "UJ_ARABIC_angry        69\n",
       "UJ_ARABIC_sad          38\n",
       "UJ_ARABIC_neutral      37\n",
       "UJ_ARABIC_happy        31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stratify_on'] = df['Dataset']+'_'+df['Emotion']\n",
    "df['stratify_on'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SIZE=0.2\n",
    "df['Emotion'] = df['Emotion'].astype(\"category\")\n",
    "df['target']=df['Emotion'].cat.codes.astype(np.int64)\n",
    "# Perform the stratified split\n",
    "train_df, valid_df = train_test_split(df, stratify=df['stratify_on'], test_size=VALID_SIZE, random_state=seed)\n",
    "# [path, class, target_code]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007990741631059764,\n",
       " 0.017277110263772006,\n",
       " 0.007990741631059764,\n",
       " 0.008208872380719663,\n",
       " 0.017277110263772006,\n",
       " 0.035645118859703284,\n",
       " 0.017277110263772006,\n",
       " 0.017277110263772006,\n",
       " 0.007990741631059764,\n",
       " 0.035645118859703284]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique classes and their counts\n",
    "stratify_on =train_df['stratify_on'].to_numpy()\n",
    "unique_classes, class_counts = np.unique(stratify_on, return_counts=True)\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "n_samples = len(stratify_on)\n",
    "class_weights = {}\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    # Weight = total_samples / (num_classes * count_per_class)\n",
    "    if cls ==\"SaudiEMO_neutral\":\n",
    "        class_weights[cls] = n_samples / (len(unique_classes) * (count+300))**0.7\n",
    "    else:\n",
    "        class_weights[cls] = n_samples / (len(unique_classes) * count)**0.9\n",
    "\n",
    "class_weights = {key :class_weights[key]/sum(class_weights.values()) for key  in class_weights}\n",
    "# Create sample weights array\n",
    "sample_weights = [class_weights[t] for t in stratify_on]\n",
    "sample_weights[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CREMA-D_angry': 0.006785072585369801,\n",
       " 'CREMA-D_happy': 0.006785072585369801,\n",
       " 'CREMA-D_neutral': 0.007808651276794735,\n",
       " 'CREMA-D_sad': 0.006785072585369801,\n",
       " 'SUBESCO_angry': 0.008420974792967988,\n",
       " 'SUBESCO_happy': 0.008420974792967988,\n",
       " 'SUBESCO_neutral': 0.008420974792967988,\n",
       " 'SUBESCO_sad': 0.008420974792967988,\n",
       " 'ShEMO_angry': 0.007990741631059764,\n",
       " 'ShEMO_happy': 0.035645118859703284,\n",
       " 'ShEMO_neutral': 0.008208872380719663,\n",
       " 'ShEMO_sad': 0.017277110263772006,\n",
       " 'UJ_ARABIC_angry': 0.09220917392917168,\n",
       " 'UJ_ARABIC_happy': 0.19054490816987674,\n",
       " 'UJ_ARABIC_neutral': 0.16170901300682874,\n",
       " 'UJ_ARABIC_sad': 0.1570065718758547,\n",
       " 'URDU_angry': 0.06689018041955934,\n",
       " 'URDU_happy': 0.06689018041955934,\n",
       " 'URDU_neutral': 0.06689018041955934,\n",
       " 'URDU_sad': 0.06689018041955934}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angry', 'happy', 'neutral', 'sad'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       3\n",
       "2       0\n",
       "3       2\n",
       "4       3\n",
       "       ..\n",
       "9770    1\n",
       "9771    3\n",
       "9772    1\n",
       "9773    1\n",
       "9774    2\n",
       "Name: target, Length: 9775, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "angry      3.490140\n",
       "neutral    3.755228\n",
       "sad        4.272918\n",
       "happy      4.691510\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)/counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3499\n",
       "2    3252\n",
       "3    2858\n",
       "1    2603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = df['Emotion'].cat.codes.value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angry', 'happy', 'neutral', 'sad'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from scratch trainnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 05:30:33.136819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 05:30:33.605108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from src.data import audio_data_loader\n",
    "from src.train import optimize\n",
    "from src.helpers import load_model, replace_insatance\n",
    "import torch.nn.functional as F\n",
    "import src.model as models\n",
    "from torch.optim import lr_scheduler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2324, 0.2694, 0.2411, 0.2571], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights= torch.tensor([len(df)/(class_counts[i]*len(class_counts))**0.5 for i in range(len(class_counts)) ],dtype=torch.float32 ,device='cuda')\n",
    "class_weights/=class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainnig parameters\n",
    "print_model=1\n",
    "batch_size = 16 # size of the minibatch for stochastic gradient descent (or Adam)\n",
    "num_epochs = 16     # number of epochs for training\n",
    "num_classes = len(counts)       # number of classes. Do not change this\n",
    "learning_rate =0.001*(0.4)**0  # Learning rate for SGD (or Adam)\n",
    "weight_decay = 0.004     # regularization. Increase this to combat overfitting\n",
    "momentum=0.9 \n",
    "accumulation_steps=4\n",
    "checkpoints_loading_order = ['best', 'last']\n",
    "# features_to_use=['emotion2vec'] #  bert, emotion2vec\n",
    "model_name= \"FullEm2vecGRU384h\"\n",
    "suffix='0.6Oversampling0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \tTraining Loss: 0.580900 \tValidation Loss: 0.455464\tTraining Acc@1: 80.563 \tValidation Acc@1: 87.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|█▌                              | 30/611 [00:09<03:13,  3.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n,torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(p)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\u001b[38;5;241m/\u001b[39mp\u001b[38;5;241m.\u001b[39mnumel()) \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#replace_insatance(model,torch.nn.LeakyReLU, models.Swish())\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#print(model)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43minteractive_tracking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/emotion2vec/src/train.py:123\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(data_loaders, model, optimizer, loss, s_epoch, n_epochs, model_name, step, topk, accumulation_steps, use_amp, interactive_tracking, checkpoints_dir, run_logs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(optimizer))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(s_epoch, s_epoch \u001b[38;5;241m+\u001b[39m n_epochs):\n\u001b[0;32m--> 123\u001b[0m     train_loss, train_accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     valid_loss, valid_accs \u001b[38;5;241m=\u001b[39m valid_one_epoch(data_loaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m], model, loss, topk)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m interactive_tracking:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;66;03m# Ensure all values are Python numbers, not tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/emotion2vec/src/train.py:47\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_dataloader, model, optimizer, loss, scaler, accumulation_steps, topk)\u001b[0m\n\u001b[1;32m     44\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     45\u001b[0m accs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(topk)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(train_dataloader),\n\u001b[1;32m     49\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader),\n\u001b[1;32m     51\u001b[0m         leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m         ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m):\n\u001b[1;32m     54\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m to_device(data, device), to_device(target, device)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scaler:\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/envs/em2vec/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(train_df), replacement=True, generator=None)\n",
    "data_loaders= {'train': audio_data_loader(train_df, sampler=sampler, batch_size=batch_size, num_workers=10),\n",
    "                'valid': audio_data_loader(valid_df,shuffle=False,batch_size=batch_size, num_workers=10)}\n",
    "torch.backends.cuda.matmul.allow_tf32=True\n",
    "torch.backends.cudnn.allow_tf32=True\n",
    "\n",
    "model = getattr(models, model_name)(num_classes=num_classes)\n",
    "# # initialize\n",
    "# def init_weights(m):\n",
    "#     if isinstance(m, torch.nn.Conv2d):\n",
    "#         torch.nn.init.orthogonal_(m.weight)\n",
    "\n",
    "# model.apply(init_weights)\n",
    "# Get the optimizer using get_optimizer and the model you just created, the learning rate,\n",
    "# the optimizer and the weight decay specified in the previous cel\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay,) #momentum=momentum,\n",
    "\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=1,threshold=1e-3,verbose=True)\n",
    "#scheduler = lr_scheduler.ExponentialLR(optimizer,gamma=0.7)\n",
    "\n",
    "\n",
    "def step(loss ,epoch=None):\n",
    "    scheduler.step(loss)\n",
    "\n",
    "def loss(output,target):\n",
    "    #target=F.one_hot(target, num_classes).float()\n",
    "    #output=torch.squeeze(output, 1)\n",
    "    return F.cross_entropy(output, target,label_smoothing=0.03, weight=class_weights) #weight=class_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s_epoch=load_model(model_name+suffix,model, loading_order=checkpoints_loading_order)\n",
    "if print_model:\n",
    "    print(f\"model {model_name} has :{sum(p.numel() for p in model.parameters())/1e6} M parameters \")\n",
    "    print(f\"Effictive W>0.01 precentage: \")\n",
    "    print('\\n'.join('layer {} has : {}'.format(n,torch.sum(torch.abs(p)>0.01)/p.numel()) for n, p in model.named_parameters()))\n",
    "#replace_insatance(model,torch.nn.LeakyReLU, models.Swish())\n",
    "#print(model)\n",
    "\n",
    "optimize(\n",
    "    data_loaders,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    s_epoch=s_epoch,\n",
    "    n_epochs=num_epochs,\n",
    "    model_name=model_name+suffix,\n",
    "    step=step,\n",
    "    accumulation_steps=accumulation_steps,\n",
    "    run_logs=True,\n",
    "    interactive_tracking=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 06:06:25,966 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt: /home/naif/.cache/modelscope/hub/iic/emotion2vec_plus_base/model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naif/miniconda3/envs/em2vec/lib/python3.8/site-packages/funasr/train_utils/load_pretrained_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  src_state = torch.load(path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n",
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naif/projects/emotion2vec/src/helpers.py:496: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'{checkpoints_dir}/{loading_order[0]}_{model_name}.pt')\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 57.45it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 58.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 57.25it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 58.17it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 51.64it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 51.08it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 52.38it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 52.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 52.27it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 38.73it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 46.18it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:02<00:00, 44.08it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 23.51it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 13.73it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 14.36it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.21it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.77it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.05it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.42it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['CREMA-D_angry', 0.2329760121665602, [96.06299212598425]],\n",
       " ['CREMA-D_happy', 0.3576825704044245, [94.48818897637796]],\n",
       " ['CREMA-D_neutral', 0.24608859234416533, [96.7741935483871]],\n",
       " ['CREMA-D_sad', 0.8230303724215728, [73.22834645669292]],\n",
       " ['SUBESCO_angry', 0.2531627096980807, [95.5]],\n",
       " ['SUBESCO_happy', 0.40642914988100526, [91.0]],\n",
       " ['SUBESCO_neutral', 0.23625634364783765, [95.0]],\n",
       " ['SUBESCO_sad', 1.0397629117965703, [66.5]],\n",
       " ['ShEMO_angry', 0.2214658980685952, [97.6303317535545]],\n",
       " ['ShEMO_happy', 0.3336712401360273, [92.5]],\n",
       " ['ShEMO_neutral', 0.28503327456916244, [95.1219512195122]],\n",
       " ['ShEMO_sad', 1.3410219494881257, [64.04494382022472]],\n",
       " ['UJ_ARABIC_angry', 0.5018020169093058, [92.3076923076923]],\n",
       " ['UJ_ARABIC_happy', 0.7374432807167371, [83.33333333333333]],\n",
       " ['UJ_ARABIC_neutral', 0.7109463278736388, [71.42857142857143]],\n",
       " ['UJ_ARABIC_sad', 2.3586066748414725, [28.571428571428573]],\n",
       " ['URDU_angry', 0.14639066830277442, [100.0]],\n",
       " ['URDU_happy', 0.44802838712930676, [85.0]],\n",
       " ['URDU_neutral', 0.40007768720388415, [90.0]],\n",
       " ['URDU_sad', 0.5619915306568146, [85.0]]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.train import valid_one_epoch\n",
    "model = getattr(models, model_name)(num_classes=num_classes).cuda()\n",
    "\n",
    "loss =lambda output,target: F.cross_entropy(output, target,label_smoothing=0.03, weight=class_weights)\n",
    "\n",
    "load_model(model_name+suffix,model,loading_order=checkpoints_loading_order[::1])#checkpoints_loading_order[::-1]\n",
    "results =[]\n",
    "for dataset in valid_df['stratify_on'].unique():\n",
    "    data_df = valid_df[valid_df['stratify_on']==dataset].reset_index(drop=True)\n",
    "    valid_loader = audio_data_loader(data_df,batch_size=1 ,shuffle=False)\n",
    "    results.append([dataset, *valid_one_epoch(valid_loader, model, loss)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 33/33 [00:01<00:00, 31.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9828774825190053, [72.72727272727273])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = valid_df[valid_df['Dataset']=='UJ_ARABIC'].reset_index(drop=True)\n",
    "valid_loader = audio_data_loader(data_df,batch_size=1 ,shuffle=False)\n",
    "valid_one_epoch(valid_loader, model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 17:19:20.100300: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 17:19:20.545791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/naif/miniconda3/envs/em2vec/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-30 17:19:24,624 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt: /home/naif/.cache/modelscope/hub/iic/emotion2vec_plus_base/model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naif/miniconda3/envs/em2vec/lib/python3.8/site-packages/funasr/train_utils/load_pretrained_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  src_state = torch.load(path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naif/projects/emotion2vec/src/helpers.py:496: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'{checkpoints_dir}/{loading_order[0]}_{model_name}.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.14it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 74.45it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:02<00:00, 72.51it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 69.54it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 60.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 62.59it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 60.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 59.83it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 49.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 36.59it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 42.38it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:02<00:00, 43.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 24.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 14.93it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.56it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 17.52it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.34it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.72it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 40.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 8 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 68.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 72.18it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 68.41it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 60.67it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 60.30it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.65it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.03it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 47.82it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 31.32it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 43.88it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 49.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 26.99it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.40it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 14.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 19.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.73it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.74it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.83it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.24it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 69.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 66.35it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 59.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 59.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 59.13it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 60.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 58.14it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 40.73it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 49.51it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 49.54it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 29.08it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.09it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.83it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 18.32it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.37it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.57it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 10 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 62.08it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 67.62it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.89it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 59.47it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 54.57it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.80it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 57.94it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 42.45it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:03<00:00, 51.50it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 45.99it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.68it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 15.41it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.72it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 18.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.62it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.72it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.06it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['phase1', 'best', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'best', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'best', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'best', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'best', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'best', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'best', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'best', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'best', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'best', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'best', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'best', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'best', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'best', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'best', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'best', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'best', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'best', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'best', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'best', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'best', 0, 'CREMA-D_angry', 0.258736290096298, [94.09448818897638]], ['phase2', 'best', 0, 'CREMA-D_happy', 0.3128238437095964, [93.7007874015748]], ['phase2', 'best', 0, 'CREMA-D_neutral', 0.2521394816686482, [96.31336405529954]], ['phase2', 'best', 0, 'CREMA-D_sad', 0.6713012946168266, [77.55905511811024]], ['phase2', 'best', 0, 'SUBESCO_angry', 0.25672499284148226, [95.0]], ['phase2', 'best', 0, 'SUBESCO_happy', 0.3820594840496778, [92.0]], ['phase2', 'best', 0, 'SUBESCO_neutral', 0.23552342765033246, [95.5]], ['phase2', 'best', 0, 'SUBESCO_sad', 0.9597138513624667, [67.5]], ['phase2', 'best', 0, 'ShEMO_angry', 0.22040177592169052, [96.6824644549763]], ['phase2', 'best', 0, 'ShEMO_happy', 0.32229302674531946, [95.0]], ['phase2', 'best', 0, 'ShEMO_neutral', 0.2571009743504408, [95.1219512195122]], ['phase2', 'best', 0, 'ShEMO_sad', 1.2437129648548833, [68.53932584269663]], ['phase2', 'best', 0, 'UJ_ARABIC_angry', 0.7015116925422962, [69.23076923076923]], ['phase2', 'best', 0, 'UJ_ARABIC_happy', 0.6391726980606716, [83.33333333333333]], ['phase2', 'best', 0, 'UJ_ARABIC_neutral', 0.7040895244904927, [57.142857142857146]], ['phase2', 'best', 0, 'UJ_ARABIC_sad', 1.8073663839272092, [28.571428571428573]], ['phase2', 'best', 0, 'URDU_angry', 0.14747261628508565, [100.0]], ['phase2', 'best', 0, 'URDU_happy', 0.45416179522871986, [85.0]], ['phase2', 'best', 0, 'URDU_neutral', 0.4689487174153329, [85.0]], ['phase2', 'best', 0, 'URDU_sad', 0.4275880873203277, [95.0]], ['phase1', 'last', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'last', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'last', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'last', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'last', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'last', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'last', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'last', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'last', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'last', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'last', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'last', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'last', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'last', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'last', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'last', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'last', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'last', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'last', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'last', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'last', 0, 'CREMA-D_angry', 0.2588579188298993, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_happy', 0.3136730649339872, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_neutral', 0.25121473428291113, [96.31336405529954]], ['phase2', 'last', 0, 'CREMA-D_sad', 0.6725248215118728, [77.55905511811024]], ['phase2', 'last', 0, 'SUBESCO_angry', 0.25600903317332274, [95.0]], ['phase2', 'last', 0, 'SUBESCO_happy', 0.3834911134093998, [92.0]], ['phase2', 'last', 0, 'SUBESCO_neutral', 0.23549174077808857, [95.5]], ['phase2', 'last', 0, 'SUBESCO_sad', 0.9606494718790054, [67.0]], ['phase2', 'last', 0, 'ShEMO_angry', 0.22003566039399522, [97.1563981042654]], ['phase2', 'last', 0, 'ShEMO_happy', 0.32309657931327823, [95.0]], ['phase2', 'last', 0, 'ShEMO_neutral', 0.2568363875877568, [95.1219512195122]], ['phase2', 'last', 0, 'ShEMO_sad', 1.2490537643767476, [66.29213483146067]], ['phase2', 'last', 0, 'UJ_ARABIC_angry', 0.6984921602102426, [69.23076923076923]], ['phase2', 'last', 0, 'UJ_ARABIC_happy', 0.6390940000613531, [83.33333333333333]], ['phase2', 'last', 0, 'UJ_ARABIC_neutral', 0.7022053365196501, [57.142857142857146]], ['phase2', 'last', 0, 'UJ_ARABIC_sad', 1.813152142933437, [28.571428571428573]], ['phase2', 'last', 0, 'URDU_angry', 0.14700243920087813, [100.0]], ['phase2', 'last', 0, 'URDU_happy', 0.4553044453263283, [85.0]], ['phase2', 'last', 0, 'URDU_neutral', 0.4675500065088273, [85.0]], ['phase2', 'last', 0, 'URDU_sad', 0.42905081287026403, [95.0]]]\n",
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 20 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 68.12it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.74it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.03it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 59.30it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.41it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 52.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 41.18it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 48.66it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 46.52it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.02it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.84it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.97it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.50it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.92it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 7 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 63.04it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 63.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.44it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.48it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.34it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.55it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 55.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 42.30it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 49.35it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 45.84it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.31it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.15it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 17.34it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.93it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.33it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.58it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.21it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 67.17it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.02it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.51it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.01it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 54.74it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.15it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 56.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 41.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 49.06it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 48.41it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.17it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.27it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.18it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.47it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.08it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 10 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 63.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.37it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 64.83it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.09it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.87it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.79it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.32it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 57.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 41.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 49.76it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 48.58it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 29.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 15.85it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.26it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.81it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.62it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 37.22it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['phase1', 'best', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'best', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'best', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'best', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'best', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'best', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'best', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'best', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'best', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'best', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'best', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'best', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'best', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'best', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'best', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'best', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'best', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'best', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'best', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'best', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'best', 0, 'CREMA-D_angry', 0.258736290096298, [94.09448818897638]], ['phase2', 'best', 0, 'CREMA-D_happy', 0.3128238437095964, [93.7007874015748]], ['phase2', 'best', 0, 'CREMA-D_neutral', 0.2521394816686482, [96.31336405529954]], ['phase2', 'best', 0, 'CREMA-D_sad', 0.6713012946168266, [77.55905511811024]], ['phase2', 'best', 0, 'SUBESCO_angry', 0.25672499284148226, [95.0]], ['phase2', 'best', 0, 'SUBESCO_happy', 0.3820594840496778, [92.0]], ['phase2', 'best', 0, 'SUBESCO_neutral', 0.23552342765033246, [95.5]], ['phase2', 'best', 0, 'SUBESCO_sad', 0.9597138513624667, [67.5]], ['phase2', 'best', 0, 'ShEMO_angry', 0.22040177592169052, [96.6824644549763]], ['phase2', 'best', 0, 'ShEMO_happy', 0.32229302674531946, [95.0]], ['phase2', 'best', 0, 'ShEMO_neutral', 0.2571009743504408, [95.1219512195122]], ['phase2', 'best', 0, 'ShEMO_sad', 1.2437129648548833, [68.53932584269663]], ['phase2', 'best', 0, 'UJ_ARABIC_angry', 0.7015116925422962, [69.23076923076923]], ['phase2', 'best', 0, 'UJ_ARABIC_happy', 0.6391726980606716, [83.33333333333333]], ['phase2', 'best', 0, 'UJ_ARABIC_neutral', 0.7040895244904927, [57.142857142857146]], ['phase2', 'best', 0, 'UJ_ARABIC_sad', 1.8073663839272092, [28.571428571428573]], ['phase2', 'best', 0, 'URDU_angry', 0.14747261628508565, [100.0]], ['phase2', 'best', 0, 'URDU_happy', 0.45416179522871986, [85.0]], ['phase2', 'best', 0, 'URDU_neutral', 0.4689487174153329, [85.0]], ['phase2', 'best', 0, 'URDU_sad', 0.4275880873203277, [95.0]], ['phase1', 'last', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'last', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'last', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'last', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'last', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'last', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'last', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'last', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'last', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'last', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'last', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'last', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'last', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'last', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'last', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'last', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'last', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'last', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'last', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'last', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'last', 0, 'CREMA-D_angry', 0.2588579188298993, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_happy', 0.3136730649339872, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_neutral', 0.25121473428291113, [96.31336405529954]], ['phase2', 'last', 0, 'CREMA-D_sad', 0.6725248215118728, [77.55905511811024]], ['phase2', 'last', 0, 'SUBESCO_angry', 0.25600903317332274, [95.0]], ['phase2', 'last', 0, 'SUBESCO_happy', 0.3834911134093998, [92.0]], ['phase2', 'last', 0, 'SUBESCO_neutral', 0.23549174077808857, [95.5]], ['phase2', 'last', 0, 'SUBESCO_sad', 0.9606494718790054, [67.0]], ['phase2', 'last', 0, 'ShEMO_angry', 0.22003566039399522, [97.1563981042654]], ['phase2', 'last', 0, 'ShEMO_happy', 0.32309657931327823, [95.0]], ['phase2', 'last', 0, 'ShEMO_neutral', 0.2568363875877568, [95.1219512195122]], ['phase2', 'last', 0, 'ShEMO_sad', 1.2490537643767476, [66.29213483146067]], ['phase2', 'last', 0, 'UJ_ARABIC_angry', 0.6984921602102426, [69.23076923076923]], ['phase2', 'last', 0, 'UJ_ARABIC_happy', 0.6390940000613531, [83.33333333333333]], ['phase2', 'last', 0, 'UJ_ARABIC_neutral', 0.7022053365196501, [57.142857142857146]], ['phase2', 'last', 0, 'UJ_ARABIC_sad', 1.813152142933437, [28.571428571428573]], ['phase2', 'last', 0, 'URDU_angry', 0.14700243920087813, [100.0]], ['phase2', 'last', 0, 'URDU_happy', 0.4553044453263283, [85.0]], ['phase2', 'last', 0, 'URDU_neutral', 0.4675500065088273, [85.0]], ['phase2', 'last', 0, 'URDU_sad', 0.42905081287026403, [95.0]], ['phase1', 'best', 1, 'CREMA-D_angry', 0.27615764127002945, [94.88188976377953]], ['phase1', 'best', 1, 'CREMA-D_happy', 0.23527879508461533, [95.66929133858268]], ['phase1', 'best', 1, 'CREMA-D_neutral', 0.25794078073193955, [96.7741935483871]], ['phase1', 'best', 1, 'CREMA-D_sad', 0.6431511204660413, [79.5275590551181]], ['phase1', 'best', 1, 'SUBESCO_angry', 0.3505234102904796, [91.0]], ['phase1', 'best', 1, 'SUBESCO_happy', 0.30056495204567907, [91.5]], ['phase1', 'best', 1, 'SUBESCO_neutral', 0.32848145671188816, [92.0]], ['phase1', 'best', 1, 'SUBESCO_sad', 0.8100879185646771, [77.0]], ['phase1', 'best', 1, 'ShEMO_angry', 0.20217314553204307, [97.6303317535545]], ['phase1', 'best', 1, 'ShEMO_happy', 0.33687370978295794, [87.5]], ['phase1', 'best', 1, 'ShEMO_neutral', 0.2413041352498823, [96.09756097560975]], ['phase1', 'best', 1, 'ShEMO_sad', 0.9465443391478462, [69.66292134831461]], ['phase1', 'best', 1, 'UJ_ARABIC_angry', 1.5619367189132252, [61.53846153846154]], ['phase1', 'best', 1, 'UJ_ARABIC_happy', 0.4149790306886037, [83.33333333333333]], ['phase1', 'best', 1, 'UJ_ARABIC_neutral', 0.6496904577527727, [85.71428571428571]], ['phase1', 'best', 1, 'UJ_ARABIC_sad', 2.2628096171787804, [0.0]], ['phase1', 'best', 1, 'URDU_angry', 0.18128371462225912, [100.0]], ['phase1', 'best', 1, 'URDU_happy', 0.40306554064154626, [90.0]], ['phase1', 'best', 1, 'URDU_neutral', 0.26644408404827113, [95.0]], ['phase1', 'best', 1, 'URDU_sad', 0.36069156974554056, [95.0]], ['phase2', 'best', 1, 'CREMA-D_angry', 0.29083810124810267, [93.30708661417323]], ['phase2', 'best', 1, 'CREMA-D_happy', 0.23703179144718511, [96.06299212598425]], ['phase2', 'best', 1, 'CREMA-D_neutral', 0.2536086365237215, [96.31336405529954]], ['phase2', 'best', 1, 'CREMA-D_sad', 0.6164421358798431, [81.10236220472441]], ['phase2', 'best', 1, 'SUBESCO_angry', 0.3682341238856316, [91.0]], ['phase2', 'best', 1, 'SUBESCO_happy', 0.3087896878272295, [92.5]], ['phase2', 'best', 1, 'SUBESCO_neutral', 0.3148857557773592, [93.0]], ['phase2', 'best', 1, 'SUBESCO_sad', 0.8102790326625107, [76.5]], ['phase2', 'best', 1, 'ShEMO_angry', 0.20505267777149144, [98.10426540284361]], ['phase2', 'best', 1, 'ShEMO_happy', 0.3488592237234116, [87.5]], ['phase2', 'best', 1, 'ShEMO_neutral', 0.22883073178733276, [96.58536585365853]], ['phase2', 'best', 1, 'ShEMO_sad', 0.928263324365187, [71.91011235955057]], ['phase2', 'best', 1, 'UJ_ARABIC_angry', 1.614923622745734, [61.53846153846154]], ['phase2', 'best', 1, 'UJ_ARABIC_happy', 0.4457362964749336, [100.0]], ['phase2', 'best', 1, 'UJ_ARABIC_neutral', 0.5752684026956558, [85.71428571428571]], ['phase2', 'best', 1, 'UJ_ARABIC_sad', 2.1905118397303993, [0.0]], ['phase2', 'best', 1, 'URDU_angry', 0.19813046157360079, [95.0]], ['phase2', 'best', 1, 'URDU_happy', 0.432581639289856, [90.0]], ['phase2', 'best', 1, 'URDU_neutral', 0.27857217714190496, [95.0]], ['phase2', 'best', 1, 'URDU_sad', 0.3698753505945205, [95.0]], ['phase1', 'last', 1, 'CREMA-D_angry', 0.2761957376726031, [94.09448818897638]], ['phase1', 'last', 1, 'CREMA-D_happy', 0.23440244113366435, [94.88188976377953]], ['phase1', 'last', 1, 'CREMA-D_neutral', 0.2563687742168453, [96.31336405529954]], ['phase1', 'last', 1, 'CREMA-D_sad', 0.6558136834990316, [79.13385826771653]], ['phase1', 'last', 1, 'SUBESCO_angry', 0.3387189420312643, [91.5]], ['phase1', 'last', 1, 'SUBESCO_happy', 0.30185804873704913, [93.0]], ['phase1', 'last', 1, 'SUBESCO_neutral', 0.325886485427618, [92.0]], ['phase1', 'last', 1, 'SUBESCO_sad', 0.8205169849097728, [76.5]], ['phase1', 'last', 1, 'ShEMO_angry', 0.19906430384276588, [98.10426540284361]], ['phase1', 'last', 1, 'ShEMO_happy', 0.343780105188489, [87.5]], ['phase1', 'last', 1, 'ShEMO_neutral', 0.2343073303379663, [96.09756097560975]], ['phase1', 'last', 1, 'ShEMO_sad', 0.9585065647457423, [70.78651685393258]], ['phase1', 'last', 1, 'UJ_ARABIC_angry', 1.5495183548102012, [61.53846153846154]], ['phase1', 'last', 1, 'UJ_ARABIC_happy', 0.4166577458381653, [83.33333333333333]], ['phase1', 'last', 1, 'UJ_ARABIC_neutral', 0.6069116996867316, [85.71428571428571]], ['phase1', 'last', 1, 'UJ_ARABIC_sad', 2.3494340351649696, [0.0]], ['phase1', 'last', 1, 'URDU_angry', 0.18395565971732136, [100.0]], ['phase1', 'last', 1, 'URDU_happy', 0.3975479431450366, [90.0]], ['phase1', 'last', 1, 'URDU_neutral', 0.2688737452030182, [95.0]], ['phase1', 'last', 1, 'URDU_sad', 0.3800719514489175, [90.0]], ['phase2', 'last', 1, 'CREMA-D_angry', 0.2906135099492672, [93.30708661417323]], ['phase2', 'last', 1, 'CREMA-D_happy', 0.2367331867260258, [96.06299212598425]], ['phase2', 'last', 1, 'CREMA-D_neutral', 0.2552252822482642, [96.31336405529954]], ['phase2', 'last', 1, 'CREMA-D_sad', 0.6159147716999991, [81.10236220472441]], ['phase2', 'last', 1, 'SUBESCO_angry', 0.3666904728859662, [91.0]], ['phase2', 'last', 1, 'SUBESCO_happy', 0.30799106523394587, [92.5]], ['phase2', 'last', 1, 'SUBESCO_neutral', 0.3148298836499454, [93.0]], ['phase2', 'last', 1, 'SUBESCO_sad', 0.8127771169692279, [76.5]], ['phase2', 'last', 1, 'ShEMO_angry', 0.20459069615291772, [98.10426540284361]], ['phase2', 'last', 1, 'ShEMO_happy', 0.34735414050519464, [87.5]], ['phase2', 'last', 1, 'ShEMO_neutral', 0.2300839459750711, [96.58536585365853]], ['phase2', 'last', 1, 'ShEMO_sad', 0.9299659379077759, [71.91011235955057]], ['phase2', 'last', 1, 'UJ_ARABIC_angry', 1.6147538813260884, [61.53846153846154]], ['phase2', 'last', 1, 'UJ_ARABIC_happy', 0.4460348735253016, [100.0]], ['phase2', 'last', 1, 'UJ_ARABIC_neutral', 0.584535104887826, [85.71428571428571]], ['phase2', 'last', 1, 'UJ_ARABIC_sad', 2.1865100519997736, [0.0]], ['phase2', 'last', 1, 'URDU_angry', 0.19664481729269026, [95.0]], ['phase2', 'last', 1, 'URDU_happy', 0.4312362901866436, [90.0]], ['phase2', 'last', 1, 'URDU_neutral', 0.28115319460630417, [95.0]], ['phase2', 'last', 1, 'URDU_sad', 0.37026820853352543, [95.0]]]\n",
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 20 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.81it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.56it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 62.42it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.22it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.37it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.82it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 53.06it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 39.40it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 48.65it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:02<00:00, 43.56it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 27.09it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 14.29it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.16it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.46it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.56it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 1 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 63.80it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 63.42it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 63.70it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.63it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.61it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.04it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.06it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.04it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 55.64it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 41.48it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 45.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 46.88it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 27.15it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 14.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.84it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.93it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.62it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.46it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 62.81it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 62.88it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.33it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.73it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.97it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 56.16it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 42.04it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:03<00:00, 52.99it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 47.33it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 26.79it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 14.41it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.24it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.57it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.42it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.44it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 10 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.86it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.76it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 64.94it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.83it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.80it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.42it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.77it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.15it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 56.13it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 42.82it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:03<00:00, 53.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 48.31it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.38it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 14.71it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.92it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 17.26it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.95it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.84it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.99it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['phase1', 'best', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'best', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'best', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'best', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'best', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'best', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'best', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'best', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'best', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'best', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'best', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'best', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'best', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'best', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'best', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'best', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'best', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'best', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'best', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'best', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'best', 0, 'CREMA-D_angry', 0.258736290096298, [94.09448818897638]], ['phase2', 'best', 0, 'CREMA-D_happy', 0.3128238437095964, [93.7007874015748]], ['phase2', 'best', 0, 'CREMA-D_neutral', 0.2521394816686482, [96.31336405529954]], ['phase2', 'best', 0, 'CREMA-D_sad', 0.6713012946168266, [77.55905511811024]], ['phase2', 'best', 0, 'SUBESCO_angry', 0.25672499284148226, [95.0]], ['phase2', 'best', 0, 'SUBESCO_happy', 0.3820594840496778, [92.0]], ['phase2', 'best', 0, 'SUBESCO_neutral', 0.23552342765033246, [95.5]], ['phase2', 'best', 0, 'SUBESCO_sad', 0.9597138513624667, [67.5]], ['phase2', 'best', 0, 'ShEMO_angry', 0.22040177592169052, [96.6824644549763]], ['phase2', 'best', 0, 'ShEMO_happy', 0.32229302674531946, [95.0]], ['phase2', 'best', 0, 'ShEMO_neutral', 0.2571009743504408, [95.1219512195122]], ['phase2', 'best', 0, 'ShEMO_sad', 1.2437129648548833, [68.53932584269663]], ['phase2', 'best', 0, 'UJ_ARABIC_angry', 0.7015116925422962, [69.23076923076923]], ['phase2', 'best', 0, 'UJ_ARABIC_happy', 0.6391726980606716, [83.33333333333333]], ['phase2', 'best', 0, 'UJ_ARABIC_neutral', 0.7040895244904927, [57.142857142857146]], ['phase2', 'best', 0, 'UJ_ARABIC_sad', 1.8073663839272092, [28.571428571428573]], ['phase2', 'best', 0, 'URDU_angry', 0.14747261628508565, [100.0]], ['phase2', 'best', 0, 'URDU_happy', 0.45416179522871986, [85.0]], ['phase2', 'best', 0, 'URDU_neutral', 0.4689487174153329, [85.0]], ['phase2', 'best', 0, 'URDU_sad', 0.4275880873203277, [95.0]], ['phase1', 'last', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'last', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'last', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'last', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'last', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'last', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'last', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'last', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'last', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'last', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'last', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'last', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'last', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'last', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'last', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'last', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'last', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'last', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'last', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'last', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'last', 0, 'CREMA-D_angry', 0.2588579188298993, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_happy', 0.3136730649339872, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_neutral', 0.25121473428291113, [96.31336405529954]], ['phase2', 'last', 0, 'CREMA-D_sad', 0.6725248215118728, [77.55905511811024]], ['phase2', 'last', 0, 'SUBESCO_angry', 0.25600903317332274, [95.0]], ['phase2', 'last', 0, 'SUBESCO_happy', 0.3834911134093998, [92.0]], ['phase2', 'last', 0, 'SUBESCO_neutral', 0.23549174077808857, [95.5]], ['phase2', 'last', 0, 'SUBESCO_sad', 0.9606494718790054, [67.0]], ['phase2', 'last', 0, 'ShEMO_angry', 0.22003566039399522, [97.1563981042654]], ['phase2', 'last', 0, 'ShEMO_happy', 0.32309657931327823, [95.0]], ['phase2', 'last', 0, 'ShEMO_neutral', 0.2568363875877568, [95.1219512195122]], ['phase2', 'last', 0, 'ShEMO_sad', 1.2490537643767476, [66.29213483146067]], ['phase2', 'last', 0, 'UJ_ARABIC_angry', 0.6984921602102426, [69.23076923076923]], ['phase2', 'last', 0, 'UJ_ARABIC_happy', 0.6390940000613531, [83.33333333333333]], ['phase2', 'last', 0, 'UJ_ARABIC_neutral', 0.7022053365196501, [57.142857142857146]], ['phase2', 'last', 0, 'UJ_ARABIC_sad', 1.813152142933437, [28.571428571428573]], ['phase2', 'last', 0, 'URDU_angry', 0.14700243920087813, [100.0]], ['phase2', 'last', 0, 'URDU_happy', 0.4553044453263283, [85.0]], ['phase2', 'last', 0, 'URDU_neutral', 0.4675500065088273, [85.0]], ['phase2', 'last', 0, 'URDU_sad', 0.42905081287026403, [95.0]], ['phase1', 'best', 1, 'CREMA-D_angry', 0.27615764127002945, [94.88188976377953]], ['phase1', 'best', 1, 'CREMA-D_happy', 0.23527879508461533, [95.66929133858268]], ['phase1', 'best', 1, 'CREMA-D_neutral', 0.25794078073193955, [96.7741935483871]], ['phase1', 'best', 1, 'CREMA-D_sad', 0.6431511204660413, [79.5275590551181]], ['phase1', 'best', 1, 'SUBESCO_angry', 0.3505234102904796, [91.0]], ['phase1', 'best', 1, 'SUBESCO_happy', 0.30056495204567907, [91.5]], ['phase1', 'best', 1, 'SUBESCO_neutral', 0.32848145671188816, [92.0]], ['phase1', 'best', 1, 'SUBESCO_sad', 0.8100879185646771, [77.0]], ['phase1', 'best', 1, 'ShEMO_angry', 0.20217314553204307, [97.6303317535545]], ['phase1', 'best', 1, 'ShEMO_happy', 0.33687370978295794, [87.5]], ['phase1', 'best', 1, 'ShEMO_neutral', 0.2413041352498823, [96.09756097560975]], ['phase1', 'best', 1, 'ShEMO_sad', 0.9465443391478462, [69.66292134831461]], ['phase1', 'best', 1, 'UJ_ARABIC_angry', 1.5619367189132252, [61.53846153846154]], ['phase1', 'best', 1, 'UJ_ARABIC_happy', 0.4149790306886037, [83.33333333333333]], ['phase1', 'best', 1, 'UJ_ARABIC_neutral', 0.6496904577527727, [85.71428571428571]], ['phase1', 'best', 1, 'UJ_ARABIC_sad', 2.2628096171787804, [0.0]], ['phase1', 'best', 1, 'URDU_angry', 0.18128371462225912, [100.0]], ['phase1', 'best', 1, 'URDU_happy', 0.40306554064154626, [90.0]], ['phase1', 'best', 1, 'URDU_neutral', 0.26644408404827113, [95.0]], ['phase1', 'best', 1, 'URDU_sad', 0.36069156974554056, [95.0]], ['phase2', 'best', 1, 'CREMA-D_angry', 0.29083810124810267, [93.30708661417323]], ['phase2', 'best', 1, 'CREMA-D_happy', 0.23703179144718511, [96.06299212598425]], ['phase2', 'best', 1, 'CREMA-D_neutral', 0.2536086365237215, [96.31336405529954]], ['phase2', 'best', 1, 'CREMA-D_sad', 0.6164421358798431, [81.10236220472441]], ['phase2', 'best', 1, 'SUBESCO_angry', 0.3682341238856316, [91.0]], ['phase2', 'best', 1, 'SUBESCO_happy', 0.3087896878272295, [92.5]], ['phase2', 'best', 1, 'SUBESCO_neutral', 0.3148857557773592, [93.0]], ['phase2', 'best', 1, 'SUBESCO_sad', 0.8102790326625107, [76.5]], ['phase2', 'best', 1, 'ShEMO_angry', 0.20505267777149144, [98.10426540284361]], ['phase2', 'best', 1, 'ShEMO_happy', 0.3488592237234116, [87.5]], ['phase2', 'best', 1, 'ShEMO_neutral', 0.22883073178733276, [96.58536585365853]], ['phase2', 'best', 1, 'ShEMO_sad', 0.928263324365187, [71.91011235955057]], ['phase2', 'best', 1, 'UJ_ARABIC_angry', 1.614923622745734, [61.53846153846154]], ['phase2', 'best', 1, 'UJ_ARABIC_happy', 0.4457362964749336, [100.0]], ['phase2', 'best', 1, 'UJ_ARABIC_neutral', 0.5752684026956558, [85.71428571428571]], ['phase2', 'best', 1, 'UJ_ARABIC_sad', 2.1905118397303993, [0.0]], ['phase2', 'best', 1, 'URDU_angry', 0.19813046157360079, [95.0]], ['phase2', 'best', 1, 'URDU_happy', 0.432581639289856, [90.0]], ['phase2', 'best', 1, 'URDU_neutral', 0.27857217714190496, [95.0]], ['phase2', 'best', 1, 'URDU_sad', 0.3698753505945205, [95.0]], ['phase1', 'last', 1, 'CREMA-D_angry', 0.2761957376726031, [94.09448818897638]], ['phase1', 'last', 1, 'CREMA-D_happy', 0.23440244113366435, [94.88188976377953]], ['phase1', 'last', 1, 'CREMA-D_neutral', 0.2563687742168453, [96.31336405529954]], ['phase1', 'last', 1, 'CREMA-D_sad', 0.6558136834990316, [79.13385826771653]], ['phase1', 'last', 1, 'SUBESCO_angry', 0.3387189420312643, [91.5]], ['phase1', 'last', 1, 'SUBESCO_happy', 0.30185804873704913, [93.0]], ['phase1', 'last', 1, 'SUBESCO_neutral', 0.325886485427618, [92.0]], ['phase1', 'last', 1, 'SUBESCO_sad', 0.8205169849097728, [76.5]], ['phase1', 'last', 1, 'ShEMO_angry', 0.19906430384276588, [98.10426540284361]], ['phase1', 'last', 1, 'ShEMO_happy', 0.343780105188489, [87.5]], ['phase1', 'last', 1, 'ShEMO_neutral', 0.2343073303379663, [96.09756097560975]], ['phase1', 'last', 1, 'ShEMO_sad', 0.9585065647457423, [70.78651685393258]], ['phase1', 'last', 1, 'UJ_ARABIC_angry', 1.5495183548102012, [61.53846153846154]], ['phase1', 'last', 1, 'UJ_ARABIC_happy', 0.4166577458381653, [83.33333333333333]], ['phase1', 'last', 1, 'UJ_ARABIC_neutral', 0.6069116996867316, [85.71428571428571]], ['phase1', 'last', 1, 'UJ_ARABIC_sad', 2.3494340351649696, [0.0]], ['phase1', 'last', 1, 'URDU_angry', 0.18395565971732136, [100.0]], ['phase1', 'last', 1, 'URDU_happy', 0.3975479431450366, [90.0]], ['phase1', 'last', 1, 'URDU_neutral', 0.2688737452030182, [95.0]], ['phase1', 'last', 1, 'URDU_sad', 0.3800719514489175, [90.0]], ['phase2', 'last', 1, 'CREMA-D_angry', 0.2906135099492672, [93.30708661417323]], ['phase2', 'last', 1, 'CREMA-D_happy', 0.2367331867260258, [96.06299212598425]], ['phase2', 'last', 1, 'CREMA-D_neutral', 0.2552252822482642, [96.31336405529954]], ['phase2', 'last', 1, 'CREMA-D_sad', 0.6159147716999991, [81.10236220472441]], ['phase2', 'last', 1, 'SUBESCO_angry', 0.3666904728859662, [91.0]], ['phase2', 'last', 1, 'SUBESCO_happy', 0.30799106523394587, [92.5]], ['phase2', 'last', 1, 'SUBESCO_neutral', 0.3148298836499454, [93.0]], ['phase2', 'last', 1, 'SUBESCO_sad', 0.8127771169692279, [76.5]], ['phase2', 'last', 1, 'ShEMO_angry', 0.20459069615291772, [98.10426540284361]], ['phase2', 'last', 1, 'ShEMO_happy', 0.34735414050519464, [87.5]], ['phase2', 'last', 1, 'ShEMO_neutral', 0.2300839459750711, [96.58536585365853]], ['phase2', 'last', 1, 'ShEMO_sad', 0.9299659379077759, [71.91011235955057]], ['phase2', 'last', 1, 'UJ_ARABIC_angry', 1.6147538813260884, [61.53846153846154]], ['phase2', 'last', 1, 'UJ_ARABIC_happy', 0.4460348735253016, [100.0]], ['phase2', 'last', 1, 'UJ_ARABIC_neutral', 0.584535104887826, [85.71428571428571]], ['phase2', 'last', 1, 'UJ_ARABIC_sad', 2.1865100519997736, [0.0]], ['phase2', 'last', 1, 'URDU_angry', 0.19664481729269026, [95.0]], ['phase2', 'last', 1, 'URDU_happy', 0.4312362901866436, [90.0]], ['phase2', 'last', 1, 'URDU_neutral', 0.28115319460630417, [95.0]], ['phase2', 'last', 1, 'URDU_sad', 0.37026820853352543, [95.0]], ['phase1', 'best', 2, 'CREMA-D_angry', 0.2533235727684707, [93.7007874015748]], ['phase1', 'best', 2, 'CREMA-D_happy', 0.2530228722048557, [95.2755905511811]], ['phase1', 'best', 2, 'CREMA-D_neutral', 0.23457274898406, [96.7741935483871]], ['phase1', 'best', 2, 'CREMA-D_sad', 0.7033744433264096, [74.80314960629921]], ['phase1', 'best', 2, 'SUBESCO_angry', 0.28369085997343063, [91.5]], ['phase1', 'best', 2, 'SUBESCO_happy', 0.354978576526046, [91.0]], ['phase1', 'best', 2, 'SUBESCO_neutral', 0.29951987594366075, [94.0]], ['phase1', 'best', 2, 'SUBESCO_sad', 0.8211694458872081, [76.0]], ['phase1', 'best', 2, 'ShEMO_angry', 0.22963467148525457, [94.7867298578199]], ['phase1', 'best', 2, 'ShEMO_happy', 0.37117515616118907, [87.5]], ['phase1', 'best', 2, 'ShEMO_neutral', 0.2645087184702479, [95.1219512195122]], ['phase1', 'best', 2, 'ShEMO_sad', 1.1678407711259435, [66.29213483146067]], ['phase1', 'best', 2, 'UJ_ARABIC_angry', 0.8589285772580367, [69.23076923076923]], ['phase1', 'best', 2, 'UJ_ARABIC_happy', 1.1219921261072159, [50.0]], ['phase1', 'best', 2, 'UJ_ARABIC_neutral', 0.6927995277302607, [71.42857142857143]], ['phase1', 'best', 2, 'UJ_ARABIC_sad', 2.4228394670145854, [14.285714285714286]], ['phase1', 'best', 2, 'URDU_angry', 0.13714252263307572, [100.0]], ['phase1', 'best', 2, 'URDU_happy', 0.5723968669772149, [85.0]], ['phase1', 'best', 2, 'URDU_neutral', 0.37807183638215064, [95.0]], ['phase1', 'best', 2, 'URDU_sad', 0.5459306403994562, [85.0]], ['phase2', 'best', 2, 'CREMA-D_angry', 0.26008654121808183, [93.30708661417323]], ['phase2', 'best', 2, 'CREMA-D_happy', 0.27641784440814043, [94.48818897637796]], ['phase2', 'best', 2, 'CREMA-D_neutral', 0.23941651906835326, [96.31336405529954]], ['phase2', 'best', 2, 'CREMA-D_sad', 0.6129553528634583, [78.74015748031496]], ['phase2', 'best', 2, 'SUBESCO_angry', 0.28470431238412836, [92.5]], ['phase2', 'best', 2, 'SUBESCO_happy', 0.37439083494246017, [91.0]], ['phase2', 'best', 2, 'SUBESCO_neutral', 0.27766273468732844, [94.5]], ['phase2', 'best', 2, 'SUBESCO_sad', 0.8193467020243407, [75.0]], ['phase2', 'best', 2, 'ShEMO_angry', 0.25258563324738453, [94.7867298578199]], ['phase2', 'best', 2, 'ShEMO_happy', 0.42693374641239645, [85.0]], ['phase2', 'best', 2, 'ShEMO_neutral', 0.24538376142338997, [96.58536585365853]], ['phase2', 'best', 2, 'ShEMO_sad', 1.1382430495505926, [67.41573033707866]], ['phase2', 'best', 2, 'UJ_ARABIC_angry', 0.8296753810002254, [69.23076923076923]], ['phase2', 'best', 2, 'UJ_ARABIC_happy', 1.3359356770912805, [16.666666666666668]], ['phase2', 'best', 2, 'UJ_ARABIC_neutral', 0.6173683766807828, [71.42857142857143]], ['phase2', 'best', 2, 'UJ_ARABIC_sad', 2.1588183109249384, [14.285714285714286]], ['phase2', 'best', 2, 'URDU_angry', 0.1395458072423935, [100.0]], ['phase2', 'best', 2, 'URDU_happy', 0.6497519172728062, [85.0]], ['phase2', 'best', 2, 'URDU_neutral', 0.3505798734724522, [90.0]], ['phase2', 'best', 2, 'URDU_sad', 0.5304099045693875, [85.0]], ['phase1', 'last', 2, 'CREMA-D_angry', 0.24984035330025234, [93.7007874015748]], ['phase1', 'last', 2, 'CREMA-D_happy', 0.26827800320828044, [94.48818897637796]], ['phase1', 'last', 2, 'CREMA-D_neutral', 0.22523403277594906, [96.7741935483871]], ['phase1', 'last', 2, 'CREMA-D_sad', 0.7022455512421336, [74.40944881889764]], ['phase1', 'last', 2, 'SUBESCO_angry', 0.27640943221747877, [92.5]], ['phase1', 'last', 2, 'SUBESCO_happy', 0.3622385337948799, [91.5]], ['phase1', 'last', 2, 'SUBESCO_neutral', 0.2780804342031481, [94.0]], ['phase1', 'last', 2, 'SUBESCO_sad', 0.8708563620597127, [72.0]], ['phase1', 'last', 2, 'ShEMO_angry', 0.23695080533129362, [95.260663507109]], ['phase1', 'last', 2, 'ShEMO_happy', 0.4093896470963954, [87.5]], ['phase1', 'last', 2, 'ShEMO_neutral', 0.25657484931189845, [96.09756097560975]], ['phase1', 'last', 2, 'ShEMO_sad', 1.2044240768705867, [64.04494382022472]], ['phase1', 'last', 2, 'UJ_ARABIC_angry', 0.822608069731639, [69.23076923076923]], ['phase1', 'last', 2, 'UJ_ARABIC_happy', 1.2717645491162937, [33.333333333333336]], ['phase1', 'last', 2, 'UJ_ARABIC_neutral', 0.6440192397151675, [71.42857142857143]], ['phase1', 'last', 2, 'UJ_ARABIC_sad', 2.438488530261176, [14.285714285714286]], ['phase1', 'last', 2, 'URDU_angry', 0.13764138370752335, [100.0]], ['phase1', 'last', 2, 'URDU_happy', 0.6141036458313465, [85.0]], ['phase1', 'last', 2, 'URDU_neutral', 0.3679290011525155, [95.0]], ['phase1', 'last', 2, 'URDU_sad', 0.5879364408552648, [85.0]], ['phase2', 'last', 2, 'CREMA-D_angry', 0.26902664311992847, [92.91338582677166]], ['phase2', 'last', 2, 'CREMA-D_happy', 0.2753782981024015, [94.09448818897638]], ['phase2', 'last', 2, 'CREMA-D_neutral', 0.2353055673535518, [96.7741935483871]], ['phase2', 'last', 2, 'CREMA-D_sad', 0.6381505948352062, [76.77165354330708]], ['phase2', 'last', 2, 'SUBESCO_angry', 0.28951828539371477, [93.0]], ['phase2', 'last', 2, 'SUBESCO_happy', 0.3764164911955594, [91.0]], ['phase2', 'last', 2, 'SUBESCO_neutral', 0.2734559410065411, [94.5]], ['phase2', 'last', 2, 'SUBESCO_sad', 0.8367165967077014, [75.0]], ['phase2', 'last', 2, 'ShEMO_angry', 0.2624106809002526, [94.31279620853081]], ['phase2', 'last', 2, 'ShEMO_happy', 0.4383957624435425, [85.0]], ['phase2', 'last', 2, 'ShEMO_neutral', 0.24030565939298504, [96.58536585365853]], ['phase2', 'last', 2, 'ShEMO_sad', 1.1727998432483562, [66.29213483146067]], ['phase2', 'last', 2, 'UJ_ARABIC_angry', 0.8413258343935013, [69.23076923076923]], ['phase2', 'last', 2, 'UJ_ARABIC_happy', 1.3033907612164815, [33.333333333333336]], ['phase2', 'last', 2, 'UJ_ARABIC_neutral', 0.6183227258069175, [71.42857142857143]], ['phase2', 'last', 2, 'UJ_ARABIC_sad', 2.216776783977236, [14.285714285714286]], ['phase2', 'last', 2, 'URDU_angry', 0.1392757534980774, [100.0]], ['phase2', 'last', 2, 'URDU_happy', 0.6227635897696018, [85.0]], ['phase2', 'last', 2, 'URDU_neutral', 0.33118396773934367, [90.0]], ['phase2', 'last', 2, 'URDU_sad', 0.5625272803008556, [75.0]]]\n",
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 24 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.18it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.51it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 64.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.45it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.30it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.43it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.05it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 51.97it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 39.39it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 45.59it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 47.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 24.94it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.46it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 14.69it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 17.86it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.02it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.36it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.09it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 3 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.99it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.64it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.27it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.85it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.77it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.29it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 52.82it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 50.64it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 39.18it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 45.27it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 48.61it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 27.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 14.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 17.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.49it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.50it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.04it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.95it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.66it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.93it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.29it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.37it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.10it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.73it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 54.17it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 39.52it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 47.13it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 49.10it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 27.88it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 17.15it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.06it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 18.11it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.82it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.68it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 35.35it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 10 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.14it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.49it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.69it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.35it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.35it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.67it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 53.96it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 40.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 47.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 48.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 26.93it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 16.16it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.22it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 17.91it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.30it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.75it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.78it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['phase1', 'best', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'best', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'best', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'best', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'best', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'best', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'best', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'best', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'best', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'best', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'best', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'best', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'best', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'best', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'best', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'best', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'best', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'best', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'best', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'best', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'best', 0, 'CREMA-D_angry', 0.258736290096298, [94.09448818897638]], ['phase2', 'best', 0, 'CREMA-D_happy', 0.3128238437095964, [93.7007874015748]], ['phase2', 'best', 0, 'CREMA-D_neutral', 0.2521394816686482, [96.31336405529954]], ['phase2', 'best', 0, 'CREMA-D_sad', 0.6713012946168266, [77.55905511811024]], ['phase2', 'best', 0, 'SUBESCO_angry', 0.25672499284148226, [95.0]], ['phase2', 'best', 0, 'SUBESCO_happy', 0.3820594840496778, [92.0]], ['phase2', 'best', 0, 'SUBESCO_neutral', 0.23552342765033246, [95.5]], ['phase2', 'best', 0, 'SUBESCO_sad', 0.9597138513624667, [67.5]], ['phase2', 'best', 0, 'ShEMO_angry', 0.22040177592169052, [96.6824644549763]], ['phase2', 'best', 0, 'ShEMO_happy', 0.32229302674531946, [95.0]], ['phase2', 'best', 0, 'ShEMO_neutral', 0.2571009743504408, [95.1219512195122]], ['phase2', 'best', 0, 'ShEMO_sad', 1.2437129648548833, [68.53932584269663]], ['phase2', 'best', 0, 'UJ_ARABIC_angry', 0.7015116925422962, [69.23076923076923]], ['phase2', 'best', 0, 'UJ_ARABIC_happy', 0.6391726980606716, [83.33333333333333]], ['phase2', 'best', 0, 'UJ_ARABIC_neutral', 0.7040895244904927, [57.142857142857146]], ['phase2', 'best', 0, 'UJ_ARABIC_sad', 1.8073663839272092, [28.571428571428573]], ['phase2', 'best', 0, 'URDU_angry', 0.14747261628508565, [100.0]], ['phase2', 'best', 0, 'URDU_happy', 0.45416179522871986, [85.0]], ['phase2', 'best', 0, 'URDU_neutral', 0.4689487174153329, [85.0]], ['phase2', 'best', 0, 'URDU_sad', 0.4275880873203277, [95.0]], ['phase1', 'last', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'last', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'last', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'last', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'last', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'last', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'last', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'last', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'last', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'last', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'last', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'last', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'last', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'last', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'last', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'last', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'last', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'last', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'last', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'last', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'last', 0, 'CREMA-D_angry', 0.2588579188298993, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_happy', 0.3136730649339872, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_neutral', 0.25121473428291113, [96.31336405529954]], ['phase2', 'last', 0, 'CREMA-D_sad', 0.6725248215118728, [77.55905511811024]], ['phase2', 'last', 0, 'SUBESCO_angry', 0.25600903317332274, [95.0]], ['phase2', 'last', 0, 'SUBESCO_happy', 0.3834911134093998, [92.0]], ['phase2', 'last', 0, 'SUBESCO_neutral', 0.23549174077808857, [95.5]], ['phase2', 'last', 0, 'SUBESCO_sad', 0.9606494718790054, [67.0]], ['phase2', 'last', 0, 'ShEMO_angry', 0.22003566039399522, [97.1563981042654]], ['phase2', 'last', 0, 'ShEMO_happy', 0.32309657931327823, [95.0]], ['phase2', 'last', 0, 'ShEMO_neutral', 0.2568363875877568, [95.1219512195122]], ['phase2', 'last', 0, 'ShEMO_sad', 1.2490537643767476, [66.29213483146067]], ['phase2', 'last', 0, 'UJ_ARABIC_angry', 0.6984921602102426, [69.23076923076923]], ['phase2', 'last', 0, 'UJ_ARABIC_happy', 0.6390940000613531, [83.33333333333333]], ['phase2', 'last', 0, 'UJ_ARABIC_neutral', 0.7022053365196501, [57.142857142857146]], ['phase2', 'last', 0, 'UJ_ARABIC_sad', 1.813152142933437, [28.571428571428573]], ['phase2', 'last', 0, 'URDU_angry', 0.14700243920087813, [100.0]], ['phase2', 'last', 0, 'URDU_happy', 0.4553044453263283, [85.0]], ['phase2', 'last', 0, 'URDU_neutral', 0.4675500065088273, [85.0]], ['phase2', 'last', 0, 'URDU_sad', 0.42905081287026403, [95.0]], ['phase1', 'best', 1, 'CREMA-D_angry', 0.27615764127002945, [94.88188976377953]], ['phase1', 'best', 1, 'CREMA-D_happy', 0.23527879508461533, [95.66929133858268]], ['phase1', 'best', 1, 'CREMA-D_neutral', 0.25794078073193955, [96.7741935483871]], ['phase1', 'best', 1, 'CREMA-D_sad', 0.6431511204660413, [79.5275590551181]], ['phase1', 'best', 1, 'SUBESCO_angry', 0.3505234102904796, [91.0]], ['phase1', 'best', 1, 'SUBESCO_happy', 0.30056495204567907, [91.5]], ['phase1', 'best', 1, 'SUBESCO_neutral', 0.32848145671188816, [92.0]], ['phase1', 'best', 1, 'SUBESCO_sad', 0.8100879185646771, [77.0]], ['phase1', 'best', 1, 'ShEMO_angry', 0.20217314553204307, [97.6303317535545]], ['phase1', 'best', 1, 'ShEMO_happy', 0.33687370978295794, [87.5]], ['phase1', 'best', 1, 'ShEMO_neutral', 0.2413041352498823, [96.09756097560975]], ['phase1', 'best', 1, 'ShEMO_sad', 0.9465443391478462, [69.66292134831461]], ['phase1', 'best', 1, 'UJ_ARABIC_angry', 1.5619367189132252, [61.53846153846154]], ['phase1', 'best', 1, 'UJ_ARABIC_happy', 0.4149790306886037, [83.33333333333333]], ['phase1', 'best', 1, 'UJ_ARABIC_neutral', 0.6496904577527727, [85.71428571428571]], ['phase1', 'best', 1, 'UJ_ARABIC_sad', 2.2628096171787804, [0.0]], ['phase1', 'best', 1, 'URDU_angry', 0.18128371462225912, [100.0]], ['phase1', 'best', 1, 'URDU_happy', 0.40306554064154626, [90.0]], ['phase1', 'best', 1, 'URDU_neutral', 0.26644408404827113, [95.0]], ['phase1', 'best', 1, 'URDU_sad', 0.36069156974554056, [95.0]], ['phase2', 'best', 1, 'CREMA-D_angry', 0.29083810124810267, [93.30708661417323]], ['phase2', 'best', 1, 'CREMA-D_happy', 0.23703179144718511, [96.06299212598425]], ['phase2', 'best', 1, 'CREMA-D_neutral', 0.2536086365237215, [96.31336405529954]], ['phase2', 'best', 1, 'CREMA-D_sad', 0.6164421358798431, [81.10236220472441]], ['phase2', 'best', 1, 'SUBESCO_angry', 0.3682341238856316, [91.0]], ['phase2', 'best', 1, 'SUBESCO_happy', 0.3087896878272295, [92.5]], ['phase2', 'best', 1, 'SUBESCO_neutral', 0.3148857557773592, [93.0]], ['phase2', 'best', 1, 'SUBESCO_sad', 0.8102790326625107, [76.5]], ['phase2', 'best', 1, 'ShEMO_angry', 0.20505267777149144, [98.10426540284361]], ['phase2', 'best', 1, 'ShEMO_happy', 0.3488592237234116, [87.5]], ['phase2', 'best', 1, 'ShEMO_neutral', 0.22883073178733276, [96.58536585365853]], ['phase2', 'best', 1, 'ShEMO_sad', 0.928263324365187, [71.91011235955057]], ['phase2', 'best', 1, 'UJ_ARABIC_angry', 1.614923622745734, [61.53846153846154]], ['phase2', 'best', 1, 'UJ_ARABIC_happy', 0.4457362964749336, [100.0]], ['phase2', 'best', 1, 'UJ_ARABIC_neutral', 0.5752684026956558, [85.71428571428571]], ['phase2', 'best', 1, 'UJ_ARABIC_sad', 2.1905118397303993, [0.0]], ['phase2', 'best', 1, 'URDU_angry', 0.19813046157360079, [95.0]], ['phase2', 'best', 1, 'URDU_happy', 0.432581639289856, [90.0]], ['phase2', 'best', 1, 'URDU_neutral', 0.27857217714190496, [95.0]], ['phase2', 'best', 1, 'URDU_sad', 0.3698753505945205, [95.0]], ['phase1', 'last', 1, 'CREMA-D_angry', 0.2761957376726031, [94.09448818897638]], ['phase1', 'last', 1, 'CREMA-D_happy', 0.23440244113366435, [94.88188976377953]], ['phase1', 'last', 1, 'CREMA-D_neutral', 0.2563687742168453, [96.31336405529954]], ['phase1', 'last', 1, 'CREMA-D_sad', 0.6558136834990316, [79.13385826771653]], ['phase1', 'last', 1, 'SUBESCO_angry', 0.3387189420312643, [91.5]], ['phase1', 'last', 1, 'SUBESCO_happy', 0.30185804873704913, [93.0]], ['phase1', 'last', 1, 'SUBESCO_neutral', 0.325886485427618, [92.0]], ['phase1', 'last', 1, 'SUBESCO_sad', 0.8205169849097728, [76.5]], ['phase1', 'last', 1, 'ShEMO_angry', 0.19906430384276588, [98.10426540284361]], ['phase1', 'last', 1, 'ShEMO_happy', 0.343780105188489, [87.5]], ['phase1', 'last', 1, 'ShEMO_neutral', 0.2343073303379663, [96.09756097560975]], ['phase1', 'last', 1, 'ShEMO_sad', 0.9585065647457423, [70.78651685393258]], ['phase1', 'last', 1, 'UJ_ARABIC_angry', 1.5495183548102012, [61.53846153846154]], ['phase1', 'last', 1, 'UJ_ARABIC_happy', 0.4166577458381653, [83.33333333333333]], ['phase1', 'last', 1, 'UJ_ARABIC_neutral', 0.6069116996867316, [85.71428571428571]], ['phase1', 'last', 1, 'UJ_ARABIC_sad', 2.3494340351649696, [0.0]], ['phase1', 'last', 1, 'URDU_angry', 0.18395565971732136, [100.0]], ['phase1', 'last', 1, 'URDU_happy', 0.3975479431450366, [90.0]], ['phase1', 'last', 1, 'URDU_neutral', 0.2688737452030182, [95.0]], ['phase1', 'last', 1, 'URDU_sad', 0.3800719514489175, [90.0]], ['phase2', 'last', 1, 'CREMA-D_angry', 0.2906135099492672, [93.30708661417323]], ['phase2', 'last', 1, 'CREMA-D_happy', 0.2367331867260258, [96.06299212598425]], ['phase2', 'last', 1, 'CREMA-D_neutral', 0.2552252822482642, [96.31336405529954]], ['phase2', 'last', 1, 'CREMA-D_sad', 0.6159147716999991, [81.10236220472441]], ['phase2', 'last', 1, 'SUBESCO_angry', 0.3666904728859662, [91.0]], ['phase2', 'last', 1, 'SUBESCO_happy', 0.30799106523394587, [92.5]], ['phase2', 'last', 1, 'SUBESCO_neutral', 0.3148298836499454, [93.0]], ['phase2', 'last', 1, 'SUBESCO_sad', 0.8127771169692279, [76.5]], ['phase2', 'last', 1, 'ShEMO_angry', 0.20459069615291772, [98.10426540284361]], ['phase2', 'last', 1, 'ShEMO_happy', 0.34735414050519464, [87.5]], ['phase2', 'last', 1, 'ShEMO_neutral', 0.2300839459750711, [96.58536585365853]], ['phase2', 'last', 1, 'ShEMO_sad', 0.9299659379077759, [71.91011235955057]], ['phase2', 'last', 1, 'UJ_ARABIC_angry', 1.6147538813260884, [61.53846153846154]], ['phase2', 'last', 1, 'UJ_ARABIC_happy', 0.4460348735253016, [100.0]], ['phase2', 'last', 1, 'UJ_ARABIC_neutral', 0.584535104887826, [85.71428571428571]], ['phase2', 'last', 1, 'UJ_ARABIC_sad', 2.1865100519997736, [0.0]], ['phase2', 'last', 1, 'URDU_angry', 0.19664481729269026, [95.0]], ['phase2', 'last', 1, 'URDU_happy', 0.4312362901866436, [90.0]], ['phase2', 'last', 1, 'URDU_neutral', 0.28115319460630417, [95.0]], ['phase2', 'last', 1, 'URDU_sad', 0.37026820853352543, [95.0]], ['phase1', 'best', 2, 'CREMA-D_angry', 0.2533235727684707, [93.7007874015748]], ['phase1', 'best', 2, 'CREMA-D_happy', 0.2530228722048557, [95.2755905511811]], ['phase1', 'best', 2, 'CREMA-D_neutral', 0.23457274898406, [96.7741935483871]], ['phase1', 'best', 2, 'CREMA-D_sad', 0.7033744433264096, [74.80314960629921]], ['phase1', 'best', 2, 'SUBESCO_angry', 0.28369085997343063, [91.5]], ['phase1', 'best', 2, 'SUBESCO_happy', 0.354978576526046, [91.0]], ['phase1', 'best', 2, 'SUBESCO_neutral', 0.29951987594366075, [94.0]], ['phase1', 'best', 2, 'SUBESCO_sad', 0.8211694458872081, [76.0]], ['phase1', 'best', 2, 'ShEMO_angry', 0.22963467148525457, [94.7867298578199]], ['phase1', 'best', 2, 'ShEMO_happy', 0.37117515616118907, [87.5]], ['phase1', 'best', 2, 'ShEMO_neutral', 0.2645087184702479, [95.1219512195122]], ['phase1', 'best', 2, 'ShEMO_sad', 1.1678407711259435, [66.29213483146067]], ['phase1', 'best', 2, 'UJ_ARABIC_angry', 0.8589285772580367, [69.23076923076923]], ['phase1', 'best', 2, 'UJ_ARABIC_happy', 1.1219921261072159, [50.0]], ['phase1', 'best', 2, 'UJ_ARABIC_neutral', 0.6927995277302607, [71.42857142857143]], ['phase1', 'best', 2, 'UJ_ARABIC_sad', 2.4228394670145854, [14.285714285714286]], ['phase1', 'best', 2, 'URDU_angry', 0.13714252263307572, [100.0]], ['phase1', 'best', 2, 'URDU_happy', 0.5723968669772149, [85.0]], ['phase1', 'best', 2, 'URDU_neutral', 0.37807183638215064, [95.0]], ['phase1', 'best', 2, 'URDU_sad', 0.5459306403994562, [85.0]], ['phase2', 'best', 2, 'CREMA-D_angry', 0.26008654121808183, [93.30708661417323]], ['phase2', 'best', 2, 'CREMA-D_happy', 0.27641784440814043, [94.48818897637796]], ['phase2', 'best', 2, 'CREMA-D_neutral', 0.23941651906835326, [96.31336405529954]], ['phase2', 'best', 2, 'CREMA-D_sad', 0.6129553528634583, [78.74015748031496]], ['phase2', 'best', 2, 'SUBESCO_angry', 0.28470431238412836, [92.5]], ['phase2', 'best', 2, 'SUBESCO_happy', 0.37439083494246017, [91.0]], ['phase2', 'best', 2, 'SUBESCO_neutral', 0.27766273468732844, [94.5]], ['phase2', 'best', 2, 'SUBESCO_sad', 0.8193467020243407, [75.0]], ['phase2', 'best', 2, 'ShEMO_angry', 0.25258563324738453, [94.7867298578199]], ['phase2', 'best', 2, 'ShEMO_happy', 0.42693374641239645, [85.0]], ['phase2', 'best', 2, 'ShEMO_neutral', 0.24538376142338997, [96.58536585365853]], ['phase2', 'best', 2, 'ShEMO_sad', 1.1382430495505926, [67.41573033707866]], ['phase2', 'best', 2, 'UJ_ARABIC_angry', 0.8296753810002254, [69.23076923076923]], ['phase2', 'best', 2, 'UJ_ARABIC_happy', 1.3359356770912805, [16.666666666666668]], ['phase2', 'best', 2, 'UJ_ARABIC_neutral', 0.6173683766807828, [71.42857142857143]], ['phase2', 'best', 2, 'UJ_ARABIC_sad', 2.1588183109249384, [14.285714285714286]], ['phase2', 'best', 2, 'URDU_angry', 0.1395458072423935, [100.0]], ['phase2', 'best', 2, 'URDU_happy', 0.6497519172728062, [85.0]], ['phase2', 'best', 2, 'URDU_neutral', 0.3505798734724522, [90.0]], ['phase2', 'best', 2, 'URDU_sad', 0.5304099045693875, [85.0]], ['phase1', 'last', 2, 'CREMA-D_angry', 0.24984035330025234, [93.7007874015748]], ['phase1', 'last', 2, 'CREMA-D_happy', 0.26827800320828044, [94.48818897637796]], ['phase1', 'last', 2, 'CREMA-D_neutral', 0.22523403277594906, [96.7741935483871]], ['phase1', 'last', 2, 'CREMA-D_sad', 0.7022455512421336, [74.40944881889764]], ['phase1', 'last', 2, 'SUBESCO_angry', 0.27640943221747877, [92.5]], ['phase1', 'last', 2, 'SUBESCO_happy', 0.3622385337948799, [91.5]], ['phase1', 'last', 2, 'SUBESCO_neutral', 0.2780804342031481, [94.0]], ['phase1', 'last', 2, 'SUBESCO_sad', 0.8708563620597127, [72.0]], ['phase1', 'last', 2, 'ShEMO_angry', 0.23695080533129362, [95.260663507109]], ['phase1', 'last', 2, 'ShEMO_happy', 0.4093896470963954, [87.5]], ['phase1', 'last', 2, 'ShEMO_neutral', 0.25657484931189845, [96.09756097560975]], ['phase1', 'last', 2, 'ShEMO_sad', 1.2044240768705867, [64.04494382022472]], ['phase1', 'last', 2, 'UJ_ARABIC_angry', 0.822608069731639, [69.23076923076923]], ['phase1', 'last', 2, 'UJ_ARABIC_happy', 1.2717645491162937, [33.333333333333336]], ['phase1', 'last', 2, 'UJ_ARABIC_neutral', 0.6440192397151675, [71.42857142857143]], ['phase1', 'last', 2, 'UJ_ARABIC_sad', 2.438488530261176, [14.285714285714286]], ['phase1', 'last', 2, 'URDU_angry', 0.13764138370752335, [100.0]], ['phase1', 'last', 2, 'URDU_happy', 0.6141036458313465, [85.0]], ['phase1', 'last', 2, 'URDU_neutral', 0.3679290011525155, [95.0]], ['phase1', 'last', 2, 'URDU_sad', 0.5879364408552648, [85.0]], ['phase2', 'last', 2, 'CREMA-D_angry', 0.26902664311992847, [92.91338582677166]], ['phase2', 'last', 2, 'CREMA-D_happy', 0.2753782981024015, [94.09448818897638]], ['phase2', 'last', 2, 'CREMA-D_neutral', 0.2353055673535518, [96.7741935483871]], ['phase2', 'last', 2, 'CREMA-D_sad', 0.6381505948352062, [76.77165354330708]], ['phase2', 'last', 2, 'SUBESCO_angry', 0.28951828539371477, [93.0]], ['phase2', 'last', 2, 'SUBESCO_happy', 0.3764164911955594, [91.0]], ['phase2', 'last', 2, 'SUBESCO_neutral', 0.2734559410065411, [94.5]], ['phase2', 'last', 2, 'SUBESCO_sad', 0.8367165967077014, [75.0]], ['phase2', 'last', 2, 'ShEMO_angry', 0.2624106809002526, [94.31279620853081]], ['phase2', 'last', 2, 'ShEMO_happy', 0.4383957624435425, [85.0]], ['phase2', 'last', 2, 'ShEMO_neutral', 0.24030565939298504, [96.58536585365853]], ['phase2', 'last', 2, 'ShEMO_sad', 1.1727998432483562, [66.29213483146067]], ['phase2', 'last', 2, 'UJ_ARABIC_angry', 0.8413258343935013, [69.23076923076923]], ['phase2', 'last', 2, 'UJ_ARABIC_happy', 1.3033907612164815, [33.333333333333336]], ['phase2', 'last', 2, 'UJ_ARABIC_neutral', 0.6183227258069175, [71.42857142857143]], ['phase2', 'last', 2, 'UJ_ARABIC_sad', 2.216776783977236, [14.285714285714286]], ['phase2', 'last', 2, 'URDU_angry', 0.1392757534980774, [100.0]], ['phase2', 'last', 2, 'URDU_happy', 0.6227635897696018, [85.0]], ['phase2', 'last', 2, 'URDU_neutral', 0.33118396773934367, [90.0]], ['phase2', 'last', 2, 'URDU_sad', 0.5625272803008556, [75.0]], ['phase1', 'best', 3, 'CREMA-D_angry', 0.21897121407384934, [96.06299212598425]], ['phase1', 'best', 3, 'CREMA-D_happy', 0.2555635284016454, [95.66929133858268]], ['phase1', 'best', 3, 'CREMA-D_neutral', 0.21585286693638914, [96.7741935483871]], ['phase1', 'best', 3, 'CREMA-D_sad', 0.7073777377840097, [75.19685039370079]], ['phase1', 'best', 3, 'SUBESCO_angry', 0.23710701055824762, [95.5]], ['phase1', 'best', 3, 'SUBESCO_happy', 0.3434567285329101, [92.0]], ['phase1', 'best', 3, 'SUBESCO_neutral', 0.281279255375266, [93.5]], ['phase1', 'best', 3, 'SUBESCO_sad', 0.7725224840641027, [76.5]], ['phase1', 'best', 3, 'ShEMO_angry', 0.2588179732244728, [96.6824644549763]], ['phase1', 'best', 3, 'ShEMO_happy', 0.4878143709152935, [90.0]], ['phase1', 'best', 3, 'ShEMO_neutral', 0.2344266333957997, [96.09756097560975]], ['phase1', 'best', 3, 'ShEMO_sad', 1.054832771755336, [70.78651685393258]], ['phase1', 'best', 3, 'UJ_ARABIC_angry', 0.5241244079974982, [76.92307692307692]], ['phase1', 'best', 3, 'UJ_ARABIC_happy', 1.225831707318624, [66.66666666666667]], ['phase1', 'best', 3, 'UJ_ARABIC_neutral', 0.21521313275609696, [100.0]], ['phase1', 'best', 3, 'UJ_ARABIC_sad', 1.5891656854322977, [28.571428571428573]], ['phase1', 'best', 3, 'URDU_angry', 0.14373421147465704, [100.0]], ['phase1', 'best', 3, 'URDU_happy', 0.23830564320087436, [95.0]], ['phase1', 'best', 3, 'URDU_neutral', 0.3117356546223164, [95.0]], ['phase1', 'best', 3, 'URDU_sad', 0.27738759741187097, [95.0]], ['phase2', 'best', 3, 'CREMA-D_angry', 0.22364605470435836, [96.06299212598425]], ['phase2', 'best', 3, 'CREMA-D_happy', 0.2588937213336391, [95.2755905511811]], ['phase2', 'best', 3, 'CREMA-D_neutral', 0.23196233101704164, [96.7741935483871]], ['phase2', 'best', 3, 'CREMA-D_sad', 0.6369640124360407, [77.95275590551181]], ['phase2', 'best', 3, 'SUBESCO_angry', 0.2467236997187138, [94.5]], ['phase2', 'best', 3, 'SUBESCO_happy', 0.3540948653966189, [92.5]], ['phase2', 'best', 3, 'SUBESCO_neutral', 0.28407134577631943, [93.0]], ['phase2', 'best', 3, 'SUBESCO_sad', 0.7273649083822967, [76.0]], ['phase2', 'best', 3, 'ShEMO_angry', 0.2668418175942524, [96.6824644549763]], ['phase2', 'best', 3, 'ShEMO_happy', 0.5090705700218677, [87.5]], ['phase2', 'best', 3, 'ShEMO_neutral', 0.22783162891864767, [96.09756097560975]], ['phase2', 'best', 3, 'ShEMO_sad', 1.0217368902450197, [70.78651685393258]], ['phase2', 'best', 3, 'UJ_ARABIC_angry', 0.6161791051809604, [76.92307692307692]], ['phase2', 'best', 3, 'UJ_ARABIC_happy', 1.2210888316233952, [66.66666666666667]], ['phase2', 'best', 3, 'UJ_ARABIC_neutral', 0.20654104011399407, [100.0]], ['phase2', 'best', 3, 'UJ_ARABIC_sad', 1.339374744466373, [28.571428571428573]], ['phase2', 'best', 3, 'URDU_angry', 0.14633126333355903, [100.0]], ['phase2', 'best', 3, 'URDU_happy', 0.24981301203370096, [95.0]], ['phase2', 'best', 3, 'URDU_neutral', 0.34116626679897305, [95.0]], ['phase2', 'best', 3, 'URDU_sad', 0.2781838342547417, [90.0]], ['phase1', 'last', 3, 'CREMA-D_angry', 0.2153352019120388, [96.06299212598425]], ['phase1', 'last', 3, 'CREMA-D_happy', 0.2583057849895295, [95.66929133858268]], ['phase1', 'last', 3, 'CREMA-D_neutral', 0.21157623127034186, [96.7741935483871]], ['phase1', 'last', 3, 'CREMA-D_sad', 0.7347471391943499, [72.83464566929133]], ['phase1', 'last', 3, 'SUBESCO_angry', 0.23271313518285763, [96.5]], ['phase1', 'last', 3, 'SUBESCO_happy', 0.34578350819647313, [92.0]], ['phase1', 'last', 3, 'SUBESCO_neutral', 0.27215616077184673, [93.5]], ['phase1', 'last', 3, 'SUBESCO_sad', 0.7787246438860891, [76.5]], ['phase1', 'last', 3, 'ShEMO_angry', 0.257463634155373, [96.6824644549763]], ['phase1', 'last', 3, 'ShEMO_happy', 0.4921661842614412, [90.0]], ['phase1', 'last', 3, 'ShEMO_neutral', 0.2355556437881983, [96.09756097560975]], ['phase1', 'last', 3, 'ShEMO_sad', 1.080654445156622, [69.66292134831461]], ['phase1', 'last', 3, 'UJ_ARABIC_angry', 0.5200148075819016, [76.92307692307692]], ['phase1', 'last', 3, 'UJ_ARABIC_happy', 1.2223902692397437, [66.66666666666667]], ['phase1', 'last', 3, 'UJ_ARABIC_neutral', 0.2123784806047167, [100.0]], ['phase1', 'last', 3, 'UJ_ARABIC_sad', 1.635129132441112, [28.571428571428573]], ['phase1', 'last', 3, 'URDU_angry', 0.1415281191468239, [100.0]], ['phase1', 'last', 3, 'URDU_happy', 0.24233263134956357, [95.0]], ['phase1', 'last', 3, 'URDU_neutral', 0.30699191167950635, [95.0]], ['phase1', 'last', 3, 'URDU_sad', 0.2787277489900589, [95.0]], ['phase2', 'last', 3, 'CREMA-D_angry', 0.2242271861457449, [96.06299212598425]], ['phase2', 'last', 3, 'CREMA-D_happy', 0.25730810496281437, [95.2755905511811]], ['phase2', 'last', 3, 'CREMA-D_neutral', 0.2256125964327342, [96.7741935483871]], ['phase2', 'last', 3, 'CREMA-D_sad', 0.6582772473063997, [77.16535433070867]], ['phase2', 'last', 3, 'SUBESCO_angry', 0.24562619954347603, [94.5]], ['phase2', 'last', 3, 'SUBESCO_happy', 0.3530384108424185, [92.5]], ['phase2', 'last', 3, 'SUBESCO_neutral', 0.2823210334777832, [93.5]], ['phase2', 'last', 3, 'SUBESCO_sad', 0.7386462476849557, [76.0]], ['phase2', 'last', 3, 'ShEMO_angry', 0.2679040304151193, [96.6824644549763]], ['phase2', 'last', 3, 'ShEMO_happy', 0.5070606425404547, [87.5]], ['phase2', 'last', 3, 'ShEMO_neutral', 0.22696804317032415, [96.09756097560975]], ['phase2', 'last', 3, 'ShEMO_sad', 1.0379315956255022, [70.78651685393258]], ['phase2', 'last', 3, 'UJ_ARABIC_angry', 0.5962140605999874, [76.92307692307692]], ['phase2', 'last', 3, 'UJ_ARABIC_happy', 1.225340927640597, [66.66666666666667]], ['phase2', 'last', 3, 'UJ_ARABIC_neutral', 0.2006605267524719, [100.0]], ['phase2', 'last', 3, 'UJ_ARABIC_sad', 1.3891573633466447, [28.571428571428573]], ['phase2', 'last', 3, 'URDU_angry', 0.14554552957415579, [100.0]], ['phase2', 'last', 3, 'URDU_happy', 0.24518935307860376, [95.0]], ['phase2', 'last', 3, 'URDU_neutral', 0.3288537606596947, [95.0]], ['phase2', 'last', 3, 'URDU_sad', 0.2796537265181542, [90.0]]]\n",
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 14 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.94it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.33it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.55it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.05it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.89it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.23it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 58.52it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 55.45it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 39.34it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 48.11it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:02<00:00, 43.85it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 26.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 13.94it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.22it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.54it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.27it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 36.13it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.34it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 4 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 65.48it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 67.28it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 65.30it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 63.92it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 53.83it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.08it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 52.90it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:04<00:00, 51.85it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:01<00:00, 39.72it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 50.56it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:02<00:00, 43.77it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 26.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 13.95it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 14.95it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.55it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.03it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.21it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 31.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 25 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 63.69it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 63.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 60.19it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 60.15it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 52.10it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 51.53it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.94it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 57.47it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 57.86it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 40.95it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:03<00:00, 51.98it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 46.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 13.77it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.60it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.65it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 32.42it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.65it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 30.63it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously trained model weights state_dict loaded...\n",
      "Previously trained for 10 number of epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 64.74it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:03<00:00, 66.03it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 217/217 [00:03<00:00, 62.33it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 254/254 [00:04<00:00, 62.34it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.05it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 54.24it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 56.00it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 200/200 [00:03<00:00, 55.20it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 211/211 [00:03<00:00, 55.77it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 40/40 [00:00<00:00, 40.68it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████| 205/205 [00:04<00:00, 50.16it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 89/89 [00:01<00:00, 45.56it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 13/13 [00:00<00:00, 28.62it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 6/6 [00:00<00:00, 13.88it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 15.06it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|█████████████████████████████████| 7/7 [00:00<00:00, 16.07it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.09it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.57it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 34.92it/s]\n",
      "Using cache found in /home/naif/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Validating: 100%|███████████████████████████████| 20/20 [00:00<00:00, 33.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['phase1', 'best', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'best', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'best', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'best', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'best', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'best', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'best', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'best', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'best', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'best', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'best', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'best', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'best', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'best', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'best', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'best', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'best', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'best', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'best', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'best', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'best', 0, 'CREMA-D_angry', 0.258736290096298, [94.09448818897638]], ['phase2', 'best', 0, 'CREMA-D_happy', 0.3128238437095964, [93.7007874015748]], ['phase2', 'best', 0, 'CREMA-D_neutral', 0.2521394816686482, [96.31336405529954]], ['phase2', 'best', 0, 'CREMA-D_sad', 0.6713012946168266, [77.55905511811024]], ['phase2', 'best', 0, 'SUBESCO_angry', 0.25672499284148226, [95.0]], ['phase2', 'best', 0, 'SUBESCO_happy', 0.3820594840496778, [92.0]], ['phase2', 'best', 0, 'SUBESCO_neutral', 0.23552342765033246, [95.5]], ['phase2', 'best', 0, 'SUBESCO_sad', 0.9597138513624667, [67.5]], ['phase2', 'best', 0, 'ShEMO_angry', 0.22040177592169052, [96.6824644549763]], ['phase2', 'best', 0, 'ShEMO_happy', 0.32229302674531946, [95.0]], ['phase2', 'best', 0, 'ShEMO_neutral', 0.2571009743504408, [95.1219512195122]], ['phase2', 'best', 0, 'ShEMO_sad', 1.2437129648548833, [68.53932584269663]], ['phase2', 'best', 0, 'UJ_ARABIC_angry', 0.7015116925422962, [69.23076923076923]], ['phase2', 'best', 0, 'UJ_ARABIC_happy', 0.6391726980606716, [83.33333333333333]], ['phase2', 'best', 0, 'UJ_ARABIC_neutral', 0.7040895244904927, [57.142857142857146]], ['phase2', 'best', 0, 'UJ_ARABIC_sad', 1.8073663839272092, [28.571428571428573]], ['phase2', 'best', 0, 'URDU_angry', 0.14747261628508565, [100.0]], ['phase2', 'best', 0, 'URDU_happy', 0.45416179522871986, [85.0]], ['phase2', 'best', 0, 'URDU_neutral', 0.4689487174153329, [85.0]], ['phase2', 'best', 0, 'URDU_sad', 0.4275880873203277, [95.0]], ['phase1', 'last', 0, 'CREMA-D_angry', 0.23132538425875457, [95.2755905511811]], ['phase1', 'last', 0, 'CREMA-D_happy', 0.30659085259897517, [93.7007874015748]], ['phase1', 'last', 0, 'CREMA-D_neutral', 0.2624118492213261, [96.7741935483871]], ['phase1', 'last', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]], ['phase1', 'last', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]], ['phase1', 'last', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]], ['phase1', 'last', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]], ['phase1', 'last', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]], ['phase1', 'last', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]], ['phase1', 'last', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]], ['phase1', 'last', 0, 'ShEMO_neutral', 0.27502004424246357, [95.60975609756098]], ['phase1', 'last', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]], ['phase1', 'last', 0, 'UJ_ARABIC_angry', 0.6361714372268089, [69.23076923076923]], ['phase1', 'last', 0, 'UJ_ARABIC_happy', 0.6464924265940984, [83.33333333333333]], ['phase1', 'last', 0, 'UJ_ARABIC_neutral', 0.7838817834854126, [57.142857142857146]], ['phase1', 'last', 0, 'UJ_ARABIC_sad', 1.8529385775327682, [28.571428571428573]], ['phase1', 'last', 0, 'URDU_angry', 0.14219589829444887, [100.0]], ['phase1', 'last', 0, 'URDU_happy', 0.4126186534762382, [85.0]], ['phase1', 'last', 0, 'URDU_neutral', 0.4651359885931015, [85.0]], ['phase1', 'last', 0, 'URDU_sad', 0.43485047519207004, [95.0]], ['phase2', 'last', 0, 'CREMA-D_angry', 0.2588579188298993, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_happy', 0.3136730649339872, [93.7007874015748]], ['phase2', 'last', 0, 'CREMA-D_neutral', 0.25121473428291113, [96.31336405529954]], ['phase2', 'last', 0, 'CREMA-D_sad', 0.6725248215118728, [77.55905511811024]], ['phase2', 'last', 0, 'SUBESCO_angry', 0.25600903317332274, [95.0]], ['phase2', 'last', 0, 'SUBESCO_happy', 0.3834911134093998, [92.0]], ['phase2', 'last', 0, 'SUBESCO_neutral', 0.23549174077808857, [95.5]], ['phase2', 'last', 0, 'SUBESCO_sad', 0.9606494718790054, [67.0]], ['phase2', 'last', 0, 'ShEMO_angry', 0.22003566039399522, [97.1563981042654]], ['phase2', 'last', 0, 'ShEMO_happy', 0.32309657931327823, [95.0]], ['phase2', 'last', 0, 'ShEMO_neutral', 0.2568363875877568, [95.1219512195122]], ['phase2', 'last', 0, 'ShEMO_sad', 1.2490537643767476, [66.29213483146067]], ['phase2', 'last', 0, 'UJ_ARABIC_angry', 0.6984921602102426, [69.23076923076923]], ['phase2', 'last', 0, 'UJ_ARABIC_happy', 0.6390940000613531, [83.33333333333333]], ['phase2', 'last', 0, 'UJ_ARABIC_neutral', 0.7022053365196501, [57.142857142857146]], ['phase2', 'last', 0, 'UJ_ARABIC_sad', 1.813152142933437, [28.571428571428573]], ['phase2', 'last', 0, 'URDU_angry', 0.14700243920087813, [100.0]], ['phase2', 'last', 0, 'URDU_happy', 0.4553044453263283, [85.0]], ['phase2', 'last', 0, 'URDU_neutral', 0.4675500065088273, [85.0]], ['phase2', 'last', 0, 'URDU_sad', 0.42905081287026403, [95.0]], ['phase1', 'best', 1, 'CREMA-D_angry', 0.27615764127002945, [94.88188976377953]], ['phase1', 'best', 1, 'CREMA-D_happy', 0.23527879508461533, [95.66929133858268]], ['phase1', 'best', 1, 'CREMA-D_neutral', 0.25794078073193955, [96.7741935483871]], ['phase1', 'best', 1, 'CREMA-D_sad', 0.6431511204660413, [79.5275590551181]], ['phase1', 'best', 1, 'SUBESCO_angry', 0.3505234102904796, [91.0]], ['phase1', 'best', 1, 'SUBESCO_happy', 0.30056495204567907, [91.5]], ['phase1', 'best', 1, 'SUBESCO_neutral', 0.32848145671188816, [92.0]], ['phase1', 'best', 1, 'SUBESCO_sad', 0.8100879185646771, [77.0]], ['phase1', 'best', 1, 'ShEMO_angry', 0.20217314553204307, [97.6303317535545]], ['phase1', 'best', 1, 'ShEMO_happy', 0.33687370978295794, [87.5]], ['phase1', 'best', 1, 'ShEMO_neutral', 0.2413041352498823, [96.09756097560975]], ['phase1', 'best', 1, 'ShEMO_sad', 0.9465443391478462, [69.66292134831461]], ['phase1', 'best', 1, 'UJ_ARABIC_angry', 1.5619367189132252, [61.53846153846154]], ['phase1', 'best', 1, 'UJ_ARABIC_happy', 0.4149790306886037, [83.33333333333333]], ['phase1', 'best', 1, 'UJ_ARABIC_neutral', 0.6496904577527727, [85.71428571428571]], ['phase1', 'best', 1, 'UJ_ARABIC_sad', 2.2628096171787804, [0.0]], ['phase1', 'best', 1, 'URDU_angry', 0.18128371462225912, [100.0]], ['phase1', 'best', 1, 'URDU_happy', 0.40306554064154626, [90.0]], ['phase1', 'best', 1, 'URDU_neutral', 0.26644408404827113, [95.0]], ['phase1', 'best', 1, 'URDU_sad', 0.36069156974554056, [95.0]], ['phase2', 'best', 1, 'CREMA-D_angry', 0.29083810124810267, [93.30708661417323]], ['phase2', 'best', 1, 'CREMA-D_happy', 0.23703179144718511, [96.06299212598425]], ['phase2', 'best', 1, 'CREMA-D_neutral', 0.2536086365237215, [96.31336405529954]], ['phase2', 'best', 1, 'CREMA-D_sad', 0.6164421358798431, [81.10236220472441]], ['phase2', 'best', 1, 'SUBESCO_angry', 0.3682341238856316, [91.0]], ['phase2', 'best', 1, 'SUBESCO_happy', 0.3087896878272295, [92.5]], ['phase2', 'best', 1, 'SUBESCO_neutral', 0.3148857557773592, [93.0]], ['phase2', 'best', 1, 'SUBESCO_sad', 0.8102790326625107, [76.5]], ['phase2', 'best', 1, 'ShEMO_angry', 0.20505267777149144, [98.10426540284361]], ['phase2', 'best', 1, 'ShEMO_happy', 0.3488592237234116, [87.5]], ['phase2', 'best', 1, 'ShEMO_neutral', 0.22883073178733276, [96.58536585365853]], ['phase2', 'best', 1, 'ShEMO_sad', 0.928263324365187, [71.91011235955057]], ['phase2', 'best', 1, 'UJ_ARABIC_angry', 1.614923622745734, [61.53846153846154]], ['phase2', 'best', 1, 'UJ_ARABIC_happy', 0.4457362964749336, [100.0]], ['phase2', 'best', 1, 'UJ_ARABIC_neutral', 0.5752684026956558, [85.71428571428571]], ['phase2', 'best', 1, 'UJ_ARABIC_sad', 2.1905118397303993, [0.0]], ['phase2', 'best', 1, 'URDU_angry', 0.19813046157360079, [95.0]], ['phase2', 'best', 1, 'URDU_happy', 0.432581639289856, [90.0]], ['phase2', 'best', 1, 'URDU_neutral', 0.27857217714190496, [95.0]], ['phase2', 'best', 1, 'URDU_sad', 0.3698753505945205, [95.0]], ['phase1', 'last', 1, 'CREMA-D_angry', 0.2761957376726031, [94.09448818897638]], ['phase1', 'last', 1, 'CREMA-D_happy', 0.23440244113366435, [94.88188976377953]], ['phase1', 'last', 1, 'CREMA-D_neutral', 0.2563687742168453, [96.31336405529954]], ['phase1', 'last', 1, 'CREMA-D_sad', 0.6558136834990316, [79.13385826771653]], ['phase1', 'last', 1, 'SUBESCO_angry', 0.3387189420312643, [91.5]], ['phase1', 'last', 1, 'SUBESCO_happy', 0.30185804873704913, [93.0]], ['phase1', 'last', 1, 'SUBESCO_neutral', 0.325886485427618, [92.0]], ['phase1', 'last', 1, 'SUBESCO_sad', 0.8205169849097728, [76.5]], ['phase1', 'last', 1, 'ShEMO_angry', 0.19906430384276588, [98.10426540284361]], ['phase1', 'last', 1, 'ShEMO_happy', 0.343780105188489, [87.5]], ['phase1', 'last', 1, 'ShEMO_neutral', 0.2343073303379663, [96.09756097560975]], ['phase1', 'last', 1, 'ShEMO_sad', 0.9585065647457423, [70.78651685393258]], ['phase1', 'last', 1, 'UJ_ARABIC_angry', 1.5495183548102012, [61.53846153846154]], ['phase1', 'last', 1, 'UJ_ARABIC_happy', 0.4166577458381653, [83.33333333333333]], ['phase1', 'last', 1, 'UJ_ARABIC_neutral', 0.6069116996867316, [85.71428571428571]], ['phase1', 'last', 1, 'UJ_ARABIC_sad', 2.3494340351649696, [0.0]], ['phase1', 'last', 1, 'URDU_angry', 0.18395565971732136, [100.0]], ['phase1', 'last', 1, 'URDU_happy', 0.3975479431450366, [90.0]], ['phase1', 'last', 1, 'URDU_neutral', 0.2688737452030182, [95.0]], ['phase1', 'last', 1, 'URDU_sad', 0.3800719514489175, [90.0]], ['phase2', 'last', 1, 'CREMA-D_angry', 0.2906135099492672, [93.30708661417323]], ['phase2', 'last', 1, 'CREMA-D_happy', 0.2367331867260258, [96.06299212598425]], ['phase2', 'last', 1, 'CREMA-D_neutral', 0.2552252822482642, [96.31336405529954]], ['phase2', 'last', 1, 'CREMA-D_sad', 0.6159147716999991, [81.10236220472441]], ['phase2', 'last', 1, 'SUBESCO_angry', 0.3666904728859662, [91.0]], ['phase2', 'last', 1, 'SUBESCO_happy', 0.30799106523394587, [92.5]], ['phase2', 'last', 1, 'SUBESCO_neutral', 0.3148298836499454, [93.0]], ['phase2', 'last', 1, 'SUBESCO_sad', 0.8127771169692279, [76.5]], ['phase2', 'last', 1, 'ShEMO_angry', 0.20459069615291772, [98.10426540284361]], ['phase2', 'last', 1, 'ShEMO_happy', 0.34735414050519464, [87.5]], ['phase2', 'last', 1, 'ShEMO_neutral', 0.2300839459750711, [96.58536585365853]], ['phase2', 'last', 1, 'ShEMO_sad', 0.9299659379077759, [71.91011235955057]], ['phase2', 'last', 1, 'UJ_ARABIC_angry', 1.6147538813260884, [61.53846153846154]], ['phase2', 'last', 1, 'UJ_ARABIC_happy', 0.4460348735253016, [100.0]], ['phase2', 'last', 1, 'UJ_ARABIC_neutral', 0.584535104887826, [85.71428571428571]], ['phase2', 'last', 1, 'UJ_ARABIC_sad', 2.1865100519997736, [0.0]], ['phase2', 'last', 1, 'URDU_angry', 0.19664481729269026, [95.0]], ['phase2', 'last', 1, 'URDU_happy', 0.4312362901866436, [90.0]], ['phase2', 'last', 1, 'URDU_neutral', 0.28115319460630417, [95.0]], ['phase2', 'last', 1, 'URDU_sad', 0.37026820853352543, [95.0]], ['phase1', 'best', 2, 'CREMA-D_angry', 0.2533235727684707, [93.7007874015748]], ['phase1', 'best', 2, 'CREMA-D_happy', 0.2530228722048557, [95.2755905511811]], ['phase1', 'best', 2, 'CREMA-D_neutral', 0.23457274898406, [96.7741935483871]], ['phase1', 'best', 2, 'CREMA-D_sad', 0.7033744433264096, [74.80314960629921]], ['phase1', 'best', 2, 'SUBESCO_angry', 0.28369085997343063, [91.5]], ['phase1', 'best', 2, 'SUBESCO_happy', 0.354978576526046, [91.0]], ['phase1', 'best', 2, 'SUBESCO_neutral', 0.29951987594366075, [94.0]], ['phase1', 'best', 2, 'SUBESCO_sad', 0.8211694458872081, [76.0]], ['phase1', 'best', 2, 'ShEMO_angry', 0.22963467148525457, [94.7867298578199]], ['phase1', 'best', 2, 'ShEMO_happy', 0.37117515616118907, [87.5]], ['phase1', 'best', 2, 'ShEMO_neutral', 0.2645087184702479, [95.1219512195122]], ['phase1', 'best', 2, 'ShEMO_sad', 1.1678407711259435, [66.29213483146067]], ['phase1', 'best', 2, 'UJ_ARABIC_angry', 0.8589285772580367, [69.23076923076923]], ['phase1', 'best', 2, 'UJ_ARABIC_happy', 1.1219921261072159, [50.0]], ['phase1', 'best', 2, 'UJ_ARABIC_neutral', 0.6927995277302607, [71.42857142857143]], ['phase1', 'best', 2, 'UJ_ARABIC_sad', 2.4228394670145854, [14.285714285714286]], ['phase1', 'best', 2, 'URDU_angry', 0.13714252263307572, [100.0]], ['phase1', 'best', 2, 'URDU_happy', 0.5723968669772149, [85.0]], ['phase1', 'best', 2, 'URDU_neutral', 0.37807183638215064, [95.0]], ['phase1', 'best', 2, 'URDU_sad', 0.5459306403994562, [85.0]], ['phase2', 'best', 2, 'CREMA-D_angry', 0.26008654121808183, [93.30708661417323]], ['phase2', 'best', 2, 'CREMA-D_happy', 0.27641784440814043, [94.48818897637796]], ['phase2', 'best', 2, 'CREMA-D_neutral', 0.23941651906835326, [96.31336405529954]], ['phase2', 'best', 2, 'CREMA-D_sad', 0.6129553528634583, [78.74015748031496]], ['phase2', 'best', 2, 'SUBESCO_angry', 0.28470431238412836, [92.5]], ['phase2', 'best', 2, 'SUBESCO_happy', 0.37439083494246017, [91.0]], ['phase2', 'best', 2, 'SUBESCO_neutral', 0.27766273468732844, [94.5]], ['phase2', 'best', 2, 'SUBESCO_sad', 0.8193467020243407, [75.0]], ['phase2', 'best', 2, 'ShEMO_angry', 0.25258563324738453, [94.7867298578199]], ['phase2', 'best', 2, 'ShEMO_happy', 0.42693374641239645, [85.0]], ['phase2', 'best', 2, 'ShEMO_neutral', 0.24538376142338997, [96.58536585365853]], ['phase2', 'best', 2, 'ShEMO_sad', 1.1382430495505926, [67.41573033707866]], ['phase2', 'best', 2, 'UJ_ARABIC_angry', 0.8296753810002254, [69.23076923076923]], ['phase2', 'best', 2, 'UJ_ARABIC_happy', 1.3359356770912805, [16.666666666666668]], ['phase2', 'best', 2, 'UJ_ARABIC_neutral', 0.6173683766807828, [71.42857142857143]], ['phase2', 'best', 2, 'UJ_ARABIC_sad', 2.1588183109249384, [14.285714285714286]], ['phase2', 'best', 2, 'URDU_angry', 0.1395458072423935, [100.0]], ['phase2', 'best', 2, 'URDU_happy', 0.6497519172728062, [85.0]], ['phase2', 'best', 2, 'URDU_neutral', 0.3505798734724522, [90.0]], ['phase2', 'best', 2, 'URDU_sad', 0.5304099045693875, [85.0]], ['phase1', 'last', 2, 'CREMA-D_angry', 0.24984035330025234, [93.7007874015748]], ['phase1', 'last', 2, 'CREMA-D_happy', 0.26827800320828044, [94.48818897637796]], ['phase1', 'last', 2, 'CREMA-D_neutral', 0.22523403277594906, [96.7741935483871]], ['phase1', 'last', 2, 'CREMA-D_sad', 0.7022455512421336, [74.40944881889764]], ['phase1', 'last', 2, 'SUBESCO_angry', 0.27640943221747877, [92.5]], ['phase1', 'last', 2, 'SUBESCO_happy', 0.3622385337948799, [91.5]], ['phase1', 'last', 2, 'SUBESCO_neutral', 0.2780804342031481, [94.0]], ['phase1', 'last', 2, 'SUBESCO_sad', 0.8708563620597127, [72.0]], ['phase1', 'last', 2, 'ShEMO_angry', 0.23695080533129362, [95.260663507109]], ['phase1', 'last', 2, 'ShEMO_happy', 0.4093896470963954, [87.5]], ['phase1', 'last', 2, 'ShEMO_neutral', 0.25657484931189845, [96.09756097560975]], ['phase1', 'last', 2, 'ShEMO_sad', 1.2044240768705867, [64.04494382022472]], ['phase1', 'last', 2, 'UJ_ARABIC_angry', 0.822608069731639, [69.23076923076923]], ['phase1', 'last', 2, 'UJ_ARABIC_happy', 1.2717645491162937, [33.333333333333336]], ['phase1', 'last', 2, 'UJ_ARABIC_neutral', 0.6440192397151675, [71.42857142857143]], ['phase1', 'last', 2, 'UJ_ARABIC_sad', 2.438488530261176, [14.285714285714286]], ['phase1', 'last', 2, 'URDU_angry', 0.13764138370752335, [100.0]], ['phase1', 'last', 2, 'URDU_happy', 0.6141036458313465, [85.0]], ['phase1', 'last', 2, 'URDU_neutral', 0.3679290011525155, [95.0]], ['phase1', 'last', 2, 'URDU_sad', 0.5879364408552648, [85.0]], ['phase2', 'last', 2, 'CREMA-D_angry', 0.26902664311992847, [92.91338582677166]], ['phase2', 'last', 2, 'CREMA-D_happy', 0.2753782981024015, [94.09448818897638]], ['phase2', 'last', 2, 'CREMA-D_neutral', 0.2353055673535518, [96.7741935483871]], ['phase2', 'last', 2, 'CREMA-D_sad', 0.6381505948352062, [76.77165354330708]], ['phase2', 'last', 2, 'SUBESCO_angry', 0.28951828539371477, [93.0]], ['phase2', 'last', 2, 'SUBESCO_happy', 0.3764164911955594, [91.0]], ['phase2', 'last', 2, 'SUBESCO_neutral', 0.2734559410065411, [94.5]], ['phase2', 'last', 2, 'SUBESCO_sad', 0.8367165967077014, [75.0]], ['phase2', 'last', 2, 'ShEMO_angry', 0.2624106809002526, [94.31279620853081]], ['phase2', 'last', 2, 'ShEMO_happy', 0.4383957624435425, [85.0]], ['phase2', 'last', 2, 'ShEMO_neutral', 0.24030565939298504, [96.58536585365853]], ['phase2', 'last', 2, 'ShEMO_sad', 1.1727998432483562, [66.29213483146067]], ['phase2', 'last', 2, 'UJ_ARABIC_angry', 0.8413258343935013, [69.23076923076923]], ['phase2', 'last', 2, 'UJ_ARABIC_happy', 1.3033907612164815, [33.333333333333336]], ['phase2', 'last', 2, 'UJ_ARABIC_neutral', 0.6183227258069175, [71.42857142857143]], ['phase2', 'last', 2, 'UJ_ARABIC_sad', 2.216776783977236, [14.285714285714286]], ['phase2', 'last', 2, 'URDU_angry', 0.1392757534980774, [100.0]], ['phase2', 'last', 2, 'URDU_happy', 0.6227635897696018, [85.0]], ['phase2', 'last', 2, 'URDU_neutral', 0.33118396773934367, [90.0]], ['phase2', 'last', 2, 'URDU_sad', 0.5625272803008556, [75.0]], ['phase1', 'best', 3, 'CREMA-D_angry', 0.21897121407384934, [96.06299212598425]], ['phase1', 'best', 3, 'CREMA-D_happy', 0.2555635284016454, [95.66929133858268]], ['phase1', 'best', 3, 'CREMA-D_neutral', 0.21585286693638914, [96.7741935483871]], ['phase1', 'best', 3, 'CREMA-D_sad', 0.7073777377840097, [75.19685039370079]], ['phase1', 'best', 3, 'SUBESCO_angry', 0.23710701055824762, [95.5]], ['phase1', 'best', 3, 'SUBESCO_happy', 0.3434567285329101, [92.0]], ['phase1', 'best', 3, 'SUBESCO_neutral', 0.281279255375266, [93.5]], ['phase1', 'best', 3, 'SUBESCO_sad', 0.7725224840641027, [76.5]], ['phase1', 'best', 3, 'ShEMO_angry', 0.2588179732244728, [96.6824644549763]], ['phase1', 'best', 3, 'ShEMO_happy', 0.4878143709152935, [90.0]], ['phase1', 'best', 3, 'ShEMO_neutral', 0.2344266333957997, [96.09756097560975]], ['phase1', 'best', 3, 'ShEMO_sad', 1.054832771755336, [70.78651685393258]], ['phase1', 'best', 3, 'UJ_ARABIC_angry', 0.5241244079974982, [76.92307692307692]], ['phase1', 'best', 3, 'UJ_ARABIC_happy', 1.225831707318624, [66.66666666666667]], ['phase1', 'best', 3, 'UJ_ARABIC_neutral', 0.21521313275609696, [100.0]], ['phase1', 'best', 3, 'UJ_ARABIC_sad', 1.5891656854322977, [28.571428571428573]], ['phase1', 'best', 3, 'URDU_angry', 0.14373421147465704, [100.0]], ['phase1', 'best', 3, 'URDU_happy', 0.23830564320087436, [95.0]], ['phase1', 'best', 3, 'URDU_neutral', 0.3117356546223164, [95.0]], ['phase1', 'best', 3, 'URDU_sad', 0.27738759741187097, [95.0]], ['phase2', 'best', 3, 'CREMA-D_angry', 0.22364605470435836, [96.06299212598425]], ['phase2', 'best', 3, 'CREMA-D_happy', 0.2588937213336391, [95.2755905511811]], ['phase2', 'best', 3, 'CREMA-D_neutral', 0.23196233101704164, [96.7741935483871]], ['phase2', 'best', 3, 'CREMA-D_sad', 0.6369640124360407, [77.95275590551181]], ['phase2', 'best', 3, 'SUBESCO_angry', 0.2467236997187138, [94.5]], ['phase2', 'best', 3, 'SUBESCO_happy', 0.3540948653966189, [92.5]], ['phase2', 'best', 3, 'SUBESCO_neutral', 0.28407134577631943, [93.0]], ['phase2', 'best', 3, 'SUBESCO_sad', 0.7273649083822967, [76.0]], ['phase2', 'best', 3, 'ShEMO_angry', 0.2668418175942524, [96.6824644549763]], ['phase2', 'best', 3, 'ShEMO_happy', 0.5090705700218677, [87.5]], ['phase2', 'best', 3, 'ShEMO_neutral', 0.22783162891864767, [96.09756097560975]], ['phase2', 'best', 3, 'ShEMO_sad', 1.0217368902450197, [70.78651685393258]], ['phase2', 'best', 3, 'UJ_ARABIC_angry', 0.6161791051809604, [76.92307692307692]], ['phase2', 'best', 3, 'UJ_ARABIC_happy', 1.2210888316233952, [66.66666666666667]], ['phase2', 'best', 3, 'UJ_ARABIC_neutral', 0.20654104011399407, [100.0]], ['phase2', 'best', 3, 'UJ_ARABIC_sad', 1.339374744466373, [28.571428571428573]], ['phase2', 'best', 3, 'URDU_angry', 0.14633126333355903, [100.0]], ['phase2', 'best', 3, 'URDU_happy', 0.24981301203370096, [95.0]], ['phase2', 'best', 3, 'URDU_neutral', 0.34116626679897305, [95.0]], ['phase2', 'best', 3, 'URDU_sad', 0.2781838342547417, [90.0]], ['phase1', 'last', 3, 'CREMA-D_angry', 0.2153352019120388, [96.06299212598425]], ['phase1', 'last', 3, 'CREMA-D_happy', 0.2583057849895295, [95.66929133858268]], ['phase1', 'last', 3, 'CREMA-D_neutral', 0.21157623127034186, [96.7741935483871]], ['phase1', 'last', 3, 'CREMA-D_sad', 0.7347471391943499, [72.83464566929133]], ['phase1', 'last', 3, 'SUBESCO_angry', 0.23271313518285763, [96.5]], ['phase1', 'last', 3, 'SUBESCO_happy', 0.34578350819647313, [92.0]], ['phase1', 'last', 3, 'SUBESCO_neutral', 0.27215616077184673, [93.5]], ['phase1', 'last', 3, 'SUBESCO_sad', 0.7787246438860891, [76.5]], ['phase1', 'last', 3, 'ShEMO_angry', 0.257463634155373, [96.6824644549763]], ['phase1', 'last', 3, 'ShEMO_happy', 0.4921661842614412, [90.0]], ['phase1', 'last', 3, 'ShEMO_neutral', 0.2355556437881983, [96.09756097560975]], ['phase1', 'last', 3, 'ShEMO_sad', 1.080654445156622, [69.66292134831461]], ['phase1', 'last', 3, 'UJ_ARABIC_angry', 0.5200148075819016, [76.92307692307692]], ['phase1', 'last', 3, 'UJ_ARABIC_happy', 1.2223902692397437, [66.66666666666667]], ['phase1', 'last', 3, 'UJ_ARABIC_neutral', 0.2123784806047167, [100.0]], ['phase1', 'last', 3, 'UJ_ARABIC_sad', 1.635129132441112, [28.571428571428573]], ['phase1', 'last', 3, 'URDU_angry', 0.1415281191468239, [100.0]], ['phase1', 'last', 3, 'URDU_happy', 0.24233263134956357, [95.0]], ['phase1', 'last', 3, 'URDU_neutral', 0.30699191167950635, [95.0]], ['phase1', 'last', 3, 'URDU_sad', 0.2787277489900589, [95.0]], ['phase2', 'last', 3, 'CREMA-D_angry', 0.2242271861457449, [96.06299212598425]], ['phase2', 'last', 3, 'CREMA-D_happy', 0.25730810496281437, [95.2755905511811]], ['phase2', 'last', 3, 'CREMA-D_neutral', 0.2256125964327342, [96.7741935483871]], ['phase2', 'last', 3, 'CREMA-D_sad', 0.6582772473063997, [77.16535433070867]], ['phase2', 'last', 3, 'SUBESCO_angry', 0.24562619954347603, [94.5]], ['phase2', 'last', 3, 'SUBESCO_happy', 0.3530384108424185, [92.5]], ['phase2', 'last', 3, 'SUBESCO_neutral', 0.2823210334777832, [93.5]], ['phase2', 'last', 3, 'SUBESCO_sad', 0.7386462476849557, [76.0]], ['phase2', 'last', 3, 'ShEMO_angry', 0.2679040304151193, [96.6824644549763]], ['phase2', 'last', 3, 'ShEMO_happy', 0.5070606425404547, [87.5]], ['phase2', 'last', 3, 'ShEMO_neutral', 0.22696804317032415, [96.09756097560975]], ['phase2', 'last', 3, 'ShEMO_sad', 1.0379315956255022, [70.78651685393258]], ['phase2', 'last', 3, 'UJ_ARABIC_angry', 0.5962140605999874, [76.92307692307692]], ['phase2', 'last', 3, 'UJ_ARABIC_happy', 1.225340927640597, [66.66666666666667]], ['phase2', 'last', 3, 'UJ_ARABIC_neutral', 0.2006605267524719, [100.0]], ['phase2', 'last', 3, 'UJ_ARABIC_sad', 1.3891573633466447, [28.571428571428573]], ['phase2', 'last', 3, 'URDU_angry', 0.14554552957415579, [100.0]], ['phase2', 'last', 3, 'URDU_happy', 0.24518935307860376, [95.0]], ['phase2', 'last', 3, 'URDU_neutral', 0.3288537606596947, [95.0]], ['phase2', 'last', 3, 'URDU_sad', 0.2796537265181542, [90.0]], ['phase1', 'best', 4, 'CREMA-D_angry', 0.23601596966737845, [96.06299212598425]], ['phase1', 'best', 4, 'CREMA-D_happy', 0.2647190968704035, [94.48818897637796]], ['phase1', 'best', 4, 'CREMA-D_neutral', 0.2174992754986759, [96.7741935483871]], ['phase1', 'best', 4, 'CREMA-D_sad', 0.6358191324031262, [77.95275590551181]], ['phase1', 'best', 4, 'SUBESCO_angry', 0.26032150544226146, [94.0]], ['phase1', 'best', 4, 'SUBESCO_happy', 0.4046996270120143, [91.0]], ['phase1', 'best', 4, 'SUBESCO_neutral', 0.2816057708114385, [94.5]], ['phase1', 'best', 4, 'SUBESCO_sad', 0.7527326751500368, [74.5]], ['phase1', 'best', 4, 'ShEMO_angry', 0.21090828560257413, [98.10426540284361]], ['phase1', 'best', 4, 'ShEMO_happy', 0.43363358937203883, [90.0]], ['phase1', 'best', 4, 'ShEMO_neutral', 0.24421804510965586, [95.60975609756098]], ['phase1', 'best', 4, 'ShEMO_sad', 1.0757271935430806, [71.91011235955057]], ['phase1', 'best', 4, 'UJ_ARABIC_angry', 0.9124016773242217, [61.53846153846154]], ['phase1', 'best', 4, 'UJ_ARABIC_happy', 0.7047847310702007, [66.66666666666667]], ['phase1', 'best', 4, 'UJ_ARABIC_neutral', 0.7610876028026854, [71.42857142857143]], ['phase1', 'best', 4, 'UJ_ARABIC_sad', 2.6012486730303084, [14.285714285714286]], ['phase1', 'best', 4, 'URDU_angry', 0.17406784221529958, [100.0]], ['phase1', 'best', 4, 'URDU_happy', 0.4008956432342529, [90.0]], ['phase1', 'best', 4, 'URDU_neutral', 0.38109260052442556, [90.0]], ['phase1', 'best', 4, 'URDU_sad', 0.4538943864405155, [90.0]], ['phase2', 'best', 4, 'CREMA-D_angry', 0.2361262004206499, [96.06299212598425]], ['phase2', 'best', 4, 'CREMA-D_happy', 0.25317100640826345, [96.06299212598425]], ['phase2', 'best', 4, 'CREMA-D_neutral', 0.2131844287918461, [96.31336405529954]], ['phase2', 'best', 4, 'CREMA-D_sad', 0.6317807730492643, [80.31496062992126]], ['phase2', 'best', 4, 'SUBESCO_angry', 0.2663305568695069, [93.0]], ['phase2', 'best', 4, 'SUBESCO_happy', 0.3762897613644597, [91.5]], ['phase2', 'best', 4, 'SUBESCO_neutral', 0.28736279852688307, [94.0]], ['phase2', 'best', 4, 'SUBESCO_sad', 0.8095103733241558, [73.5]], ['phase2', 'best', 4, 'ShEMO_angry', 0.21042971447180805, [97.6303317535545]], ['phase2', 'best', 4, 'ShEMO_happy', 0.41430393047630787, [92.5]], ['phase2', 'best', 4, 'ShEMO_neutral', 0.23367814828709865, [95.60975609756098]], ['phase2', 'best', 4, 'ShEMO_sad', 1.1567277565096206, [70.78651685393258]], ['phase2', 'best', 4, 'UJ_ARABIC_angry', 0.8720047829242852, [61.53846153846154]], ['phase2', 'best', 4, 'UJ_ARABIC_happy', 0.6683743322889011, [83.33333333333333]], ['phase2', 'best', 4, 'UJ_ARABIC_neutral', 0.6954945304564069, [71.42857142857143]], ['phase2', 'best', 4, 'UJ_ARABIC_sad', 2.7718456046921864, [14.285714285714286]], ['phase2', 'best', 4, 'URDU_angry', 0.1761239476501942, [100.0]], ['phase2', 'best', 4, 'URDU_happy', 0.3733721382915974, [90.0]], ['phase2', 'best', 4, 'URDU_neutral', 0.35047106742858886, [90.0]], ['phase2', 'best', 4, 'URDU_sad', 0.49744015783071516, [90.0]], ['phase1', 'last', 4, 'CREMA-D_angry', 0.22938335868786638, [96.45669291338582]], ['phase1', 'last', 4, 'CREMA-D_happy', 0.256381470856704, [95.66929133858268]], ['phase1', 'last', 4, 'CREMA-D_neutral', 0.21377483525309146, [96.31336405529954]], ['phase1', 'last', 4, 'CREMA-D_sad', 0.6696316125238035, [79.13385826771653]], ['phase1', 'last', 4, 'SUBESCO_angry', 0.25442983388900764, [93.5]], ['phase1', 'last', 4, 'SUBESCO_happy', 0.3703559590131043, [91.5]], ['phase1', 'last', 4, 'SUBESCO_neutral', 0.2844240380078555, [94.0]], ['phase1', 'last', 4, 'SUBESCO_sad', 0.8398633705079555, [72.0]], ['phase1', 'last', 4, 'ShEMO_angry', 0.2041794370277234, [98.10426540284361]], ['phase1', 'last', 4, 'ShEMO_happy', 0.42243074290454385, [90.0]], ['phase1', 'last', 4, 'ShEMO_neutral', 0.2380644141173944, [95.60975609756098]], ['phase1', 'last', 4, 'ShEMO_sad', 1.1790745139456866, [70.78651685393258]], ['phase1', 'last', 4, 'UJ_ARABIC_angry', 0.8356973781035496, [61.53846153846154]], ['phase1', 'last', 4, 'UJ_ARABIC_happy', 0.6779217074314754, [66.66666666666667]], ['phase1', 'last', 4, 'UJ_ARABIC_neutral', 0.742201417684555, [71.42857142857143]], ['phase1', 'last', 4, 'UJ_ARABIC_sad', 2.8539814778736656, [14.285714285714286]], ['phase1', 'last', 4, 'URDU_angry', 0.16685246601700782, [100.0]], ['phase1', 'last', 4, 'URDU_happy', 0.3604918904602528, [90.0]], ['phase1', 'last', 4, 'URDU_neutral', 0.35472164079546925, [90.0]], ['phase1', 'last', 4, 'URDU_sad', 0.5122244484722613, [90.0]], ['phase2', 'last', 4, 'CREMA-D_angry', 0.23674321379952554, [96.06299212598425]], ['phase2', 'last', 4, 'CREMA-D_happy', 0.2533158694547929, [96.06299212598425]], ['phase2', 'last', 4, 'CREMA-D_neutral', 0.21404500030976836, [96.31336405529954]], ['phase2', 'last', 4, 'CREMA-D_sad', 0.6232991788800307, [80.31496062992126]], ['phase2', 'last', 4, 'SUBESCO_angry', 0.2674137561768295, [93.0]], ['phase2', 'last', 4, 'SUBESCO_happy', 0.3773390660434963, [91.5]], ['phase2', 'last', 4, 'SUBESCO_neutral', 0.28835168465971944, [94.0]], ['phase2', 'last', 4, 'SUBESCO_sad', 0.8041036230325703, [74.5]], ['phase2', 'last', 4, 'ShEMO_angry', 0.21114526173514775, [97.6303317535545]], ['phase2', 'last', 4, 'ShEMO_happy', 0.41422169618308546, [92.5]], ['phase2', 'last', 4, 'ShEMO_neutral', 0.23352482064468089, [95.60975609756098]], ['phase2', 'last', 4, 'ShEMO_sad', 1.1520001620389098, [71.91011235955057]], ['phase2', 'last', 4, 'UJ_ARABIC_angry', 0.8738656880763861, [61.53846153846154]], ['phase2', 'last', 4, 'UJ_ARABIC_happy', 0.668916642665863, [83.33333333333333]], ['phase2', 'last', 4, 'UJ_ARABIC_neutral', 0.6917763884578434, [71.42857142857143]], ['phase2', 'last', 4, 'UJ_ARABIC_sad', 2.7498678394726346, [14.285714285714286]], ['phase2', 'last', 4, 'URDU_angry', 0.17710236012935637, [100.0]], ['phase2', 'last', 4, 'URDU_happy', 0.3758681103587151, [90.0]], ['phase2', 'last', 4, 'URDU_neutral', 0.3506046175956726, [90.0]], ['phase2', 'last', 4, 'URDU_sad', 0.49426112696528435, [90.0]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'CREMA-D_angry',\n",
       "  0.23132538425875457,\n",
       "  [95.2755905511811]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'CREMA-D_happy',\n",
       "  0.30659085259897517,\n",
       "  [93.7007874015748]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2624118492213261,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'best', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]],\n",
       " ['phase1', 'best', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]],\n",
       " ['phase1', 'best', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]],\n",
       " ['phase1', 'best', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]],\n",
       " ['phase1', 'best', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]],\n",
       " ['phase1', 'best', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]],\n",
       " ['phase1', 'best', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'ShEMO_neutral',\n",
       "  0.27502004424246357,\n",
       "  [95.60975609756098]],\n",
       " ['phase1', 'best', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.6361714372268089,\n",
       "  [69.23076923076923]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.6464924265940984,\n",
       "  [83.33333333333333]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.7838817834854126,\n",
       "  [57.142857142857146]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.8529385775327682,\n",
       "  [28.571428571428573]],\n",
       " ['phase1', 'best', 0, 'URDU_angry', 0.14219589829444887, [100.0]],\n",
       " ['phase1', 'best', 0, 'URDU_happy', 0.4126186534762382, [85.0]],\n",
       " ['phase1', 'best', 0, 'URDU_neutral', 0.4651359885931015, [85.0]],\n",
       " ['phase1', 'best', 0, 'URDU_sad', 0.43485047519207004, [95.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'CREMA-D_angry',\n",
       "  0.258736290096298,\n",
       "  [94.09448818897638]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'CREMA-D_happy',\n",
       "  0.3128238437095964,\n",
       "  [93.7007874015748]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2521394816686482,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'best', 0, 'CREMA-D_sad', 0.6713012946168266, [77.55905511811024]],\n",
       " ['phase2', 'best', 0, 'SUBESCO_angry', 0.25672499284148226, [95.0]],\n",
       " ['phase2', 'best', 0, 'SUBESCO_happy', 0.3820594840496778, [92.0]],\n",
       " ['phase2', 'best', 0, 'SUBESCO_neutral', 0.23552342765033246, [95.5]],\n",
       " ['phase2', 'best', 0, 'SUBESCO_sad', 0.9597138513624667, [67.5]],\n",
       " ['phase2', 'best', 0, 'ShEMO_angry', 0.22040177592169052, [96.6824644549763]],\n",
       " ['phase2', 'best', 0, 'ShEMO_happy', 0.32229302674531946, [95.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'ShEMO_neutral',\n",
       "  0.2571009743504408,\n",
       "  [95.1219512195122]],\n",
       " ['phase2', 'best', 0, 'ShEMO_sad', 1.2437129648548833, [68.53932584269663]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.7015116925422962,\n",
       "  [69.23076923076923]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.6391726980606716,\n",
       "  [83.33333333333333]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.7040895244904927,\n",
       "  [57.142857142857146]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  0,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.8073663839272092,\n",
       "  [28.571428571428573]],\n",
       " ['phase2', 'best', 0, 'URDU_angry', 0.14747261628508565, [100.0]],\n",
       " ['phase2', 'best', 0, 'URDU_happy', 0.45416179522871986, [85.0]],\n",
       " ['phase2', 'best', 0, 'URDU_neutral', 0.4689487174153329, [85.0]],\n",
       " ['phase2', 'best', 0, 'URDU_sad', 0.4275880873203277, [95.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'CREMA-D_angry',\n",
       "  0.23132538425875457,\n",
       "  [95.2755905511811]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'CREMA-D_happy',\n",
       "  0.30659085259897517,\n",
       "  [93.7007874015748]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2624118492213261,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'last', 0, 'CREMA-D_sad', 0.6884657806885521, [76.77165354330708]],\n",
       " ['phase1', 'last', 0, 'SUBESCO_angry', 0.23836650431156167, [95.5]],\n",
       " ['phase1', 'last', 0, 'SUBESCO_happy', 0.3772680326551199, [92.0]],\n",
       " ['phase1', 'last', 0, 'SUBESCO_neutral', 0.24521834380924715, [95.0]],\n",
       " ['phase1', 'last', 0, 'SUBESCO_sad', 0.9525371509045363, [67.5]],\n",
       " ['phase1', 'last', 0, 'ShEMO_angry', 0.21216610582518927, [97.6303317535545]],\n",
       " ['phase1', 'last', 0, 'ShEMO_happy', 0.3126272886991502, [92.5]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'ShEMO_neutral',\n",
       "  0.27502004424246357,\n",
       "  [95.60975609756098]],\n",
       " ['phase1', 'last', 0, 'ShEMO_sad', 1.2037365490131162, [66.29213483146067]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.6361714372268089,\n",
       "  [69.23076923076923]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.6464924265940984,\n",
       "  [83.33333333333333]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.7838817834854126,\n",
       "  [57.142857142857146]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.8529385775327682,\n",
       "  [28.571428571428573]],\n",
       " ['phase1', 'last', 0, 'URDU_angry', 0.14219589829444887, [100.0]],\n",
       " ['phase1', 'last', 0, 'URDU_happy', 0.4126186534762382, [85.0]],\n",
       " ['phase1', 'last', 0, 'URDU_neutral', 0.4651359885931015, [85.0]],\n",
       " ['phase1', 'last', 0, 'URDU_sad', 0.43485047519207004, [95.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'CREMA-D_angry',\n",
       "  0.2588579188298993,\n",
       "  [93.7007874015748]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'CREMA-D_happy',\n",
       "  0.3136730649339872,\n",
       "  [93.7007874015748]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'CREMA-D_neutral',\n",
       "  0.25121473428291113,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'last', 0, 'CREMA-D_sad', 0.6725248215118728, [77.55905511811024]],\n",
       " ['phase2', 'last', 0, 'SUBESCO_angry', 0.25600903317332274, [95.0]],\n",
       " ['phase2', 'last', 0, 'SUBESCO_happy', 0.3834911134093998, [92.0]],\n",
       " ['phase2', 'last', 0, 'SUBESCO_neutral', 0.23549174077808857, [95.5]],\n",
       " ['phase2', 'last', 0, 'SUBESCO_sad', 0.9606494718790054, [67.0]],\n",
       " ['phase2', 'last', 0, 'ShEMO_angry', 0.22003566039399522, [97.1563981042654]],\n",
       " ['phase2', 'last', 0, 'ShEMO_happy', 0.32309657931327823, [95.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'ShEMO_neutral',\n",
       "  0.2568363875877568,\n",
       "  [95.1219512195122]],\n",
       " ['phase2', 'last', 0, 'ShEMO_sad', 1.2490537643767476, [66.29213483146067]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.6984921602102426,\n",
       "  [69.23076923076923]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.6390940000613531,\n",
       "  [83.33333333333333]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.7022053365196501,\n",
       "  [57.142857142857146]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  0,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.813152142933437,\n",
       "  [28.571428571428573]],\n",
       " ['phase2', 'last', 0, 'URDU_angry', 0.14700243920087813, [100.0]],\n",
       " ['phase2', 'last', 0, 'URDU_happy', 0.4553044453263283, [85.0]],\n",
       " ['phase2', 'last', 0, 'URDU_neutral', 0.4675500065088273, [85.0]],\n",
       " ['phase2', 'last', 0, 'URDU_sad', 0.42905081287026403, [95.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'CREMA-D_angry',\n",
       "  0.27615764127002945,\n",
       "  [94.88188976377953]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'CREMA-D_happy',\n",
       "  0.23527879508461533,\n",
       "  [95.66929133858268]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'CREMA-D_neutral',\n",
       "  0.25794078073193955,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'best', 1, 'CREMA-D_sad', 0.6431511204660413, [79.5275590551181]],\n",
       " ['phase1', 'best', 1, 'SUBESCO_angry', 0.3505234102904796, [91.0]],\n",
       " ['phase1', 'best', 1, 'SUBESCO_happy', 0.30056495204567907, [91.5]],\n",
       " ['phase1', 'best', 1, 'SUBESCO_neutral', 0.32848145671188816, [92.0]],\n",
       " ['phase1', 'best', 1, 'SUBESCO_sad', 0.8100879185646771, [77.0]],\n",
       " ['phase1', 'best', 1, 'ShEMO_angry', 0.20217314553204307, [97.6303317535545]],\n",
       " ['phase1', 'best', 1, 'ShEMO_happy', 0.33687370978295794, [87.5]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'ShEMO_neutral',\n",
       "  0.2413041352498823,\n",
       "  [96.09756097560975]],\n",
       " ['phase1', 'best', 1, 'ShEMO_sad', 0.9465443391478462, [69.66292134831461]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'UJ_ARABIC_angry',\n",
       "  1.5619367189132252,\n",
       "  [61.53846153846154]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.4149790306886037,\n",
       "  [83.33333333333333]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  1,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6496904577527727,\n",
       "  [85.71428571428571]],\n",
       " ['phase1', 'best', 1, 'UJ_ARABIC_sad', 2.2628096171787804, [0.0]],\n",
       " ['phase1', 'best', 1, 'URDU_angry', 0.18128371462225912, [100.0]],\n",
       " ['phase1', 'best', 1, 'URDU_happy', 0.40306554064154626, [90.0]],\n",
       " ['phase1', 'best', 1, 'URDU_neutral', 0.26644408404827113, [95.0]],\n",
       " ['phase1', 'best', 1, 'URDU_sad', 0.36069156974554056, [95.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'CREMA-D_angry',\n",
       "  0.29083810124810267,\n",
       "  [93.30708661417323]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'CREMA-D_happy',\n",
       "  0.23703179144718511,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2536086365237215,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'best', 1, 'CREMA-D_sad', 0.6164421358798431, [81.10236220472441]],\n",
       " ['phase2', 'best', 1, 'SUBESCO_angry', 0.3682341238856316, [91.0]],\n",
       " ['phase2', 'best', 1, 'SUBESCO_happy', 0.3087896878272295, [92.5]],\n",
       " ['phase2', 'best', 1, 'SUBESCO_neutral', 0.3148857557773592, [93.0]],\n",
       " ['phase2', 'best', 1, 'SUBESCO_sad', 0.8102790326625107, [76.5]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'ShEMO_angry',\n",
       "  0.20505267777149144,\n",
       "  [98.10426540284361]],\n",
       " ['phase2', 'best', 1, 'ShEMO_happy', 0.3488592237234116, [87.5]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'ShEMO_neutral',\n",
       "  0.22883073178733276,\n",
       "  [96.58536585365853]],\n",
       " ['phase2', 'best', 1, 'ShEMO_sad', 0.928263324365187, [71.91011235955057]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'UJ_ARABIC_angry',\n",
       "  1.614923622745734,\n",
       "  [61.53846153846154]],\n",
       " ['phase2', 'best', 1, 'UJ_ARABIC_happy', 0.4457362964749336, [100.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  1,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.5752684026956558,\n",
       "  [85.71428571428571]],\n",
       " ['phase2', 'best', 1, 'UJ_ARABIC_sad', 2.1905118397303993, [0.0]],\n",
       " ['phase2', 'best', 1, 'URDU_angry', 0.19813046157360079, [95.0]],\n",
       " ['phase2', 'best', 1, 'URDU_happy', 0.432581639289856, [90.0]],\n",
       " ['phase2', 'best', 1, 'URDU_neutral', 0.27857217714190496, [95.0]],\n",
       " ['phase2', 'best', 1, 'URDU_sad', 0.3698753505945205, [95.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'CREMA-D_angry',\n",
       "  0.2761957376726031,\n",
       "  [94.09448818897638]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'CREMA-D_happy',\n",
       "  0.23440244113366435,\n",
       "  [94.88188976377953]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2563687742168453,\n",
       "  [96.31336405529954]],\n",
       " ['phase1', 'last', 1, 'CREMA-D_sad', 0.6558136834990316, [79.13385826771653]],\n",
       " ['phase1', 'last', 1, 'SUBESCO_angry', 0.3387189420312643, [91.5]],\n",
       " ['phase1', 'last', 1, 'SUBESCO_happy', 0.30185804873704913, [93.0]],\n",
       " ['phase1', 'last', 1, 'SUBESCO_neutral', 0.325886485427618, [92.0]],\n",
       " ['phase1', 'last', 1, 'SUBESCO_sad', 0.8205169849097728, [76.5]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'ShEMO_angry',\n",
       "  0.19906430384276588,\n",
       "  [98.10426540284361]],\n",
       " ['phase1', 'last', 1, 'ShEMO_happy', 0.343780105188489, [87.5]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'ShEMO_neutral',\n",
       "  0.2343073303379663,\n",
       "  [96.09756097560975]],\n",
       " ['phase1', 'last', 1, 'ShEMO_sad', 0.9585065647457423, [70.78651685393258]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'UJ_ARABIC_angry',\n",
       "  1.5495183548102012,\n",
       "  [61.53846153846154]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.4166577458381653,\n",
       "  [83.33333333333333]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  1,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6069116996867316,\n",
       "  [85.71428571428571]],\n",
       " ['phase1', 'last', 1, 'UJ_ARABIC_sad', 2.3494340351649696, [0.0]],\n",
       " ['phase1', 'last', 1, 'URDU_angry', 0.18395565971732136, [100.0]],\n",
       " ['phase1', 'last', 1, 'URDU_happy', 0.3975479431450366, [90.0]],\n",
       " ['phase1', 'last', 1, 'URDU_neutral', 0.2688737452030182, [95.0]],\n",
       " ['phase1', 'last', 1, 'URDU_sad', 0.3800719514489175, [90.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'CREMA-D_angry',\n",
       "  0.2906135099492672,\n",
       "  [93.30708661417323]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'CREMA-D_happy',\n",
       "  0.2367331867260258,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2552252822482642,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'last', 1, 'CREMA-D_sad', 0.6159147716999991, [81.10236220472441]],\n",
       " ['phase2', 'last', 1, 'SUBESCO_angry', 0.3666904728859662, [91.0]],\n",
       " ['phase2', 'last', 1, 'SUBESCO_happy', 0.30799106523394587, [92.5]],\n",
       " ['phase2', 'last', 1, 'SUBESCO_neutral', 0.3148298836499454, [93.0]],\n",
       " ['phase2', 'last', 1, 'SUBESCO_sad', 0.8127771169692279, [76.5]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'ShEMO_angry',\n",
       "  0.20459069615291772,\n",
       "  [98.10426540284361]],\n",
       " ['phase2', 'last', 1, 'ShEMO_happy', 0.34735414050519464, [87.5]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'ShEMO_neutral',\n",
       "  0.2300839459750711,\n",
       "  [96.58536585365853]],\n",
       " ['phase2', 'last', 1, 'ShEMO_sad', 0.9299659379077759, [71.91011235955057]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'UJ_ARABIC_angry',\n",
       "  1.6147538813260884,\n",
       "  [61.53846153846154]],\n",
       " ['phase2', 'last', 1, 'UJ_ARABIC_happy', 0.4460348735253016, [100.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  1,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.584535104887826,\n",
       "  [85.71428571428571]],\n",
       " ['phase2', 'last', 1, 'UJ_ARABIC_sad', 2.1865100519997736, [0.0]],\n",
       " ['phase2', 'last', 1, 'URDU_angry', 0.19664481729269026, [95.0]],\n",
       " ['phase2', 'last', 1, 'URDU_happy', 0.4312362901866436, [90.0]],\n",
       " ['phase2', 'last', 1, 'URDU_neutral', 0.28115319460630417, [95.0]],\n",
       " ['phase2', 'last', 1, 'URDU_sad', 0.37026820853352543, [95.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'CREMA-D_angry',\n",
       "  0.2533235727684707,\n",
       "  [93.7007874015748]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'CREMA-D_happy',\n",
       "  0.2530228722048557,\n",
       "  [95.2755905511811]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'CREMA-D_neutral',\n",
       "  0.23457274898406,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'best', 2, 'CREMA-D_sad', 0.7033744433264096, [74.80314960629921]],\n",
       " ['phase1', 'best', 2, 'SUBESCO_angry', 0.28369085997343063, [91.5]],\n",
       " ['phase1', 'best', 2, 'SUBESCO_happy', 0.354978576526046, [91.0]],\n",
       " ['phase1', 'best', 2, 'SUBESCO_neutral', 0.29951987594366075, [94.0]],\n",
       " ['phase1', 'best', 2, 'SUBESCO_sad', 0.8211694458872081, [76.0]],\n",
       " ['phase1', 'best', 2, 'ShEMO_angry', 0.22963467148525457, [94.7867298578199]],\n",
       " ['phase1', 'best', 2, 'ShEMO_happy', 0.37117515616118907, [87.5]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'ShEMO_neutral',\n",
       "  0.2645087184702479,\n",
       "  [95.1219512195122]],\n",
       " ['phase1', 'best', 2, 'ShEMO_sad', 1.1678407711259435, [66.29213483146067]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.8589285772580367,\n",
       "  [69.23076923076923]],\n",
       " ['phase1', 'best', 2, 'UJ_ARABIC_happy', 1.1219921261072159, [50.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6927995277302607,\n",
       "  [71.42857142857143]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.4228394670145854,\n",
       "  [14.285714285714286]],\n",
       " ['phase1', 'best', 2, 'URDU_angry', 0.13714252263307572, [100.0]],\n",
       " ['phase1', 'best', 2, 'URDU_happy', 0.5723968669772149, [85.0]],\n",
       " ['phase1', 'best', 2, 'URDU_neutral', 0.37807183638215064, [95.0]],\n",
       " ['phase1', 'best', 2, 'URDU_sad', 0.5459306403994562, [85.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'CREMA-D_angry',\n",
       "  0.26008654121808183,\n",
       "  [93.30708661417323]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'CREMA-D_happy',\n",
       "  0.27641784440814043,\n",
       "  [94.48818897637796]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'CREMA-D_neutral',\n",
       "  0.23941651906835326,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'best', 2, 'CREMA-D_sad', 0.6129553528634583, [78.74015748031496]],\n",
       " ['phase2', 'best', 2, 'SUBESCO_angry', 0.28470431238412836, [92.5]],\n",
       " ['phase2', 'best', 2, 'SUBESCO_happy', 0.37439083494246017, [91.0]],\n",
       " ['phase2', 'best', 2, 'SUBESCO_neutral', 0.27766273468732844, [94.5]],\n",
       " ['phase2', 'best', 2, 'SUBESCO_sad', 0.8193467020243407, [75.0]],\n",
       " ['phase2', 'best', 2, 'ShEMO_angry', 0.25258563324738453, [94.7867298578199]],\n",
       " ['phase2', 'best', 2, 'ShEMO_happy', 0.42693374641239645, [85.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'ShEMO_neutral',\n",
       "  0.24538376142338997,\n",
       "  [96.58536585365853]],\n",
       " ['phase2', 'best', 2, 'ShEMO_sad', 1.1382430495505926, [67.41573033707866]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.8296753810002254,\n",
       "  [69.23076923076923]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.3359356770912805,\n",
       "  [16.666666666666668]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6173683766807828,\n",
       "  [71.42857142857143]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  2,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.1588183109249384,\n",
       "  [14.285714285714286]],\n",
       " ['phase2', 'best', 2, 'URDU_angry', 0.1395458072423935, [100.0]],\n",
       " ['phase2', 'best', 2, 'URDU_happy', 0.6497519172728062, [85.0]],\n",
       " ['phase2', 'best', 2, 'URDU_neutral', 0.3505798734724522, [90.0]],\n",
       " ['phase2', 'best', 2, 'URDU_sad', 0.5304099045693875, [85.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'CREMA-D_angry',\n",
       "  0.24984035330025234,\n",
       "  [93.7007874015748]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'CREMA-D_happy',\n",
       "  0.26827800320828044,\n",
       "  [94.48818897637796]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'CREMA-D_neutral',\n",
       "  0.22523403277594906,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'last', 2, 'CREMA-D_sad', 0.7022455512421336, [74.40944881889764]],\n",
       " ['phase1', 'last', 2, 'SUBESCO_angry', 0.27640943221747877, [92.5]],\n",
       " ['phase1', 'last', 2, 'SUBESCO_happy', 0.3622385337948799, [91.5]],\n",
       " ['phase1', 'last', 2, 'SUBESCO_neutral', 0.2780804342031481, [94.0]],\n",
       " ['phase1', 'last', 2, 'SUBESCO_sad', 0.8708563620597127, [72.0]],\n",
       " ['phase1', 'last', 2, 'ShEMO_angry', 0.23695080533129362, [95.260663507109]],\n",
       " ['phase1', 'last', 2, 'ShEMO_happy', 0.4093896470963954, [87.5]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'ShEMO_neutral',\n",
       "  0.25657484931189845,\n",
       "  [96.09756097560975]],\n",
       " ['phase1', 'last', 2, 'ShEMO_sad', 1.2044240768705867, [64.04494382022472]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.822608069731639,\n",
       "  [69.23076923076923]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.2717645491162937,\n",
       "  [33.333333333333336]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6440192397151675,\n",
       "  [71.42857142857143]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.438488530261176,\n",
       "  [14.285714285714286]],\n",
       " ['phase1', 'last', 2, 'URDU_angry', 0.13764138370752335, [100.0]],\n",
       " ['phase1', 'last', 2, 'URDU_happy', 0.6141036458313465, [85.0]],\n",
       " ['phase1', 'last', 2, 'URDU_neutral', 0.3679290011525155, [95.0]],\n",
       " ['phase1', 'last', 2, 'URDU_sad', 0.5879364408552648, [85.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'CREMA-D_angry',\n",
       "  0.26902664311992847,\n",
       "  [92.91338582677166]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'CREMA-D_happy',\n",
       "  0.2753782981024015,\n",
       "  [94.09448818897638]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2353055673535518,\n",
       "  [96.7741935483871]],\n",
       " ['phase2', 'last', 2, 'CREMA-D_sad', 0.6381505948352062, [76.77165354330708]],\n",
       " ['phase2', 'last', 2, 'SUBESCO_angry', 0.28951828539371477, [93.0]],\n",
       " ['phase2', 'last', 2, 'SUBESCO_happy', 0.3764164911955594, [91.0]],\n",
       " ['phase2', 'last', 2, 'SUBESCO_neutral', 0.2734559410065411, [94.5]],\n",
       " ['phase2', 'last', 2, 'SUBESCO_sad', 0.8367165967077014, [75.0]],\n",
       " ['phase2', 'last', 2, 'ShEMO_angry', 0.2624106809002526, [94.31279620853081]],\n",
       " ['phase2', 'last', 2, 'ShEMO_happy', 0.4383957624435425, [85.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'ShEMO_neutral',\n",
       "  0.24030565939298504,\n",
       "  [96.58536585365853]],\n",
       " ['phase2', 'last', 2, 'ShEMO_sad', 1.1727998432483562, [66.29213483146067]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.8413258343935013,\n",
       "  [69.23076923076923]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.3033907612164815,\n",
       "  [33.333333333333336]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6183227258069175,\n",
       "  [71.42857142857143]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  2,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.216776783977236,\n",
       "  [14.285714285714286]],\n",
       " ['phase2', 'last', 2, 'URDU_angry', 0.1392757534980774, [100.0]],\n",
       " ['phase2', 'last', 2, 'URDU_happy', 0.6227635897696018, [85.0]],\n",
       " ['phase2', 'last', 2, 'URDU_neutral', 0.33118396773934367, [90.0]],\n",
       " ['phase2', 'last', 2, 'URDU_sad', 0.5625272803008556, [75.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'CREMA-D_angry',\n",
       "  0.21897121407384934,\n",
       "  [96.06299212598425]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'CREMA-D_happy',\n",
       "  0.2555635284016454,\n",
       "  [95.66929133858268]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'CREMA-D_neutral',\n",
       "  0.21585286693638914,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'best', 3, 'CREMA-D_sad', 0.7073777377840097, [75.19685039370079]],\n",
       " ['phase1', 'best', 3, 'SUBESCO_angry', 0.23710701055824762, [95.5]],\n",
       " ['phase1', 'best', 3, 'SUBESCO_happy', 0.3434567285329101, [92.0]],\n",
       " ['phase1', 'best', 3, 'SUBESCO_neutral', 0.281279255375266, [93.5]],\n",
       " ['phase1', 'best', 3, 'SUBESCO_sad', 0.7725224840641027, [76.5]],\n",
       " ['phase1', 'best', 3, 'ShEMO_angry', 0.2588179732244728, [96.6824644549763]],\n",
       " ['phase1', 'best', 3, 'ShEMO_happy', 0.4878143709152935, [90.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'ShEMO_neutral',\n",
       "  0.2344266333957997,\n",
       "  [96.09756097560975]],\n",
       " ['phase1', 'best', 3, 'ShEMO_sad', 1.054832771755336, [70.78651685393258]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.5241244079974982,\n",
       "  [76.92307692307692]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.225831707318624,\n",
       "  [66.66666666666667]],\n",
       " ['phase1', 'best', 3, 'UJ_ARABIC_neutral', 0.21521313275609696, [100.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  3,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.5891656854322977,\n",
       "  [28.571428571428573]],\n",
       " ['phase1', 'best', 3, 'URDU_angry', 0.14373421147465704, [100.0]],\n",
       " ['phase1', 'best', 3, 'URDU_happy', 0.23830564320087436, [95.0]],\n",
       " ['phase1', 'best', 3, 'URDU_neutral', 0.3117356546223164, [95.0]],\n",
       " ['phase1', 'best', 3, 'URDU_sad', 0.27738759741187097, [95.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'CREMA-D_angry',\n",
       "  0.22364605470435836,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'CREMA-D_happy',\n",
       "  0.2588937213336391,\n",
       "  [95.2755905511811]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'CREMA-D_neutral',\n",
       "  0.23196233101704164,\n",
       "  [96.7741935483871]],\n",
       " ['phase2', 'best', 3, 'CREMA-D_sad', 0.6369640124360407, [77.95275590551181]],\n",
       " ['phase2', 'best', 3, 'SUBESCO_angry', 0.2467236997187138, [94.5]],\n",
       " ['phase2', 'best', 3, 'SUBESCO_happy', 0.3540948653966189, [92.5]],\n",
       " ['phase2', 'best', 3, 'SUBESCO_neutral', 0.28407134577631943, [93.0]],\n",
       " ['phase2', 'best', 3, 'SUBESCO_sad', 0.7273649083822967, [76.0]],\n",
       " ['phase2', 'best', 3, 'ShEMO_angry', 0.2668418175942524, [96.6824644549763]],\n",
       " ['phase2', 'best', 3, 'ShEMO_happy', 0.5090705700218677, [87.5]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'ShEMO_neutral',\n",
       "  0.22783162891864767,\n",
       "  [96.09756097560975]],\n",
       " ['phase2', 'best', 3, 'ShEMO_sad', 1.0217368902450197, [70.78651685393258]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.6161791051809604,\n",
       "  [76.92307692307692]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.2210888316233952,\n",
       "  [66.66666666666667]],\n",
       " ['phase2', 'best', 3, 'UJ_ARABIC_neutral', 0.20654104011399407, [100.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  3,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.339374744466373,\n",
       "  [28.571428571428573]],\n",
       " ['phase2', 'best', 3, 'URDU_angry', 0.14633126333355903, [100.0]],\n",
       " ['phase2', 'best', 3, 'URDU_happy', 0.24981301203370096, [95.0]],\n",
       " ['phase2', 'best', 3, 'URDU_neutral', 0.34116626679897305, [95.0]],\n",
       " ['phase2', 'best', 3, 'URDU_sad', 0.2781838342547417, [90.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'CREMA-D_angry',\n",
       "  0.2153352019120388,\n",
       "  [96.06299212598425]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'CREMA-D_happy',\n",
       "  0.2583057849895295,\n",
       "  [95.66929133858268]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'CREMA-D_neutral',\n",
       "  0.21157623127034186,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'last', 3, 'CREMA-D_sad', 0.7347471391943499, [72.83464566929133]],\n",
       " ['phase1', 'last', 3, 'SUBESCO_angry', 0.23271313518285763, [96.5]],\n",
       " ['phase1', 'last', 3, 'SUBESCO_happy', 0.34578350819647313, [92.0]],\n",
       " ['phase1', 'last', 3, 'SUBESCO_neutral', 0.27215616077184673, [93.5]],\n",
       " ['phase1', 'last', 3, 'SUBESCO_sad', 0.7787246438860891, [76.5]],\n",
       " ['phase1', 'last', 3, 'ShEMO_angry', 0.257463634155373, [96.6824644549763]],\n",
       " ['phase1', 'last', 3, 'ShEMO_happy', 0.4921661842614412, [90.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'ShEMO_neutral',\n",
       "  0.2355556437881983,\n",
       "  [96.09756097560975]],\n",
       " ['phase1', 'last', 3, 'ShEMO_sad', 1.080654445156622, [69.66292134831461]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.5200148075819016,\n",
       "  [76.92307692307692]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.2223902692397437,\n",
       "  [66.66666666666667]],\n",
       " ['phase1', 'last', 3, 'UJ_ARABIC_neutral', 0.2123784806047167, [100.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  3,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.635129132441112,\n",
       "  [28.571428571428573]],\n",
       " ['phase1', 'last', 3, 'URDU_angry', 0.1415281191468239, [100.0]],\n",
       " ['phase1', 'last', 3, 'URDU_happy', 0.24233263134956357, [95.0]],\n",
       " ['phase1', 'last', 3, 'URDU_neutral', 0.30699191167950635, [95.0]],\n",
       " ['phase1', 'last', 3, 'URDU_sad', 0.2787277489900589, [95.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'CREMA-D_angry',\n",
       "  0.2242271861457449,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'CREMA-D_happy',\n",
       "  0.25730810496281437,\n",
       "  [95.2755905511811]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2256125964327342,\n",
       "  [96.7741935483871]],\n",
       " ['phase2', 'last', 3, 'CREMA-D_sad', 0.6582772473063997, [77.16535433070867]],\n",
       " ['phase2', 'last', 3, 'SUBESCO_angry', 0.24562619954347603, [94.5]],\n",
       " ['phase2', 'last', 3, 'SUBESCO_happy', 0.3530384108424185, [92.5]],\n",
       " ['phase2', 'last', 3, 'SUBESCO_neutral', 0.2823210334777832, [93.5]],\n",
       " ['phase2', 'last', 3, 'SUBESCO_sad', 0.7386462476849557, [76.0]],\n",
       " ['phase2', 'last', 3, 'ShEMO_angry', 0.2679040304151193, [96.6824644549763]],\n",
       " ['phase2', 'last', 3, 'ShEMO_happy', 0.5070606425404547, [87.5]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'ShEMO_neutral',\n",
       "  0.22696804317032415,\n",
       "  [96.09756097560975]],\n",
       " ['phase2', 'last', 3, 'ShEMO_sad', 1.0379315956255022, [70.78651685393258]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.5962140605999874,\n",
       "  [76.92307692307692]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'UJ_ARABIC_happy',\n",
       "  1.225340927640597,\n",
       "  [66.66666666666667]],\n",
       " ['phase2', 'last', 3, 'UJ_ARABIC_neutral', 0.2006605267524719, [100.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  3,\n",
       "  'UJ_ARABIC_sad',\n",
       "  1.3891573633466447,\n",
       "  [28.571428571428573]],\n",
       " ['phase2', 'last', 3, 'URDU_angry', 0.14554552957415579, [100.0]],\n",
       " ['phase2', 'last', 3, 'URDU_happy', 0.24518935307860376, [95.0]],\n",
       " ['phase2', 'last', 3, 'URDU_neutral', 0.3288537606596947, [95.0]],\n",
       " ['phase2', 'last', 3, 'URDU_sad', 0.2796537265181542, [90.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'CREMA-D_angry',\n",
       "  0.23601596966737845,\n",
       "  [96.06299212598425]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'CREMA-D_happy',\n",
       "  0.2647190968704035,\n",
       "  [94.48818897637796]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2174992754986759,\n",
       "  [96.7741935483871]],\n",
       " ['phase1', 'best', 4, 'CREMA-D_sad', 0.6358191324031262, [77.95275590551181]],\n",
       " ['phase1', 'best', 4, 'SUBESCO_angry', 0.26032150544226146, [94.0]],\n",
       " ['phase1', 'best', 4, 'SUBESCO_happy', 0.4046996270120143, [91.0]],\n",
       " ['phase1', 'best', 4, 'SUBESCO_neutral', 0.2816057708114385, [94.5]],\n",
       " ['phase1', 'best', 4, 'SUBESCO_sad', 0.7527326751500368, [74.5]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'ShEMO_angry',\n",
       "  0.21090828560257413,\n",
       "  [98.10426540284361]],\n",
       " ['phase1', 'best', 4, 'ShEMO_happy', 0.43363358937203883, [90.0]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'ShEMO_neutral',\n",
       "  0.24421804510965586,\n",
       "  [95.60975609756098]],\n",
       " ['phase1', 'best', 4, 'ShEMO_sad', 1.0757271935430806, [71.91011235955057]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.9124016773242217,\n",
       "  [61.53846153846154]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.7047847310702007,\n",
       "  [66.66666666666667]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.7610876028026854,\n",
       "  [71.42857142857143]],\n",
       " ['phase1',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.6012486730303084,\n",
       "  [14.285714285714286]],\n",
       " ['phase1', 'best', 4, 'URDU_angry', 0.17406784221529958, [100.0]],\n",
       " ['phase1', 'best', 4, 'URDU_happy', 0.4008956432342529, [90.0]],\n",
       " ['phase1', 'best', 4, 'URDU_neutral', 0.38109260052442556, [90.0]],\n",
       " ['phase1', 'best', 4, 'URDU_sad', 0.4538943864405155, [90.0]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'CREMA-D_angry',\n",
       "  0.2361262004206499,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'CREMA-D_happy',\n",
       "  0.25317100640826345,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'CREMA-D_neutral',\n",
       "  0.2131844287918461,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'best', 4, 'CREMA-D_sad', 0.6317807730492643, [80.31496062992126]],\n",
       " ['phase2', 'best', 4, 'SUBESCO_angry', 0.2663305568695069, [93.0]],\n",
       " ['phase2', 'best', 4, 'SUBESCO_happy', 0.3762897613644597, [91.5]],\n",
       " ['phase2', 'best', 4, 'SUBESCO_neutral', 0.28736279852688307, [94.0]],\n",
       " ['phase2', 'best', 4, 'SUBESCO_sad', 0.8095103733241558, [73.5]],\n",
       " ['phase2', 'best', 4, 'ShEMO_angry', 0.21042971447180805, [97.6303317535545]],\n",
       " ['phase2', 'best', 4, 'ShEMO_happy', 0.41430393047630787, [92.5]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'ShEMO_neutral',\n",
       "  0.23367814828709865,\n",
       "  [95.60975609756098]],\n",
       " ['phase2', 'best', 4, 'ShEMO_sad', 1.1567277565096206, [70.78651685393258]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.8720047829242852,\n",
       "  [61.53846153846154]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.6683743322889011,\n",
       "  [83.33333333333333]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6954945304564069,\n",
       "  [71.42857142857143]],\n",
       " ['phase2',\n",
       "  'best',\n",
       "  4,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.7718456046921864,\n",
       "  [14.285714285714286]],\n",
       " ['phase2', 'best', 4, 'URDU_angry', 0.1761239476501942, [100.0]],\n",
       " ['phase2', 'best', 4, 'URDU_happy', 0.3733721382915974, [90.0]],\n",
       " ['phase2', 'best', 4, 'URDU_neutral', 0.35047106742858886, [90.0]],\n",
       " ['phase2', 'best', 4, 'URDU_sad', 0.49744015783071516, [90.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'CREMA-D_angry',\n",
       "  0.22938335868786638,\n",
       "  [96.45669291338582]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'CREMA-D_happy',\n",
       "  0.256381470856704,\n",
       "  [95.66929133858268]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'CREMA-D_neutral',\n",
       "  0.21377483525309146,\n",
       "  [96.31336405529954]],\n",
       " ['phase1', 'last', 4, 'CREMA-D_sad', 0.6696316125238035, [79.13385826771653]],\n",
       " ['phase1', 'last', 4, 'SUBESCO_angry', 0.25442983388900764, [93.5]],\n",
       " ['phase1', 'last', 4, 'SUBESCO_happy', 0.3703559590131043, [91.5]],\n",
       " ['phase1', 'last', 4, 'SUBESCO_neutral', 0.2844240380078555, [94.0]],\n",
       " ['phase1', 'last', 4, 'SUBESCO_sad', 0.8398633705079555, [72.0]],\n",
       " ['phase1', 'last', 4, 'ShEMO_angry', 0.2041794370277234, [98.10426540284361]],\n",
       " ['phase1', 'last', 4, 'ShEMO_happy', 0.42243074290454385, [90.0]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'ShEMO_neutral',\n",
       "  0.2380644141173944,\n",
       "  [95.60975609756098]],\n",
       " ['phase1', 'last', 4, 'ShEMO_sad', 1.1790745139456866, [70.78651685393258]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.8356973781035496,\n",
       "  [61.53846153846154]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.6779217074314754,\n",
       "  [66.66666666666667]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.742201417684555,\n",
       "  [71.42857142857143]],\n",
       " ['phase1',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.8539814778736656,\n",
       "  [14.285714285714286]],\n",
       " ['phase1', 'last', 4, 'URDU_angry', 0.16685246601700782, [100.0]],\n",
       " ['phase1', 'last', 4, 'URDU_happy', 0.3604918904602528, [90.0]],\n",
       " ['phase1', 'last', 4, 'URDU_neutral', 0.35472164079546925, [90.0]],\n",
       " ['phase1', 'last', 4, 'URDU_sad', 0.5122244484722613, [90.0]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'CREMA-D_angry',\n",
       "  0.23674321379952554,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'CREMA-D_happy',\n",
       "  0.2533158694547929,\n",
       "  [96.06299212598425]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'CREMA-D_neutral',\n",
       "  0.21404500030976836,\n",
       "  [96.31336405529954]],\n",
       " ['phase2', 'last', 4, 'CREMA-D_sad', 0.6232991788800307, [80.31496062992126]],\n",
       " ['phase2', 'last', 4, 'SUBESCO_angry', 0.2674137561768295, [93.0]],\n",
       " ['phase2', 'last', 4, 'SUBESCO_happy', 0.3773390660434963, [91.5]],\n",
       " ['phase2', 'last', 4, 'SUBESCO_neutral', 0.28835168465971944, [94.0]],\n",
       " ['phase2', 'last', 4, 'SUBESCO_sad', 0.8041036230325703, [74.5]],\n",
       " ['phase2', 'last', 4, 'ShEMO_angry', 0.21114526173514775, [97.6303317535545]],\n",
       " ['phase2', 'last', 4, 'ShEMO_happy', 0.41422169618308546, [92.5]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'ShEMO_neutral',\n",
       "  0.23352482064468089,\n",
       "  [95.60975609756098]],\n",
       " ['phase2', 'last', 4, 'ShEMO_sad', 1.1520001620389098, [71.91011235955057]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_angry',\n",
       "  0.8738656880763861,\n",
       "  [61.53846153846154]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_happy',\n",
       "  0.668916642665863,\n",
       "  [83.33333333333333]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_neutral',\n",
       "  0.6917763884578434,\n",
       "  [71.42857142857143]],\n",
       " ['phase2',\n",
       "  'last',\n",
       "  4,\n",
       "  'UJ_ARABIC_sad',\n",
       "  2.7498678394726346,\n",
       "  [14.285714285714286]],\n",
       " ['phase2', 'last', 4, 'URDU_angry', 0.17710236012935637, [100.0]],\n",
       " ['phase2', 'last', 4, 'URDU_happy', 0.3758681103587151, [90.0]],\n",
       " ['phase2', 'last', 4, 'URDU_neutral', 0.3506046175956726, [90.0]],\n",
       " ['phase2', 'last', 4, 'URDU_sad', 0.49426112696528435, [90.0]]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.train import valid_one_epoch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import src.model as models\n",
    "from src.helpers import train_test_split, seed, load_model\n",
    "from src.data import audio_data_loader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "df =pd.read_csv('data/metadata.csv')\n",
    "checkpoints_loading_order = ['best', 'last']\n",
    "results =[]\n",
    "df =df[~((df['Emotion']=='surprise') | (df['Emotion']=='disgust')| (df['Emotion']=='fear') |  (df['Dataset']=='RAVDESS')|(df['Dataset']=='EYASE')| (df['Dataset']=='SaudiEMO') )].reset_index(drop=True) #|(df['Dataset']=='SaudiEMO')| (df['Dataset']=='RAVDESS')| (df['Dataset']=='EYASE') | (df['Dataset']=='CREMA-D') | (df['Dataset']=='ShEMO')| (df['Dataset']=='CREMA-D')\n",
    "# df =df[df['Dataset']=='UJ_ARABIC' ] #| ((df['Dataset']=='SaudiEMO') & (df['Emotion']=='sad')) |(df['Emotion']=='neutral') (df['Dataset']=='EYASE') | ((df['Dataset']=='SaudiEMO')&((df['Emotion']!='happy')))|\n",
    "counts=df['Emotion'].value_counts()\n",
    "df['stratify_on'] = df['Dataset']+'_'+df['Emotion']\n",
    "df['Emotion'] = df['Emotion'].astype(\"category\")\n",
    "df['target']=df['Emotion'].cat.codes.astype(np.int64)\n",
    "\n",
    "model = getattr(models, \"FullEm2vecGRU384h\")(num_classes=len(counts)).cuda()\n",
    "\n",
    "loss =lambda output,target: F.cross_entropy(output, target,label_smoothing=0.03)\n",
    "for fold in range(5):\n",
    "    current_seed = seed + fold\n",
    "    \n",
    "    torch.manual_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    \n",
    "\n",
    "    train_fold, valid_fold = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2,\n",
    "        stratify=df['stratify_on'],\n",
    "        random_state=current_seed\n",
    "    )\n",
    "\n",
    "    model_name = f\"phase1_fold{fold}\"\n",
    "    load_model(model_name,model,loading_order=checkpoints_loading_order, checkpoints_dir=\"experiments/20241130_064549_two_phase_training/model_checkpoints\")#checkpoints_loading_order[::-1]\n",
    "    # phase1\n",
    "    for dataset in valid_fold['stratify_on'].unique():\n",
    "        data_df = valid_fold[valid_fold['stratify_on']==dataset].reset_index(drop=True)\n",
    "        valid_loader = audio_data_loader(data_df,batch_size=1 ,shuffle=False)\n",
    "        results.append([\"phase1\",\"best\",fold,dataset, *valid_one_epoch(valid_loader, model, loss)])\n",
    "\n",
    "    model_name = f\"phase2_fold{fold}\"\n",
    "    load_model(model_name,model,loading_order=checkpoints_loading_order, checkpoints_dir=\"experiments/20241130_064549_two_phase_training/model_checkpoints\")#checkpoints_loading_order[::-1]\n",
    "    # phase1\n",
    "    for dataset in valid_fold['stratify_on'].unique():\n",
    "        data_df = valid_fold[valid_fold['stratify_on']==dataset].reset_index(drop=True)\n",
    "        valid_loader = audio_data_loader(data_df,batch_size=1 ,shuffle=False)\n",
    "        results.append([\"phase2\",\"best\",fold,dataset, *valid_one_epoch(valid_loader, model, loss)])\n",
    "\n",
    "    model_name = f\"phase1_fold{fold}\"\n",
    "    load_model(model_name,model,loading_order=checkpoints_loading_order[::-1], checkpoints_dir=\"experiments/20241130_064549_two_phase_training/model_checkpoints\")#checkpoints_loading_order[::-1]\n",
    "    # phase1\n",
    "    for dataset in valid_fold['stratify_on'].unique():\n",
    "        data_df = valid_fold[valid_fold['stratify_on']==dataset].reset_index(drop=True)\n",
    "        valid_loader = audio_data_loader(data_df,batch_size=1 ,shuffle=False)\n",
    "        results.append([\"phase1\",\"last\",fold,dataset, *valid_one_epoch(valid_loader, model, loss)])\n",
    "\n",
    "    model_name = f\"phase2_fold{fold}\"\n",
    "    load_model(model_name,model,loading_order=checkpoints_loading_order[::-1], checkpoints_dir=\"experiments/20241130_064549_two_phase_training/model_checkpoints\")#checkpoints_loading_order[::-1]\n",
    "    # phase1\n",
    "    for dataset in valid_fold['stratify_on'].unique():\n",
    "        data_df = valid_fold[valid_fold['stratify_on']==dataset].reset_index(drop=True)\n",
    "        valid_loader = audio_data_loader(data_df,batch_size=1 ,shuffle=False)\n",
    "        results.append([\"phase2\",\"last\",fold,dataset, *valid_one_epoch(valid_loader, model, loss)])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Phase checkpoint  Fold          Dataset      Loss              Accuracy\n",
      "0    phase1       best     0    CREMA-D_angry  0.231325    [95.2755905511811]\n",
      "1    phase1       best     0    CREMA-D_happy  0.306591    [93.7007874015748]\n",
      "2    phase1       best     0  CREMA-D_neutral  0.262412    [96.7741935483871]\n",
      "3    phase1       best     0      CREMA-D_sad  0.688466   [76.77165354330708]\n",
      "4    phase1       best     0    SUBESCO_angry  0.238367                [95.5]\n",
      "..      ...        ...   ...              ...       ...                   ...\n",
      "395  phase2       last     4    UJ_ARABIC_sad  2.749868  [14.285714285714286]\n",
      "396  phase2       last     4       URDU_angry  0.177102               [100.0]\n",
      "397  phase2       last     4       URDU_happy  0.375868                [90.0]\n",
      "398  phase2       last     4     URDU_neutral  0.350605                [90.0]\n",
      "399  phase2       last     4         URDU_sad  0.494261                [90.0]\n",
      "\n",
      "[400 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with descriptive column names\n",
    "df = pd.DataFrame(results, columns=['Phase','checkpoint', 'Fold', 'Dataset', 'Loss', 'Accuracy'])\n",
    "\n",
    "# Optional: print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"meta.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.rnn,'GRU.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1778833/2959341848.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.projector =torch.load('proj.pt')\n"
     ]
    }
   ],
   "source": [
    "model.projector =torch.load('proj.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 02:53:23,802 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt: /home/naif/.cache/modelscope/hub/iic/emotion2vec_plus_base/model.pt\n",
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|█████████████████████████████| 530/530 [00:11<00:00, 45.23it/s]\n",
      "Validating: 100%|█████████████████████████████| 225/225 [00:05<00:00, 38.00it/s]\n",
      "Validating: 100%|█████████████████████████████| 514/514 [00:13<00:00, 37.87it/s]\n",
      "Validating: 100%|█████████████████████████████| 101/101 [00:02<00:00, 39.54it/s]\n",
      "Validating: 100%|█████████████████████████████| 636/636 [00:13<00:00, 46.20it/s]\n",
      "Validating: 100%|█████████████████████████████| 636/636 [00:13<00:00, 45.86it/s]\n",
      "Validating: 100%|█████████████████████████████| 636/636 [00:13<00:00, 45.69it/s]\n",
      "Validating: 100%|█████████████████████████████| 544/544 [00:11<00:00, 45.54it/s]\n",
      "Validating: 100%|███████████████████████████████| 50/50 [00:01<00:00, 39.47it/s]\n",
      "Validating: 100%|███████████████████████████████| 50/50 [00:01<00:00, 38.59it/s]\n",
      "Validating: 100%|███████████████████████████████| 50/50 [00:01<00:00, 38.67it/s]\n",
      "Validating: 100%|███████████████████████████████| 50/50 [00:01<00:00, 38.85it/s]\n",
      "Validating: 100%|███████████████████████████████| 35/35 [00:01<00:00, 30.70it/s]\n",
      "Validating: 100%|███████████████████████████████| 16/16 [00:00<00:00, 24.06it/s]\n",
      "Validating: 100%|███████████████████████████████| 19/19 [00:00<00:00, 23.15it/s]\n",
      "Validating: 100%|███████████████████████████████| 19/19 [00:00<00:00, 22.48it/s]\n",
      "Validating: 100%|███████████████████████████████| 36/36 [00:01<00:00, 31.28it/s]\n",
      "Validating: 100%|█████████████████████████████| 261/261 [00:06<00:00, 39.97it/s]\n",
      "Validating: 100%|███████████████████████████████| 29/29 [00:00<00:00, 33.58it/s]\n",
      "Validating: 100%|███████████████████████████████| 84/84 [00:02<00:00, 37.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['ShEMO_angry', 0.6313660236361548, [93.9622641509434]],\n",
       " ['ShEMO_sad', 2.635327648201336, [84.22222222222223]],\n",
       " ['ShEMO_neutral', 0.9424898337682933, [85.60311284046692]],\n",
       " ['ShEMO_happy', 1.3384973176699921, [79.20792079207921]],\n",
       " ['CREMA-D_angry', 0.34798652494166926, [94.88993710691824]],\n",
       " ['CREMA-D_sad', 0.8742313235636252, [86.55660377358491]],\n",
       " ['CREMA-D_happy', 0.6648911659654918, [90.40880503144655]],\n",
       " ['CREMA-D_neutral', 0.09881785538015328, [97.88602941176471]],\n",
       " ['URDU_neutral', 0.1295671649468027, [97.0]],\n",
       " ['URDU_angry', 0.5243660743927862, [92.0]],\n",
       " ['URDU_sad', 1.2722682539913879, [90.0]],\n",
       " ['URDU_happy', 1.152156426681484, [86.0]],\n",
       " ['UJ_ARABIC_angry', 2.4565693540370597, [64.28571428571429]],\n",
       " ['UJ_ARABIC_happy', 3.0864874319383926, [56.25]],\n",
       " ['UJ_ARABIC_sad', 6.866490197306695, [42.10526315789474]],\n",
       " ['UJ_ARABIC_neutral', 1.395467316094856, [81.57894736842105]],\n",
       " ['SaudiEMO_sad', 5.440301708282078, [37.5]],\n",
       " ['SaudiEMO_neutral', 3.491840683522906, [50.383141762452105]],\n",
       " ['SaudiEMO_happy', 1.137825006857646, [72.41379310344827]],\n",
       " ['SaudiEMO_angry', 2.984139824322492, [57.142857142857146]]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.train import valid_one_epoch\n",
    "model = getattr(models, 'FullEm2vecTest')(num_classes=num_classes).cuda()\n",
    "\n",
    "loss =lambda output,target: F.cross_entropy(output, target)\n",
    "\n",
    "# load_model(model_name+suffix,model, checkpoints_loading_order[::-1])\n",
    "results =[]\n",
    "for dataset in df['stratify_on'].unique():\n",
    "    data_df = df[df['stratify_on']==dataset]\n",
    "    valid_loader = audio_data_loader(data_df,batch_size=2 ,shuffle=False)\n",
    "    results.append([dataset, *valid_one_epoch(valid_loader, model, loss)])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 18:39:29,061 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt: /home/naif/.cache/modelscope/hub/iic/emotion2vec_plus_base/model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naif/miniconda3/envs/em2vec/lib/python3.8/site-packages/funasr/train_utils/load_pretrained_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  src_state = torch.load(path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    }
   ],
   "source": [
    "from funasr import AutoModel\n",
    "from src.helpers import load_audio\n",
    "exc = AutoModel(model=\"iic/emotion2vec_plus_base\",  device=\"cuda\")\n",
    "wav_file = f\"رياكشن معصب ٢.wav\"\n",
    "res = exc.inference(wav_file, granularity=\"utterance\", extract_embedding=True,disable_pbar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    out = model(torch.from_numpy(res[0]['feats']).to('cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['Emotion'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0229, 0.0054, 0.9504, 0.0063, 0.0150], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angry', 'fear', 'happy', 'neutral', 'sad'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'رياكشن معصب ٢',\n",
       " 'labels': ['生气/angry',\n",
       "  '厌恶/disgusted',\n",
       "  '恐惧/fearful',\n",
       "  '开心/happy',\n",
       "  '中立/neutral',\n",
       "  '其他/other',\n",
       "  '难过/sad',\n",
       "  '吃惊/surprised',\n",
       "  '<unk>'],\n",
       " 'scores': [3.271125592618773e-07,\n",
       "  4.801344388738471e-08,\n",
       "  1.5675524878133729e-07,\n",
       "  0.999998927116394,\n",
       "  5.552280413212429e-09,\n",
       "  1.1092309293303515e-08,\n",
       "  4.856877922065905e-07,\n",
       "  8.402290774256471e-08,\n",
       "  1.57839663561532e-09],\n",
       " 'feats': array([-1.95248723e+00,  5.59582174e-01,  6.73812985e-01, -8.30257162e-02,\n",
       "        -1.22654605e+00,  1.68417907e+00,  3.78143042e-01, -7.12111831e-01,\n",
       "        -5.32430172e-01,  9.73603308e-01,  2.87339360e-01, -1.19878554e+00,\n",
       "         1.48408413e-02,  2.61405969e+00, -3.24683619e+00,  1.22547472e+00,\n",
       "        -1.14901733e+00,  7.62447834e-01, -9.01012301e-01, -6.68091655e-01,\n",
       "        -1.25311911e+00, -1.06551325e+00, -7.04200685e-01, -1.19542569e-01,\n",
       "         6.96588457e-01,  1.51642263e+00, -3.23578501e+00, -2.81354249e-01,\n",
       "         1.70422697e+00,  2.02054277e-01,  3.04226637e-01,  1.46696448e-01,\n",
       "         8.88115764e-01,  2.84062177e-01, -1.51286721e+00, -7.50861406e-01,\n",
       "        -4.01372731e-01, -5.70386171e-01, -1.07409525e+00,  3.50211787e+00,\n",
       "        -1.78126597e+00, -1.50596118e+00,  6.02012634e-01,  1.85213178e-01,\n",
       "        -1.18965209e+00, -9.10836339e-01, -8.00598621e-01,  1.07542241e+00,\n",
       "         6.15890741e-01, -3.45369846e-01,  1.44257927e+00, -8.26042414e-01,\n",
       "        -5.61628878e-01,  3.96923214e-01,  1.12506688e+00, -1.88000154e+00,\n",
       "         2.39775732e-01, -5.15435278e-01,  9.18844283e-01, -7.28443325e-01,\n",
       "        -1.42115176e-01,  9.04207647e-01, -1.15602803e+00,  2.57878611e-03,\n",
       "         8.15779809e-03,  4.47669566e-01,  9.49996769e-01,  7.30826020e-01,\n",
       "        -7.74181128e-01, -3.15880090e-01,  4.78859723e-01,  1.38709950e+00,\n",
       "        -6.39771402e-01, -1.41952157e+00,  2.97420055e-01, -7.68081665e-01,\n",
       "         6.27663314e-01, -4.80892546e-02, -1.21998489e-01, -7.24873304e-01,\n",
       "         7.83130944e-01,  8.16633582e-01,  1.08263183e+00,  5.37228435e-02,\n",
       "         4.40143198e-01, -4.00026515e-02,  9.11389887e-01,  1.10014200e+00,\n",
       "         1.36573756e+00, -1.50015637e-01,  1.30724549e+00,  5.07462084e-01,\n",
       "         6.16054118e-01,  5.86954728e-02,  9.47016597e-01, -4.62498337e-01,\n",
       "        -1.18945086e+00, -7.52997637e-01, -2.39829585e-01, -9.37713563e-01,\n",
       "         1.04713237e+00,  8.63418937e-01,  5.99996567e-01,  2.47480303e-01,\n",
       "         1.12576008e+00,  2.04726529e+00,  1.41861483e-01, -3.68399024e-01,\n",
       "         8.06925535e-01,  1.04073989e+00, -1.59081709e+00, -1.23365963e+00,\n",
       "         1.05773854e+00, -1.12523937e+00,  1.02131915e+00, -1.11938381e+00,\n",
       "        -1.20830238e+00, -5.46972036e-01,  4.17927504e-01,  5.73347926e-01,\n",
       "        -6.44567907e-01, -4.84504670e-01, -3.53078730e-02,  1.95471942e-01,\n",
       "         1.33160782e+00,  5.18039882e-01,  1.08397882e-02,  2.76280254e-01,\n",
       "         6.30094260e-02, -1.06079921e-01, -6.92191124e-01,  6.62680328e-01,\n",
       "        -1.12040877e+00,  6.70540094e-01, -7.44326949e-01, -1.09851599e+00,\n",
       "         1.74109727e-01, -5.19487299e-02, -3.97554934e-01,  8.03279996e-01,\n",
       "        -1.22315133e+00,  3.55897307e-01, -1.35158801e+00, -2.24307358e-01,\n",
       "        -1.47367036e+00,  4.65620488e-01,  8.90571237e-01,  7.50684857e-01,\n",
       "         6.94644094e-01,  1.23149686e-01,  2.26559296e-01, -1.47884977e+00,\n",
       "         2.44219750e-01, -3.98317277e-01,  1.17698991e+00,  9.78662074e-01,\n",
       "        -8.53663266e-01, -4.06062126e-01,  1.45642728e-01,  2.67382234e-01,\n",
       "         9.50967789e-01,  5.34138143e-01, -1.56686977e-01, -9.77051020e-01,\n",
       "        -6.78603590e-01, -6.64102674e-01, -2.15987444e-01,  1.33860922e+00,\n",
       "         4.57834959e-01,  1.24530876e+00,  1.78980362e+00,  1.22496557e+00,\n",
       "         7.47107506e-01,  1.49966860e+00,  8.60907495e-01, -3.81732255e-01,\n",
       "         5.38234472e-01,  8.87093127e-01,  5.05129039e-01, -1.13832675e-01,\n",
       "        -8.38757277e-01, -1.09602106e+00,  1.20666635e+00, -2.64657974e-01,\n",
       "         1.03206742e+00,  4.08437133e-01, -9.85648274e-01,  7.44286299e-01,\n",
       "        -6.89230978e-01, -9.86137450e-01, -4.80150670e-01,  1.07350075e+00,\n",
       "        -1.00830197e+00,  1.26902256e+01,  1.83809578e-01,  1.32014621e-02,\n",
       "        -3.18146825e-01,  1.42070961e+00,  7.97899544e-01,  9.89785135e-01,\n",
       "        -1.17000091e+00,  7.46885538e-01,  1.55877984e+00, -1.16210508e+00,\n",
       "        -8.90395105e-01,  1.18961072e+00, -4.02283877e-01, -1.34376478e+00,\n",
       "         5.64492047e-01,  1.36159301e-01, -3.48181486e-01,  9.56962824e-01,\n",
       "        -1.02813113e+00, -7.80921280e-01, -5.83388686e-01,  1.49504292e+00,\n",
       "        -1.10350466e+00,  2.79493183e-01, -5.52493191e+00, -1.62305370e-01,\n",
       "         1.04691714e-01,  3.68905872e-01, -1.22360185e-01, -1.15793324e+00,\n",
       "         1.06855750e+00, -1.24799967e+00, -5.82237422e-01, -1.90521026e+00,\n",
       "        -1.32454073e+00, -1.11675179e+00,  2.82195234e+00, -3.35373878e-01,\n",
       "        -1.46634054e+00,  6.87225163e-01, -7.57071435e-01, -1.71746105e-01,\n",
       "        -9.64026272e-01, -6.71614408e-01,  3.68317246e-01, -1.24020720e+00,\n",
       "         7.32032508e-02, -2.94622451e-01, -6.18979454e-01,  1.34314108e+00,\n",
       "         5.05512416e-01,  1.44070637e+00, -3.21934968e-01, -2.20287085e-01,\n",
       "        -6.83928132e-01,  2.08111954e+00, -2.03705266e-01, -5.01421750e-01,\n",
       "         7.99423277e-01, -8.11614454e-01, -6.32021606e-01,  7.46616066e-01,\n",
       "        -1.16616869e+00,  4.51194823e-01,  9.41638589e-01,  1.11836739e-01,\n",
       "        -9.76320028e-01, -5.15511751e-01,  1.82434022e+00, -1.03830934e+00,\n",
       "        -6.24701023e-01,  8.23709965e-01,  1.01134562e+00, -1.72907874e-01,\n",
       "        -1.01048481e+00,  7.09225416e-01,  7.09488466e-02, -7.56025672e-01,\n",
       "         7.32005477e-01,  4.05758411e-01,  1.08351612e+00, -1.17300963e+00,\n",
       "        -3.27029765e-01,  2.77215779e-01, -1.08251429e+00,  1.30581677e+00,\n",
       "        -3.03316027e-01, -6.46571457e-01,  2.34122545e-01,  6.87311709e-01,\n",
       "         1.40416026e-02, -2.18174085e-01, -2.06481600e+00, -2.74857223e-01,\n",
       "        -3.30807984e-01,  3.52980763e-01, -3.22915649e+00,  7.80921996e-01,\n",
       "        -1.18871665e+00, -3.13276112e-01, -9.97976542e-01,  8.81569266e-01,\n",
       "        -2.02783301e-01,  7.57836878e-01, -4.45089549e-01,  6.76423311e-01,\n",
       "         3.44016361e+00,  2.76243612e-02,  5.86570263e-01,  8.79790127e-01,\n",
       "        -7.28113651e-01,  3.97169620e-01,  2.37225592e-01,  1.55397522e+00,\n",
       "         8.66821587e-01, -1.94548929e+00,  1.02861583e+00, -2.30125412e-01,\n",
       "        -1.24042010e+00, -8.96757960e-01,  6.32425725e-01,  9.82845962e-01,\n",
       "         7.68996358e-01,  5.66171408e-01, -1.78282261e-02,  3.29664797e-01,\n",
       "        -4.08934593e-01,  4.28567268e-02, -3.83776605e-01, -5.56044221e-01,\n",
       "         5.07380307e-01,  2.46958613e-01, -5.85263371e-01,  1.06593704e+00,\n",
       "         4.30195451e-01,  1.69559717e+00, -1.96532607e-01, -1.20040631e+00,\n",
       "         7.85770267e-02,  1.22249454e-01, -5.64644635e-01,  2.18023658e+00,\n",
       "        -5.84582925e-01,  1.29482830e+00,  1.65084526e-01,  3.70773697e+00,\n",
       "        -2.81979471e-01, -9.23100531e-01, -4.50898767e-01, -2.05525112e+00,\n",
       "        -2.65823674e+00, -1.74828970e+00,  4.79056180e-01, -9.67039287e-01,\n",
       "         7.23662674e-01,  4.22690153e-01,  3.38537097e-01, -7.98145592e-01,\n",
       "        -3.93202752e-01,  5.72110534e-01,  1.86203420e-01, -2.76812047e-01,\n",
       "         2.16409636e+00, -7.85857260e-01, -2.16997600e+00,  1.70537949e+00,\n",
       "         5.23340166e-01,  2.36697102e+00, -1.80688226e+00, -5.23211539e-01,\n",
       "         7.61984706e-01,  7.67665684e-01,  1.22893333e+00,  1.43199876e-01,\n",
       "         8.46979082e-01, -1.54881760e-01, -6.58962488e-01, -4.77429330e-01,\n",
       "        -5.32388628e-01, -1.82362151e+00,  1.13624297e-01, -1.92752972e-01,\n",
       "         2.38024688e+00, -1.14050078e+00,  1.40475348e-01,  7.34141290e-01,\n",
       "         1.07093893e-01, -1.24581933e+00,  5.94019473e-01, -5.21134555e-01,\n",
       "         4.04709160e-01,  5.09213150e-01,  8.81466642e-02, -7.58301556e-01,\n",
       "         2.67081141e-01, -7.94860959e-01,  1.88985765e-01, -9.89318848e-01,\n",
       "        -1.63495851e+00, -4.61764857e-02,  9.06761110e-01,  7.48098314e-01,\n",
       "        -1.29771268e+00,  7.76507854e-01, -6.48927391e-01,  6.72284186e-01,\n",
       "         2.01230615e-01, -6.35273814e-01,  2.66263247e-01,  1.10449839e+00,\n",
       "        -6.77025393e-02, -1.51617205e+00,  9.96571958e-01, -1.02071539e-01,\n",
       "         1.14997908e-01, -8.38874519e-01, -7.58470833e-01, -2.05006152e-01,\n",
       "        -9.44924831e-01,  7.97856867e-01,  1.10902858e+00,  6.87500119e-01,\n",
       "        -2.21051499e-01,  3.39770883e-01,  2.86983937e-01, -6.81869805e-01,\n",
       "        -7.40617812e-01, -1.03372729e+00, -1.99907407e-01, -1.21127927e+00,\n",
       "        -1.27331316e+00, -1.04684138e+00, -5.88084102e-01, -1.14371162e-02,\n",
       "        -1.02934813e+00, -8.40546906e-01,  2.87030011e-01,  4.15191859e-01,\n",
       "         6.28080845e-01,  7.07442462e-01, -4.10447985e-01,  4.28173751e-01,\n",
       "         3.32985073e-01, -6.58547342e-01, -1.75259322e-01, -1.03062105e+00,\n",
       "        -2.82069057e-01,  7.87884474e-01,  9.39997911e-01,  8.14751565e-01,\n",
       "         1.31316388e+00,  5.32867014e-02,  1.21743512e+00, -8.11732411e-01,\n",
       "        -5.42564452e-01,  1.46373475e+00, -1.38177395e-01, -8.33491027e-01,\n",
       "         2.07664990e+00,  8.01372528e-01, -1.13548040e+00,  7.50404671e-02,\n",
       "        -1.06285512e+00,  3.99281323e-01, -3.46000016e-01, -4.36485082e-01,\n",
       "         4.31162715e-01, -4.12242293e-01,  1.19143486e+00, -8.60834181e-01,\n",
       "        -1.02151382e+00, -4.77627397e-01,  4.11114722e-01,  4.59712930e-02,\n",
       "         1.60473287e+00, -9.57910299e-01, -6.70018137e-01,  1.64836630e-01,\n",
       "        -1.44718802e+00, -7.33802393e-02,  2.83446431e-01,  7.29148090e-01,\n",
       "        -1.80941391e+00, -7.61493221e-02,  5.80100954e-01, -6.99653327e-01,\n",
       "         3.93715734e-03, -3.09363842e-01,  2.90969551e-01,  1.03670025e+00,\n",
       "         1.41874528e+00,  2.44816542e-01,  6.02331236e-02, -5.56255169e-02,\n",
       "         3.54219735e-01,  2.28756532e-01, -5.20778239e-01,  3.14447731e-01,\n",
       "        -1.03182442e-01, -1.42447805e+00,  2.86610991e-01,  4.43290561e-01,\n",
       "         5.96723080e-01,  6.02257907e-01,  3.43348712e-01, -9.43377435e-01,\n",
       "         6.53116584e-01, -9.95804727e-01, -5.23810446e-01,  6.54317200e-01,\n",
       "        -9.35263872e-01,  2.73373798e-02,  1.05622935e+00,  1.70006871e+00,\n",
       "         1.22960746e-01,  9.96703744e-01,  1.09448397e+00,  8.40534151e-01,\n",
       "        -1.72460496e+00, -1.80766180e-01, -7.05081880e-01,  2.22591177e-01,\n",
       "        -1.15362847e+00,  9.98012871e-02,  3.37561607e-01, -1.50219826e-02,\n",
       "         1.25359082e+00,  1.01058315e-02,  6.66264117e-01, -3.14272434e-01,\n",
       "        -6.68002784e-01, -6.71723545e-01, -1.19929798e-01,  4.79051918e-01,\n",
       "         1.43186837e-01,  2.35807318e-02,  6.72187984e-01,  2.60614872e+00,\n",
       "        -7.63005018e-01,  7.82152832e-01, -1.07016361e+00, -2.16128409e-01,\n",
       "        -7.51279354e-01, -2.09915563e-01,  5.73566198e-01, -7.36325383e-01,\n",
       "         1.49772465e-01, -4.41196293e-01,  1.78305104e-01, -8.23602378e-02,\n",
       "        -5.20150065e-01,  1.29475489e-01, -8.25927198e-01, -6.30583227e-01,\n",
       "         3.89456481e-01,  7.58481205e-01,  2.70581208e-02, -9.22443941e-02,\n",
       "        -6.88181877e-01, -1.30964443e-01,  4.19393033e-01, -7.21871018e-01,\n",
       "         7.44426012e-01, -4.70730484e-01, -8.22430253e-02,  1.72412336e-01,\n",
       "         7.81096876e-01,  8.52785587e-01,  7.23138452e-01,  1.93252087e+00,\n",
       "         2.66765893e-01,  1.86388171e+00,  1.57933569e+00, -2.58315238e-03,\n",
       "        -8.76578927e-01,  1.45541653e-01, -1.16561592e+00,  6.08172566e-02,\n",
       "        -1.33016956e+00, -5.61056137e-02, -4.22125250e-01,  6.13396227e-01,\n",
       "         2.99531817e-01, -5.39554060e-02,  1.14230192e+00,  1.01968920e+00,\n",
       "        -1.28277102e-02, -5.72062731e-01,  6.75550163e-01, -4.09218818e-01,\n",
       "        -1.97200024e+00,  3.25325638e-01, -8.15172553e-01,  7.87057161e-01,\n",
       "        -5.58199596e+00, -7.55575657e-01,  5.98541386e-02,  6.71396703e-02,\n",
       "         1.11206281e+00, -1.44980395e+00, -9.38178599e-02,  1.44281983e+00,\n",
       "         6.37416422e-01,  1.51699698e+00, -1.02779877e+00, -8.51756394e-01,\n",
       "         1.44899666e+00,  1.71785116e-01, -1.72743690e+00,  7.55865633e-01,\n",
       "        -2.13334188e-01, -3.86388183e-01, -5.83376028e-02, -6.50529027e-01,\n",
       "         2.72810555e+00,  1.56057107e+00, -9.64161694e-01,  4.79644090e-01,\n",
       "        -9.01869118e-01,  4.58438516e-01, -1.70965828e-02,  6.98031843e-01,\n",
       "        -6.42836750e-01, -9.51533079e-01,  1.22155907e-04,  8.22125435e-01,\n",
       "         1.19823420e+00,  6.22970387e-02, -2.96115220e-01,  4.47407633e-01,\n",
       "         7.68028677e-01, -8.23774099e-01,  5.43236375e-01, -1.07259981e-01,\n",
       "        -4.34361212e-02,  7.80798435e-01, -3.38447187e-03,  1.32788777e+00,\n",
       "         1.17761922e+00,  2.58849651e-01,  6.16487324e-01,  2.27081442e+00,\n",
       "         1.23242497e+00,  8.18298876e-01, -2.75862217e+00,  4.75158751e-01,\n",
       "        -2.53002048e-01, -1.71722040e-01,  3.46667141e-01, -9.97875273e-01,\n",
       "        -5.30541003e-01,  6.79857492e-01, -1.93680495e-01, -1.31697506e-01,\n",
       "         9.24650371e-01, -1.06160998e-01, -6.90914810e-01, -1.03523254e+00,\n",
       "        -8.25827494e-02, -5.65495670e-01, -3.57962549e-01, -1.41706860e+00,\n",
       "         1.07680583e+00,  7.63744295e-01, -8.45362008e-01, -7.70231485e-01,\n",
       "         1.40028286e+00,  4.14183214e-02, -3.56561430e-02,  1.32811703e-02,\n",
       "        -5.57887971e-01,  3.74468416e-01, -1.01909864e+00, -2.97358990e-01,\n",
       "         1.07615352e+00, -8.30146074e-01,  8.53859186e-01, -9.26398218e-01,\n",
       "        -7.62030840e-01,  5.71843624e-01, -6.01836324e-01,  3.94668207e-02,\n",
       "         4.49906617e-01,  8.58533289e-03,  8.66657734e-01,  5.73063612e-01,\n",
       "        -6.90997660e-01,  1.93848526e+00, -3.60929370e-01,  1.92404830e+00,\n",
       "         2.70449758e-01,  2.74217218e-01, -4.19559360e-01, -9.87912416e-01,\n",
       "        -1.24213898e+00, -1.30698249e-01,  4.69038546e-01, -3.99389535e-01,\n",
       "        -1.17496920e+00, -2.00141883e+00,  7.82162964e-01, -9.09900546e-01,\n",
       "        -1.34592101e-01, -5.16925633e-01,  8.09808552e-01,  1.28378487e+00,\n",
       "        -5.89001119e-01,  1.42288554e+00, -2.54652882e+00,  1.36389405e-01,\n",
       "        -1.76785338e+00, -9.74117815e-01,  1.94051176e-01,  4.26828235e-01,\n",
       "        -3.02694976e-01,  2.61327416e-01, -3.94698262e-01, -1.17939293e+00,\n",
       "        -3.26833665e-01, -1.29920650e+00,  1.30531538e+00,  3.50071080e-02,\n",
       "        -7.24213958e-01, -4.66484278e-01, -5.67901552e-01,  2.35091254e-01,\n",
       "        -1.76446879e+00,  5.16142488e-01, -2.62057245e-01, -1.20791113e+00,\n",
       "        -6.62594140e-01, -1.00971162e+00,  2.50720382e-01, -6.08803272e-01,\n",
       "        -1.88609064e-01, -1.64823616e+00, -6.35425687e-01, -1.91884184e+00,\n",
       "         1.32884726e-01,  2.21154884e-01, -1.00834489e+00,  7.32104003e-01,\n",
       "         1.77554006e-03, -1.11695492e+00, -5.25045931e-01, -1.10326421e+00,\n",
       "         1.82175314e+00,  1.24194646e+00, -2.07774714e-01,  3.81735951e-01,\n",
       "        -1.06266296e+00, -1.86677784e-01,  5.92665076e-01, -1.17261730e-01,\n",
       "         1.83431610e-01,  3.80867600e-01,  3.55005205e-01, -4.74390417e-01,\n",
       "        -9.95372355e-01, -5.93604147e-02,  1.05974054e+00, -1.37741840e+00,\n",
       "        -1.82457179e-01, -4.77017134e-01,  1.86540580e+00,  1.73621666e+00,\n",
       "        -1.07674921e+00,  2.42543387e+00, -1.10815835e+00,  1.20661175e+00,\n",
       "         5.19826114e-01,  1.23914249e-01,  2.04284400e-01,  2.02899605e-01],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 04:04:55,483 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt: /home/naif/.cache/modelscope/hub/iic/emotion2vec_plus_base/model.pt\n",
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    }
   ],
   "source": [
    "model = getattr(models, 'FullEm2vecTest')(num_classes=num_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def create_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create confusion matrix from true and predicted labels without using sklearn.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Array of true labels\n",
    "        y_pred: Array of predicted labels\n",
    "        num_classes: Number of classes\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Confusion matrix\n",
    "    \"\"\"\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    num_classes: int,\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    class_names: Optional[List[str]] = None,\n",
    "    figsize: tuple = (10, 8),\n",
    "    cmap: str = 'Blues',\n",
    "    normalize: bool = True,\n",
    "    title: str = 'Confusion Matrix',\n",
    "    save_path: Optional[str] = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Generate and plot confusion matrix from PyTorch model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model in evaluation mode\n",
    "        dataloader: PyTorch dataloader containing validation/test data\n",
    "        num_classes: Number of classes in the dataset\n",
    "        device: Device to run predictions on ('cuda' or 'cpu')\n",
    "        class_names: List of class names for labels\n",
    "        figsize: Figure size for the plot\n",
    "        cmap: Color map for the confusion matrix\n",
    "        normalize: Whether to normalize confusion matrix values\n",
    "        title: Title for the plot\n",
    "        save_path: Path to save the plot (optional)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (confusion matrix array, figure object)\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(num_classes)]\n",
    "    \n",
    "    # Ensure correct number of class names\n",
    "    assert len(class_names) == num_classes, \"Number of class names must match num_classes\"\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Lists to store predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Disable gradient computation for inference\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Handle different dataloader formats\n",
    "            if isinstance(batch, (tuple, list)):\n",
    "                inputs, labels = batch[:2]\n",
    "            else:\n",
    "                raise ValueError(\"Dataloader must return tuple/list with (inputs, labels)\")\n",
    "            \n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = create_confusion_matrix(all_labels, all_preds, num_classes)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        # True Positives, False Positives, False Negatives\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics[class_names[i]] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        row_sums = cm.sum(axis=1)\n",
    "        cm_norm = cm / row_sums[:, np.newaxis]\n",
    "        cm_plot = cm_norm\n",
    "        fmt = '.2%'\n",
    "    else:\n",
    "        cm_plot = cm\n",
    "        fmt = 'd'\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm_plot, \n",
    "                annot=True, \n",
    "                fmt=fmt,\n",
    "                cmap=cmap,\n",
    "                square=True,\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                ax=ax)\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Tight layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    return cm, metrics, fig\n",
    "\n",
    "def print_classification_report(metrics: dict):\n",
    "    \"\"\"\n",
    "    Print a formatted classification report from metrics dictionary.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing precision, recall, and F1 score for each class\n",
    "    \"\"\"\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1-Score':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Calculate macro averages\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    avg_f1 = 0\n",
    "    n_classes = len(metrics)\n",
    "    \n",
    "    for class_name, scores in metrics.items():\n",
    "        print(f\"{class_name:<15} {scores['precision']:>10.2%} {scores['recall']:>10.2%} {scores['f1_score']:>10.2%}\")\n",
    "        avg_precision += scores['precision']\n",
    "        avg_recall += scores['recall']\n",
    "        avg_f1 += scores['f1_score']\n",
    "    \n",
    "    # Print macro averages\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Macro Average':<15} {avg_precision/n_classes:>10.2%} {avg_recall/n_classes:>10.2%} {avg_f1/n_classes:>10.2%}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the confusion matrix plotting function\"\"\"\n",
    "    # Assuming you have your model and dataloader ready\n",
    "    # model = YourModel()\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    class_names = df['Emotion'].cat.categories  # Replace with your class names\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    cm, metrics, fig = plot_confusion_matrix(\n",
    "        model=model,\n",
    "        dataloader=audio_data_loader(df,batch_size=2 ,shuffle=False),\n",
    "        num_classes=num_classes,\n",
    "        class_names=class_names,\n",
    "        normalize=True,\n",
    "        title='Model Confusion Matrix'\n",
    "    )\n",
    "    \n",
    "    # Display plot\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print_classification_report(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "------------------------------------------------------------\n",
      "Class            Precision     Recall   F1-Score\n",
      "------------------------------------------------------------\n",
      "angry               96.35%     90.57%     93.37%\n",
      "happy               93.92%     85.51%     89.52%\n",
      "neutral             75.33%     96.22%     84.50%\n",
      "sad                 96.35%     77.74%     86.05%\n",
      "------------------------------------------------------------\n",
      "Macro Average       90.49%     87.51%     88.36%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
